\chapter{Appendix: A Mathematical Toolbox}
This book is designed so that even high schoolers can access quantum mechanics. As such, this section is a reference section, which acknowledges the fact that high school curricula might teach different topics in slightly different orders and detail, and also that some of the prerequisite mathematics assumed here might be just a small reach beyond high school syllabus. The reference section contains some instruction, but it should not be used for anyone learning these topics for the first time, as I have been quite brief so as not to distract from the main topic of this book. For those who are unfamiliar with using any of the mathematical rules referenced as assumed knowledge, they should turn to any textbook which covers these topics and complete a thorough run through some exercises before reading any other part of this book. For many other students, they can simply look at the exercises in these sections, and if they do not see anything unfamiliar, they can simply skip this reference chapter.
\\\\
\section{Mathematical Syntax}
There are multiple symbols used as figures of proof in this book, which represent common phrases one might turn to were they reciting a proof out loud. As these are ubiquitous symbols, being fluent reading them will be necessary, as otherwise they can be especially easy to confuse with each other.
\\\\
\begin{tabular}{| c | c |}
\hline
 \textbf{Symbol} & \textbf{Meaning} \\ 
 \hline
 $:=$ & `define equals'\\
 \hline
 $\forall$ & `For all' \\
 \hline
 $\exists$ & `There exists'\\
 \hline
 $\in$ & `In' (for a set)\\
 \hline
 $\implies$ & `Implies' \\
 \hline
 $\iff$ & `Implies each other' \\
 \hline
`Iff' & `If and only if' \\
\hline
 $\sum_{i\neq j}$ & `Sum over all $i$ not equal to $j$' \\
 \hline
 $\sum_{\{i\}}$ & `Sum over all $x$ in a set $\{x\}$'\\
 \hline
 $\square$ & Q.E.D \\
 \hline
 $f'(x)$ & First Derivative of $f(x)$ \\
 \hline
 $f''(x)$ & Second Derivative of $f(x)$\\
 \hline
 $\setof{x_{i}}$ & `the set of values $x_{i}$'\\
 \hline
 $x\to y$ & `x approaches y'\\
 \hline
 $\mathbb{Z}$ & the set of all integers \\ 
 \hline
 $\mathbb{R}$ & the set of all real numbers \\
 \hline
 $\mathbb{Z}^{+}$ & the set of all positive integers \\
 \hline
 $\equiv$ & `is equivalent to'\\
 \hline
\end{tabular}
\\\\
Fluency with basic summation and product notation is assumed. There is also interval notation commonly used for inequalities:
\begin{itemize}
    \item $x\in[a,b)\iff a\leq x < b$
    \item $x\in(a,b]\iff a< x \leq b$
    \item $x\in(a,b) \iff a< x< b$
    \item $x\in [a,b] \iff a \leq x \leq b$
\end{itemize}
and round brackets are always used for any side of the inequality bounded by $\pm \infty$.
\section{Probability$\ast$}
In this book, as correct and conventional, probabilities are numbers between $0$ and $1$, sometimes represented by fractions. 

For a random $X$ variable which can take multiple values $x_{i}$
\section{Complex Numbers$\ast$}
Define the imaginary unit, $i$, to be equal to the square root of $-1$. We then have:
\begin{itemize}
    \item $i^2= (\sqrt{-1})^2=-1$
    \item $\forall\stab k \in \mathbb{R}, \mtab \frac{k}{i}=\frac{ik}{i^2}=\frac{ik}{-1}=-ik$
    \item $\forall\stab k \in \mathbb{R}^+, \mtab \sqrt{-k}\equiv\sqrt{-1} \times \sqrt{k}=i\sqrt{k}$
    \item $\forall\stab a,b \in \mathbb{R}, \mtab ai \pm bi = a\sqrt{-1}\pm b\sqrt{-1}= (a\pm b)\sqrt{-1}=(a\pm b)i$
\end{itemize}
Complex numbers are numbers usually represented in the form 
$$
z:=a+bi
$$
for some $a,b\in\mathbb{R}$. Then there are two functions commonly referred too:
\begin{itemize}
\item $\text{Re}(z)$ is called the `real part' of the imaginary number $z$, which is, the real number $a$ if $z:=a+bi$.
\item $\text{Im}(z)$ is called the `imaginary part' of the imaginary number $z$, which is, the real number $b$ if $z:=a+bi$. Note that $\text{Im}(z)=b$ and not $\text{Im}(z)=bi$. 
\end{itemize}
It is then clear that a real number is also a complex number, but with imaginary part $0$. Conversely, if a complex number has imaginary part $0$, then all that remains is its real part and so it must be a real number. Therefore,
\begin{itemize}
    \item $z \in \mathbb{R} \iff \text{Im}(z)=0$.
\end{itemize}
We may represent any complex number on a real-valued two dimensional plane called an Argand diagram (figure). From this figure we can create a few more useful definitions:
\begin{itemize}
    \item The modulus of a complex number $z$ is denoted $|z|$ and defined to be $|z|:=\sqrt{\text{Re}(z)^2+\text{Im}(z)^2}$. This is the application of Pythagoras' Theorem to find the distance from any point $z$ represented on the Argand diagram to the origin.
    \item For all complex numbers $z:=a+bi$, we denote with $z^{\ast}$ the complex conjugate of the complex number, defined by $z^{\ast}=a-bi$ (in other words, reversing the sign of the imaginary part).
    \item For all complex numbers $z$, we define the modulus squared (or square modulus) of that complex number to be $|z|^{2}:=zz^{\ast}$. This modulus squared is always real.
\subsubsection{Exercises on Complex Numbers*}
\begin{enumerate}
    \item Express as a single complex number with distinct real and imaginary parts:\begin{enumerate}
    \item $(4i)^3$ \\
    \item $(5-7i)(6-8i)$ \\
    \item $\frac{3+5i}{2-4i}$ \\
    \item $(-4-7i)^{\ast}$ \\
    \item $|2+3i|^2$ \\
    \end{enumerate}
    \item Show, with proof, whether each of the following statements are true \begin{enumerate}
        \item $|z|>|\text{Re}(z)|$ and $|z|\geq|\text{Im}(z)|$
        \\
        \item $\forall\stab z\in\mathbb{C}, \mtab |z|^2\in\mathbb{R}$ \\
    \end{enumerate}
    \item Show that $|r\cos\theta+ir\sin\theta|=r$ \\
    \item 
\end{enumerate}
\end{itemize}
\section{Matrices*}
A matrix is an array of values. They are placed into rectangular arrangements in rows and columns, and are an extremely important way of mathematically listing values. We shall see that the structure of a matrix allows for powerful computations to be done. For now, there are many definitions to cover.
\\\\
A matrix with $m$ rows and $n$ columns is said to be an $m\times n$ matrix. A $n\times n$ matrix is called a square matrix, a $n\times 1$ matrix is usually called a column vector, and a $1\times n$ matrix is usually called a row vector. 
\\\\
The values of a matrix are usually called the element of that matrix. For a matrix denoted by any symbol, for example $M$, the notation $M_{ij}$ usually represents the element in the $i$'th row and $j$'th column of the matrix. Therefore, we can show the four usual types of matrices we might often see:
\begin{enumerate}
    \item An arbitrary $m\times n$ matrix:
    $$
    \begin{bmatrix}
        \text{M}_{11} & \text{M}_{12} & \dots &\dots & \text{M}_{1n} \\
        \text{M}_{21} & \ddots & \dots &\dots & \vdots \\
        \vdots & \dots & \ddots & \dots & \vdots \\
        \text{M}_{m1} & \dots & \dots & \dots & \text{M}_{mn} \\
    \end{bmatrix}
    $$
    \item A $n\times n$ square matrix:
    $$
    \begin{bmatrix}
        \text{M}_{11} & \text{M}_{12} & \dots & \text{M}_{1n} \\
        \text{M}_{21} & \ddots & \dots & \vdots \\
        \vdots & \dots & \ddots  & \vdots \\
        \text{M}_{n1} & \dots & \dots & \text{M}_{nn} \\
    \end{bmatrix}
    $$
    \item A $n\times 1$ column vector:
    $$
    \begin{bmatrix}
        \text{M}_{11} \\
        \text{M}_{21} \\
        \vdots \\
        \vdots \\ 
        \text{M}_{n1} \\
    \end{bmatrix}
    $$
    \item A $1\times n$ row vector:
    $$
    \begin{bmatrix}
        \text{M}_{11} &
        \text{M}_{12} &
        \dots &
        \dots &
        \text{M}_{1n} 
    \end{bmatrix}
    $$
\end{enumerate}
As shown in the examples, a variety of dots are used often to show the idea that we can have a large number of unlisted elements between elements explicitly written. This should not be a great worry, since the arrangements of elements in a matrix are always in ordered rows and columns anyway. We now move onto defining the operations:
\begin{enumerate}
    \item Scalar multiplication of a matrix $M$ by a scalar $a$ results in a new matrix $aM$ with each previous element $M_{ij}$ replaced with a new element $aM_{ij}$. So for example,
    $$
    -1\times\begin{bmatrix}
        5 & 4 & -9 \\
        0 & -1 & 6 \\ 
    \end{bmatrix}
    =
    \begin{bmatrix}
        -5 & -4 & 9 \\
        0 & 1 & -6 \\ 
    \end{bmatrix}
    $$
    \item Two matrices can be summed iff they are of the same dimensions. If they are of the same dimensions, the sum of two matrices $M$ and $N$ are obtained by creating a new matrix $S:=M+N$, of the same dimensions, where each new element of the resultant matrix $S_{ij}$ is the result of summing corresponding position elements in $M$: in other words, $\forall\stab i,j,S:=M+N, \mtab S_{ij}=M_{ij}+N_{ij}$. So for example,
    $$
    \begin{bmatrix}
        3 & 2 \\
        -1 & 8 \\
        0 & 5
    \end{bmatrix} + 
    \begin{bmatrix}
        7 & -3 \\
        -1 & 0 \\
        0 & 2 
    \end{bmatrix} = 
     \begin{bmatrix}
        3+7 & 2-3 \\
        -1-1 & 8+0 \\
        0+0 & 5+2 
    \end{bmatrix}= \begin{bmatrix}
        10 & -1 \\
        -2 & 8 \\
        0 & 7
    \end{bmatrix}
    $$
    \item A matrix multiplying another matrix is more complicated. First of all, for two matrices $M$ and $N$ the matrix products $M\times N$ and $N\times M$ are rarely the same: whichever matrix is on the left of the product changes the result. We say that matrix multiplication is non-commutative. The terms left-multiply and right-multiply emerge naturally from this non-commutativity, where left-multiplying a matrix $M$ by a matrix $N$ means performing a matrix multiplication with that matrix $N$ on the left of the multiplication, and vice versa.
    \\\\
    Next, matrix multiplication can only be performed if the number of columns of the matrix on the left are the same as the number of rows of the matrix on the right. If we have an $a\times b$ matrix $M$ and a $c\times d$ matrix $N$, then the product $M\times N$ is only possible if $b=c$. The result can be best memorised by the crude representation:
    $$
    b=c\implies
    (a\times b) \times (c \times d) = (a\times \cancel{b}) \times (\cancel{b} \times d) = a\times d.
    $$
    Here, we get the idea that the `middle two' values when we list the dimensions of the two matrices being multiplied together in the correct order must match up and then are discarded, leaving the number of rows of the left matrix and the number of columns of the right matrix as the dimensions of the resulting matrix after multiplication.
    \\\\
    
\end{enumerate}
\section{Calculus*}
\section{Misc.*}