\chapter{Chapter 5: Commutation Relations and Simultaneous States}
Another question still remains. We know that state vectors represent states-- and that is, physical states. If we think to use our classical intuition, we know a physical state can be thought about from the perspective of one observable (such as when we consider the momentum of two colliding objects). However, that does not mean it does not possess any values for all other physical observables, as it still has energy, angular momentum, and so on: only that we are not currently considering the other observables. Similarly, we would not be particularly pleased if the physical states pertaining to a certain measurement of one specific observable-- these so called pure states-- suddenly contained absolutely no information on any other observables. This is a question of information encapsulation: how does an energy pure state store extractable information about momentum, for example? To understand this question, which is far more complicated than the classical one of ``it's there, and just hasn't been measured yet'', we treat the two seminal theorems on the matter. 
\\\\
It turns out that much of this information, on whether a state made hold certain values for both observables represented by a given pair of observable operators-- in short, whether it can be a \textbf{simultaneous state} for these two observables-- turns out to be related intimately with their \textit{commutation relation}. That is, if we define the \textbf{commutator}
$$
[\hat{A},\hat{B}] := \hat{A}\hat{B}-\hat{B}\hat{A}
$$
then the specific value of this commutator relates to our question of ``simultaneous states".
\\\\
Here, we explore more consequences of the commutation relation between two arbitrary observable operators. Along the way, we will also be able to practise the mathematical operations we have introduced at a level and intensity we have not had the occasion to do so far.
% \section{The Position and Momentum Operators}
% Very early on in classical mechanics, it was already established that two observables, position and momentum, are enough to completely specify the state of a single body. We will not concern ourselves with why that is true, but the same logic carries over to quantum mechanics. There are three operators which are by far the most important:
\section{Simultaneous states}
The question of ``simultaneous states'' is to investigate which states ($\leftrightarrow$ state vectors) can contain information on multiple observables at a time, and which observables these are. We have seen in the Stern Gerlach experiment that for example $x$ and $y$ spin simultaneous states are impossible, so this is a relevant, fully quantum in nature, problem.
\\\\
It turns out that the ability for different observables to have information represented in the same state vectors depends strongly on the commutator of their observable operators, as these in turn relate their orthonormal eigenvectors. To see this, there is one sweeping but simple theorem on operators for observables which can be measured simultaneously, and one dramatically anticlassical one for observables which cannot.
\subsection{The Compatibility Theorem}
Consider an unperturbed system, two physical observables, and three measurements ordered chronologically. The first and third measurements are for the first physical observable denoted $\mathcal{A}$, but the second measurement is for the second observable $\mathcal{B}$. We know from the Measurement Postulate that:
\begin{itemize}
    \item The first measurement forces the wavefunction into some pure eigenstate $\bm{\alpha_{i}}$ of the first physical observable operator $\hat{A}$.
    \item The second measurement forces the wavefunction into some pure eigenstate $\bm{\beta}_{i}$ of the second physical observable operator $\hat{B}$.
    \item If the second measurement of the different observable did not exist, then we would have successive measurements of the same state (which is, the operator acting on the same pure state the starting state vector was forced into following the first measurement) and we would expect with certainty the same measured value of $\mathcal{A}$ which is the eigenvalue $A_{i}$ associated with the pure state $\bm{\alpha_{i}}$.
\end{itemize}
The question is therefore whether or not this second measurement changes the result of the third. This is a profound question, because if it does, then we would conclude the simple act of measuring the second observable has moved the state vector out of the pure eigenstate it was in after the first measurement; that would then imply the second measurement is in itself a perturbation to the system: a confusing result. Indeed-- the reader will recognise that this is exactly the class of behaviour which appeared so surpisingly in the Stern Gerlach experiment.
\\\\ 
We define $\mathcal{A}$ and $\mathcal{B}$ to be \textbf{compatible observables} iff the first and third measurements yield the same value regardless of the starting state before measurement 1, and regardless of the value of the second observable measured in the second measurement. If we call the values measured $\mathcal{A}^{(1)}$, $\mathcal{B}^{(1)}$, $\mathcal{A}^{(2)}$, then observable $\mathcal{A}$ and $\mathcal{B}$ are compatible iff 
$$
\forall\:\Psi,\:\forall\:\mathcal{B}^{(1)},\:\:\mathcal{A}^{(1)}=\mathcal{A}^{(2)}.
$$
\underline{\textbf{Compatibility Theorem}}:\\\\
The following three conditions all equivalent to each other:
\begin{enumerate}
    \item $\mathcal{A}$ and $\mathcal{B}$ are compatible observables.
    \item $\hat{A}$ and $\hat{B}$ share a common eigenbasis.
    \item $\hat{A}$ and $\hat{B}$ commute.
\end{enumerate}
\\\\
\underline{Proof:}
\\\\
First we prove that $\hat{A}$ commutes with $\hat{B}$ iff they possess a common eigenbasis. \\\\ Consider two observable operators which commute, and define their eigenbases to be $\{\bm{\alpha}_{i}\}$ and $\{\bm{\beta}_{i}\}$. Now take an arbitrary eigenvector $\bm{\alpha}_{i}$ of $\hat{A}$ with eigenvalue $A_{i}$. We have
$$
\hat{A}\hat{B}=\hat{B}\hat{A}
$$
so we get 
$$
\hat{A}\hat{B}\bm{\alpha}_{i}=\hat{B}\hat{A}\bm{\alpha}_{i}=\hat{B}A_{i}\bm{\alpha}_{i}.
$$
However, we can now pull the constant eigenvalue out:
$$
\hat{A}(\hat{B}\bm{\alpha}_{i})=A_{i}(\hat{B}\bm{\alpha}_{i})
$$
so clearly $\hat{B}\bm{\alpha}_{i}$ is an eigenvector of $\hat{A}$ corresponding to eigenvalue $A_{i}$. Assuming that the eigenvalues are nondegenerate this implies that $\hat{B}(\bm{\alpha}_{i})$ coincides with $\bm{\alpha}_{i}$ as the eigenvalue $A_{i}$ has only one distinguishable eigenvector. The fact that 
$$
\hat{B}\bm{\alpha}_{i}\equiv\bm{\alpha}_{i}
$$
means we must have
$$
\hat{B}\bm{\alpha}_{i}=c\bm{\alpha}_{i}
$$
for some scalar multiple $c$. This means that $\bm{\alpha}_{i}$ is an eigenvector of $\hat{B}$ corresponding to eigenvalue $c$. So we can say that $\forall\:i, \bm{\alpha}_{i}$ is an eigenvector of $\hat{A}$ and $\hat{B}$: which means that they have the same eigenbasis. This isn't of course, to say, the eigenvalues are the same for $\hat{B}$ and $\hat{A}$ even though it may correspond to the same eigenvector (above, they are not the same unless $A_{i}=c$). Yet at the same time this is clearly helpful: if we know two physical observable operators commute and we have the eigenbasis of one then we automatically have an eigenbasis of the other. 
\\\\
Now, we prove it the other way around. Assume $\hat{A}$ and $\hat{B}$ both possess the eigenbasis $\{\gamma_{i}\}$: that is, the vectors $\bm{\gamma}_{i}$ are eigenvectors for both $\hat{A}$ and $\hat{B}$. We want to prove they commute. As they possess the same eigenbasis with eigenvalues $\{A_{i}\}$ and $\{B_{i}\}$ respectively, we can write
$$
\hat{A}\hat{B}\gamma_{i}=\hat{A}B_{i}\gamma_{i}=B_{i}\hat{A}\gamma_{i}=B_{i}A_{i}\gamma_{i}
$$
and the exact same applies for $\hat{B}\hat{A}\gamma_{i}$:
$$
\hat{B}\hat{A}\gamma_{i}=\hat{B}A_{i}\gamma_{i}=A_{i}\hat{B}\gamma_{i}=A_{i}B_{i}\gamma_{i}.
$$
Clearly, as $A_{i}$ and $B_{i}$ are constant eigenvalues, 
$$
A_{i}B_{i}\equiv B_{i}A_{i}.
$$
So this easily proves that two observable operators possessing the same eigenbasis must commute. Thus the implication works both ways and therefore two observable operators commute iff they share a common eigenbasis.
\\\\
Now to look at the practical definition: we are probably more interested in the concept of compatibility, as it concerns whether or not a measurement of a second observable in between measurements of a first observable will alter the measured results from the first measurement, effectively  forcing the state vector out of the pure eigenstate it was forced into. Let's first prove that two observables having common operator eigenbases is necessary and sufficient for the above defined definition of compatibility to hold.
\\\\
Start by considering two observables $\mathcal{A}$ and $\mathcal{B}$ represented by operators $\hat{A}$ and $\hat{B}$ respectively. Define the measurements to be $\mathcal{A}^{(1)},\mathcal{B}^{(1)}$, $\mathcal{A}^{(2)}$. For the observables to be compatible we need $\mathcal{A}^{(1)}$ to be the same as $\mathcal{A}^{(2)}$ regardless of the starting state and $\mathcal{B}^{(2)}$. Assume to begin with that the two operators $\hat{A}$ and $\hat{B}$ have the common eigenbasis $\{\gamma_{i}\}$. By definition the first measurement of $\mathcal{A}$ must force the state vector into a single eigenvector in the eigenbasis of the operator $\hat{A}$: that is, some $\gamma_{i}$ such that the measured value is for observable $\mathcal{A}$ the eigenvalue $A_{i}$. Next, measurement ${{B}}^{(1)}$ is the action of the operator $\hat{B}$ on the eigenvector $\gamma_{i}$. But by the Measurement Postulate of quantum mechanics,
$$
P(\bm{\alpha}_{i})=|\oip{\bm{\alpha}_{i}}{\Psi}|^2
$$
That is, the probability that the arbitrary operator $\hat{A}$ forces the state vector into an arbitrary eigenvector $\bm{\alpha}_{i}$ from its eigenbasis. Here, then, since the state vector has been forced into the eigenstate $\gamma_{i}$ by the first measurement, the probability the second measurement of the other observable $\mathcal{B}$ forces the state vector into the same eigenstate is:
$$
P(\gamma_{i})=|\oip{\gamma_{i}}{\gamma_{i}}|^2=1
$$
where we assume as per usual that the eigenvectors $\gamma_{i}$ have been normalised. So we can say that measurement B will not alter the eigenstate the state vector is in and therefore the third measurement will follow the same logic to yield the exact same value, the eigenvalue $A_{i}$ corresponding to $\gamma_{i}$. Thus, if two observable operators possess the same eigenbasis, they are compatible observables.
\\\\
If the observables are compatible then this implies their operators have the same eigenbasis. The proof for this is simple. If the observables $\mathcal{A}$ and $\mathcal{B}$ are compatible then for the successive measurements $\mathcal{A}^{(1)},\mathcal{B}^{(1)},\mathcal{A}^{(2)}$ the measured values for $\mathcal{A}^{(1)}$ and $\mathcal{A}^{(2)}$ must be the same. The measurement $\mathcal{A}^{(1)}$ must have forced the wavefunction into an eigenvector of $\hat{A}$, some arbitrary $\bm{\alpha}_{i}$. Then, the measurement $\mathcal{B}^{(1)}$ must force the wavefunction into some arbitrary eigenvector $\bm{\beta}_{i}$ of the operator $\hat{B}$. However, the final measurement must yield the same result as the first if the observables are compatible, which is, the same eigenvalue corresponding to the same eigenvector $\bm{\alpha}_{i}$ of operator $\hat{A}$ as it originally was in. The probability that the measurement forces the wavefunction, currently in the eigenstate $\bm{\beta}_{i}$ of $\hat{B}$ as the measurement $\mathcal{B}^{(1)}$ has just been performed, into the same eigenstate $\bm{\alpha}_{i}$ as originally measured is:
$$
P(\bm{\alpha}_{i})=|\oip{\bm{\alpha}_{i}}{\bm{\beta}_{i}}|^2.
$$
However, if these observables are to be compatible, the final measurement must with certainty yield the eigenvalue $A_{i}$ again and therefore the above probability of measurement $\mathcal{A}^{(2)}$ forcing it back into the original eigenstate must be 1. So 
$$
|\oip{\bm{\alpha}_{i}}{\bm{\beta}_{i}}|^2=1 \Rightarrow\:\: \bm{\alpha}_{i}\equiv\bm{\beta}_{i}
$$
and therefore their eigenbases must be the same as the above holds true for any arbitrary $\bm{\alpha}_{i}$ and corresponding $\bm{\beta}_{i}$ from the measurements.
\\\\
The Compatibility Theorem is now complete. We have shown that:
\begin{itemize}
    \item Two operators commuting is necessary and sufficient for them to possess a common eigenbasis.
    \item Two operators possessing a common eigenbasis is necessary and sufficient for the two observables they represent to be compatible.
    \item Therefore, two observable operators commuting is also necessary and sufficient for them to represent compatible observables.
\end{itemize}
The logical implications of these facts all run three ways.
\\\\
While we have now seen facts about compatible observables, an example of incompatible observables sticks in our mind-- that of the Stern Gerlach experiment. We saw exactly that $x$ and $y$ spins were incompatible, because measuring the $x$ spin in between two $y$ measurements stopped the second $y$ measurement from being the same as the first with certainty-- which is to say, we now know, that the measurement of $x$ spin forced it out of the eigenstate of $y$ spin it had been previously forced into. All the questions about the quantum state raised by the Stern Gerlach experiment will finally come to an end with this section. We would like to formalise our understanding of how incompatibility affected the experiment. To explain it all, we witness-- and prove ourselves-- one of Physics' most groundbreaking and shocking theorems, the famous \textbf{Heisenberg Uncertainty Principle}.
\section{The Heisenberg Uncertainty Principle}
The idea of commuting observable operators being necessary and sufficient for the two observables they represent to be compatible is a very important one for the question of simultaneous states, and has been shown above. Now we must surely consider when two observable operators do not commute: in other words, when they represent \textbf{incompatible} observables. One of the most important and dramatic results of all quantum mechanics, the Heisenberg Uncertainty Principle, results when we carry out some elegant mathematics to investigate this problem. Before we begin the statement and proof, let us define the commutator between two operators to be 
$$
[\hat{A},\hat{B}]:=\hat{A}\hat{B}-\hat{B}\hat{A}
$$
so that if we have two commuting operators $\hat{A}$ and $\hat{B}$, then 
$$
[\hat{A},\hat{B}]=\hat{A}\hat{B}-\hat{B}\hat{A}=0
$$
since $\hat{A}\hat{B}=\hat{B}\hat{A}$ iff they commute. For operators which do not commute, their commutator may take a wide variety of forms: which is why it is useful under universal convention to have this shorthand.
\begin{tcolorbox}
\textbf{\underline{Heisenberg Uncertainty Principle}}\\\\
For any state $\Psi_{t}$,
$$
\Delta A_{t}\Delta B_{t}\geq\frac{1}{2}|\oip{\Psi_{t}}{[\hat{A},\hat{B}]\Psi_{t}}|
$$
where $\Delta A_{t}$ is the standard deviation of measurable values of observable $\mathcal{A}$ at time $t$: which is therefore a measure of uncertainty for these variables.
\end{tcolorbox}
\underline{\textbf{Proof:}}\\\\
We will continue to refer to arbitrary observables $\mathcal{A}$ and $\mathcal{B}$ for the proof; all the proof is relevant at any instant of time and so time subscripts will be eschewed. The notation $\Delta A$ refers to the standard deviation of the measurements of observable $\mathcal{A}$; this standard deviation is no different from the statistical definition:
$$
\Delta A=\sqrt{\langle \hat{A}^2\rangle-\langle \hat{A}\rangle^2}
$$
where the symbol $\langle X\rangle$ is the expected value of the variable $X$, as seen in the probability preliminary. First we note that this principle is valid for compatible observables: as compatible observables, their operators must commute. Thus
$$
[\hat{A},\hat{B}]=0 \Rightarrow\:\: \Delta A_{t}\Delta B_{t}\geq\frac{1}{2}|\oip{\Psi_{t}}{[\hat{A},\hat{B}]\Psi_{t}}| = \frac{1}{2}|\oip{\Psi_{t}}{0} |=0. 
$$
So for compatible observables, 
$$
\Delta A_{t}\Delta B_{t}\geq 0
$$
which is neither interesting nor invalid at all since the standard deviation of any measurement can never be negative. Now, we will prove this for all physical operators, regardless of whether they commute.
\\\\
\underline{\textbf{Lemma 1:}}\\
Any operator $\hat{X}':=\hat{X}-\qexp{\hat{X}}$ where $\hat{X}$ is a Hermitian physical operator is also Hermitian.\\\\
\underline{\textbf{Proof:}}\\\\
Recall that the definition for an expected value of a variable is the sum of its possible values multiplied by the probabilities of the variable taking those values. Therefore, we can say that, over the eigenbasis $\{\xi_{i}\}$ of $\hat{X}$ with eigenvalues $\{X_{i}\}$, 
$$
\qexp{\hat{X}}=\sum_{\{i\}}P(\xi_{i})X_{i},
$$
but by our knowledge of the previous postulates we can describe the probability more precisely: the measurement postulate defines this to be
$$
\qexp{\hat{X}}=\sum_{\{i\}}X_{i}|\oip{\xi_{i}}{\Psi}|^2.
$$
Our job is to prove that the operator $\hat{X}':=\hat{X}-\qexp{\hat{X}}$ is hermitian if $\hat{X}$ is hermitian for all quantum operators. That is, we need to prove that:
$$
\oip{\Psi_{1}}{\hat{X}'\Psi_{2}}=\oip{\hat{X}'\Psi_{1}}{\Psi_{2}}
$$
for all Hilbert space functions $\Psi_{1}$ and $\Psi_{2}$. The operator $\hat{X}$ must be hermitian as $\hat{X}$ is defined to be a quantum operator corresponding to a physical observable. Meanwhile, the expectation value
$$
\qexp{\hat{X}}=\sum_{\{i\}}X_{i}|\oip{\xi_{i}}{\Psi}|^2.
$$
is clearly a real scalar, as the probabilities, which are square moduli, will all be real numbers and so will each eigenvalue of the hermitian operators. Therefore, 
$$
\oip{\hat{X}\Psi_{1}}{\Psi_{2}}\equiv\oip{\Psi_{1}}{\hat{X}\Psi_{2}}
$$
and 
$$
\oip{\qexp{\hat{X}}\Psi_{1}}{\Psi_{2}}\equiv\oip{\Psi_{1}}{\langle\hat{X}\rangle\Psi_{2}}\equiv\qexp{\hat{X}}\oip{\Psi_{1}}{\Psi_{2}}
$$
so for any physical operator $\hat{X}$ the defined operator $\hat{X}'$ is the sum of two hermitian operators. So
$$
\begin{aligned}
\oip{\hat{X}'\Psi_{1}}{\Psi_{2}}&=\oip{[\hat{X}-\langle\hat{X}\rangle]\Psi_{1}}{\Psi_{2}}=\oip{\hat{X}\Psi_{1}}{\Psi_{2}}-\oip{\langle\hat{X}\rangle\Psi_{1}}{\Psi_{2}}\\
&=\oip{\Psi_{1}}{\hat{X}\Psi_{2}}-\oip{\Psi_{1}}{\langle\hat{X}\rangle\Psi_{2}}\\
&=\oip{\Psi_{1}}{\hat{X}'\Psi_{2}}
\end{aligned}
$$
using the linear properties of the inner product. Thus, the operator $\hat{X}'$ is Hermitian for any physical operator. Therefore, defining $\hat{A'}:=\hat{A}-\qexp{\hat{A}}$ and $\hat{B}':=\hat{B}-\qexp{\hat{B}}$ for the purpose of the problem also gives us two hermitian operators. $\square$
\\\\
The commutator in the generalised principle might give pause with regards to the development of these new operators, but, importantly,
$$
[\hat{A}',\hat{B}']=[\hat{A},\hat{B}].
$$
This fact can be proved quite simply:
$$
\begin{aligned}
[\hat{A}',\hat{B}']&= \hat{A}'\hat{B}'- \hat{A}'\hat{B}'\\
&= (\hat{A}-\qexp{\hat{A}})(\hat{B}-\qexp{\hat{B}})-(\hat{B}-\qexp{\hat{B}}) (\hat{A}-\qexp{\hat{A}})\\
&=(\hat{A}\hat{B}-\hat{A}\qexp{\hat{B}}-\qexp{\hat{A}}\hat{B}-\qexp{\hat{A}}\qexp{\hat{B}})-(\hat{B}\hat{A}-\hat{B}\qexp{\hat{A}}-\qexp{\hat{B}}\hat{A}-\qexp{\hat{B}}\qexp{\hat{A}})
\end{aligned}
$$
but as the expectation values $\qexp{\hat{A}}$ and $\qexp{\hat{B}}$ are real scalars it is clear that $\qexp{\hat{A}}\qexp{\hat{B}}=\qexp{\hat{B}}\qexp{\hat{A}}$, and $\qexp{\hat{A}}\hat{B}=\hat{B}\qexp{\hat{A}}$ and vice versa swapping the $A$ and $B$ around. So the terms cancel out and we are left with
$$
[\hat{A}',\hat{B}']=\hat{A}\hat{B}-\hat{B}\hat{A}:=[\hat{A},\hat{B}]. \:\:\square
$$
Now, one last important lemma:\\\\
\underline{\textbf{Lemma 2:}}\\
$$\oip{\hat{A}'\Psi}{\hat{A}'\Psi}=(\Delta\hat{A})^2$$
\\\\
\underline{\textbf{Proof:}}\\
By the Hermiticity of $\hat{A}'$,
$$
\oip{\hat{A}'\Psi}{\hat{A}'\Psi}=\oip{\Psi}{([\hat{A}']^2\Psi}.
$$
Expanding the definition,
$$
\begin{aligned}
\oip{\hat{A}'\Psi}{\hat{A}'\Psi}&=\oip{\Psi}{\hat{A}'^2\Psi}\\
&=\oip{\Psi}{[\hat{A}-\qexp{\hat{A}}][\hat{A}-\qexp{\hat{A}}]\Psi}\\
&=\oip{\Psi}{[\hat{A}^{2}]\Psi-2\qexp{\hat{A}}\hat{A}\Psi+\qexp{\hat{A}}^2\Psi}\\
&=\oip{\Psi}{[\hat{A}^{2}]\Psi}-2\qexp{\hat{A}}\oip{\Psi}{\hat{A}\Psi}+\qexp{\hat{A}}^2\oip{\Psi}{\Psi}\\
&=\langle\hat{A}^2\rangle\oip{\Psi}{\Psi}-2\qexp{\hat{A}}\qexp{\hat{A}}\oip{\Psi}{\Psi}+\qexp{\hat{A}}^2\oip{\Psi}{\Psi}\\
&= \langle\hat{A}^2\rangle-2\qexp{\hat{A}}\qexp{\hat{A}}+\qexp{\hat{A}}^2\\
&=\langle\hat{A}^2\rangle-\qexp{\hat{A}}^2\\
&=(\Delta\hat{A})^2 \:\:\:\:\:\:\square
\end{aligned}
$$
Now we can use these lemmas to prove the problem. We want to prove that 
$$
\Delta{A}\Delta{B}\geq\frac{1}{2}|\oip{\Psi}{[\hat{A},\hat{B}]\Psi}|
$$
at all times $t$. We start by replacing $[\hat{A},\hat{B}]$ with $[\hat{A}',\hat{B}']$. Then, we have,
$$
\oip{\Psi}{[\hat{A},\hat{B}]\Psi}=\oip{\Psi}{[\hat{A}',\hat{B}']\Psi}=\oip{\Psi}{[\hat{A}'\hat{B}'-\hat{B}'\hat{A}']\Psi}.
$$
This is, 
$$
\oip{\Psi}{[\hat{A},\hat{B}]\Psi}=\oip{\Psi}{\hat{A}'\hat{B}'\Psi}-\oip{\Psi}{\hat{B}'\hat{A}'\Psi}
.$$
We can rearrange this by the hermiticity of $\hat{A}'$ and $\hat{B}'$:
$$
\oip{\Psi}{[\hat{A},\hat{B}]\Psi}=\oip{\hat{A}'\Psi}{\hat{B}'\Psi}-\oip{\hat{B}'\Psi}{\hat{A}'\Psi}=\oip{\hat{A}'\Psi}{\hat{B}'\Psi}-\oip{\hat{A}'\Psi}{\hat{B}'\Psi}^{\ast}
$$
so this is
$$
\oip{\Psi}{[\hat{A},\hat{B}]\Psi}=2i\text{Im}\left(\oip{\hat{A}'\Psi}{\hat{B}'\Psi}\right)
$$
according to rudimentary arithmetic of complex numbers. Then, the expression we need is
$$
\frac{1}{2}|\oip{\Psi}{[\hat{A},\hat{B}]\Psi}|\leq\frac{1}{2}\times2|\oip{\hat{A}'\Psi}{\hat{B}'\Psi}|=|\oip{\hat{A}'\Psi}{\hat{B}'\Psi}|.
$$
This is because of the above expression for $\oip{\Psi}{[\hat{A},\hat{B}]\Psi}$ and the fact that the modulus of the imaginary part of a scalar cannot be greater than the modulus of the scalar (Exercise 1.3.2a). Then, by Lemma 2
$$
\oip{\hat{A}'\Psi}{\hat{A}'\Psi}=(\Delta\hat{A})^2\Rightarrow\:\:\Delta\hat{A}=\sqrt{\oip{\hat{A}'\Psi}{\hat{A}'\Psi}}.
$$
So 
$$
\Delta\hat{A}\Delta\hat{B}=\sqrt{\oip{\hat{A}'\Psi}{\hat{A}'\Psi}}\sqrt{\oip{\hat{B}'\Psi}{\hat{B}'\Psi}}.
$$
By Cauchy-Schwartz, 
$$
\sqrt{\oip{\hat{A}'\Psi}{\hat{A}'\Psi}}\sqrt{\oip{\hat{B}'\Psi}{\hat{B}'\Psi}}\geq|\oip{\hat{A}'\Psi}{\hat{B}'\Psi}|
$$
and so, conclusively,
$$
\Delta\hat{A}\Delta\hat{B}=\sqrt{(\hat{A}'\Psi,\hat{A}'\Psi)}\sqrt{(\hat{B}'\Psi,\hat{B}'\Psi)}\geq|\oip{\hat{A}'\Psi}{\hat{B}'\Psi}|\geq\frac{1}{2}|\oip{\Psi}{[\hat{A},\hat{B}]\Psi}|
$$
so 
$$
\Delta\hat{A}\Delta\hat{B}\geq\frac{1}{2}|\oip{\Psi}{[\hat{A},\hat{B}]\Psi}|.
$$
This proves Heisenberg's Uncertainty Principle. $\square$
\\\\
This general form we have above is still difficult to interpret, but if we consider a few examples we will realise this is a very important result. One of the most famous iterations comes with considering simply the two central operators of quantum mechanics: the position and momentum operators, which we have not yet introduced but will for now just use for calculation purposes. We can calculate the commutator:
$$
\begin{aligned}
&[\hat{X},\hat{P}]=\hat{X}\hat{P}-\hat{P}\hat{X}\\
\Rightarrow\:\:&[\hat{X},\hat{P}]\Psi(x)=-xi\hbar\frac{\partial}{\partial x}\Psi(x)--i\hbar\frac{\partial}{\partial x}\biggl(x\Psi(x)\biggr)\\
\Rightarrow\:\:&[\hat{X},\hat{P}]\Psi(x)=-i\hbar x\frac{\partial \Psi}{\partial x}--i\hbar\biggl[\frac{dx}{dx}\Psi(x)+x\frac{\partial\Psi}{\partial x}\biggr]\\
\Rightarrow\:\:&[\hat{X},\hat{P}]\Psi(x)=-i\hbar\biggl[x\frac{\partial\Psi}{\partial x}-\Psi(x)-x\frac{\partial\Psi}{\partial x}\biggr]\\
\Rightarrow\:\:&[\hat{X},\hat{P}]\Psi(x)=i\hbar\Psi(x)\\
\Rightarrow\:\:&[\hat{X},\hat{P}]\equiv i\hbar.
\end{aligned}
$$
After this, if we plug this into the Generalised Uncertainty principle and assume that the wavefunction is normalised,
$$
\Delta{\hat{X}}\Delta{\hat{P}}\geq\frac{1}{2}|\oip{\Psi}{i\hbar\Psi}| \Rightarrow \Delta{\hat{X}}\Delta{\hat{P}}\geq\frac{1}{2}|i\hbar\oip{\Psi}{\Psi}|=\frac{1}{2}|i\hbar|=\frac{1}{2}\sqrt{-i\hbar\times i\hbar}=\frac{\hbar}{2}.
$$
The key end result is that 
$$
\Delta{\hat{X}}\Delta{\hat{P}}\geq\frac{\hbar}{2}.
$$
This is the most well known case of the Uncertainty Principle in action, but we can see that the Generalised Uncertainty Principle can be applied more broadly than just to the two observables of position and momentum. 
\\\\
Returning to our considerations of the physical results of trying to measure two incompatible observables, it is clear how bizarre this result is. Consider if we have just made a measurement for the position of a particle. Then we have forced its wavefunction into a position eigenstate and therefore we can say that the uncertainty in the position is now $0$: we know the successive measurement must yield the same position value with probability 1. However, if we plug in $\Delta{\hat{X}}$ into the Uncertainty Principle we get 
$$
0\times\Delta\hat{P}\geq\frac{\hbar}{2}
$$
which implies somehow that the uncertainty in momentum must be infinite! 
\\\\
Thus, Heisenberg's Uncertainty Principle tells us that if we know the value of the position with certainty we are completely have infinite uncertainty in momentum. The relationship works both ways so the same applies for the momentum: if we know the momentum of a particle then we necessarily have infinite uncertainty in the position of the particle and we have not a clue where it is. This is undoubtedly one of the most anti-classical results in quantum mechanics, and yet it results beautifully from the mathematics we have defined (and has never been experimentally refuted). If nothing else, it should now be clear that the mathematical manipulations of quantum mechanics are rich and impactful.
\\\\
The same is manifested, of course, in the Stern-Gerlach experiment. By knowing $x$ spin, we had infinite uncertainty in $y$ spin- with absolutely no way to tell if an electron would be up or down spin. By knowing the $y$ spin, we had infinite uncertainty in the $x$ spin. This is one example of an experimental verification of the Heisenberg Uncertainty Principle.
\\\\
\section{Formulation of Observable Operators}
We have discussed in Chapter 4 the fact that the specific functional form of an operator, just like the specific expansion of a state vector, depends on the basis we are working on. Nevertheless, we would like to ask the question of how to form the observable operator for a given physical observable. Otherwise, our discussions above on Compatibility and Incompatibility, specifically via the study of commutation relations, would be completely useless, as we would never be able to specify the commutator at all!
\subsection{Dirac's Canonical Commutation Relation}
Very early on in classical mechanics, it was already established that two observables, position, $x$ and momentum, $p$, are enough to completely specify the state of a single body. We will not concern ourselves with why that is true, but the same logic carries over to quantum mechanics. Position and momentum are, it turns out, the only two operators we need to generate any new observable operator, at least for observables which have classical counterparts.
\\\\
The form of the position and momentum operators, in position space, are:
$$
\hat{X}\psi(x) = x\psi(x)
$$
and
$$
\hat{P}\psi(x) = -i\hbar \frac{\partial}{\partial x}\psi(x)
$$
There are a few things to note with these forms:
\begin{enumerate}
    \item Note that we use $\psi(x)$ instead of the usual notation for the general state vector, $\Psi$. This is because we are working in position space, so the position/momentum operators in the position space must act on the position space expansion $\psi(x)$ of $\Psi$.
    \item The position space expansion, of course, can be written as $\psi(x)$ since we explicitly defined it as the  map from basis vectors to the components of the state vector in those basis directions. Thus, everything depends on an input eigenstate of $\hat{X}$; hence, we use $x$ to denote this.
    \item These forms would not be the same in, for example, momentum space! The reason why we use the position space form is because, as aforementioned, the majority of quantum problems are solved in position space.
\end{enumerate}
How do we know these forms? Are they postulates? The answer is that they are \textit{derived} from a postulate. The postulate which really generates these forms is Dirac's Canonical Commutation Relation:
\\\\
\textbf{\underline{Dirac's Canonical Commutation Relation}}
\\\\
$$
[\hat{X},\hat{P}]=i\hbar
$$
where $i\hbar$ is the notation for the scalar $i\hbar$ multiplying the identity operator.
\\\\
We already quoted this commutation relation, when we plugged it into the Heisenberg Uncertainty Principle. Here, the reader understands that it is a postulate. However, this simple postulate remarkably generates the forms of nearly all observable operators!
\\\\
Indeed, from this seemingly small postulate, we can \textit{deduce} the position space (and momentum space) forms of the position operator $\hat{X}$ and $\hat{P}$ shown above. The full derivation will come in Chapter 8, since we require the tools developed there to handle continuous observables, but for now we can take the position space form of the operators for granted.
\\\\
With this in mind, we can introduce the next Postulate, which tells us how to form the observable operator of any observable, within a given basis.
\subsection{Formation of Observable Operators}
\Answer
\textbf{\underline{Postulate 4: The Formation of Observable Operators}}
\\\\
The two canonical operators in quantum mechanics, expressed in position space, are the position operator:
$$
\hat{X}\Psi=x\Psi
$$
and the momentum operator:
$$
\hat{P}\Psi=-i\hbar\frac{\partial}{\partial x}\Psi.
$$  
which can be both shown to be Hermitian. To form any other physical observable operator, express the classical observable in terms of the classical variables of $x$ (position) and $p$ (momentum), and replace the $x$ terms with the operator $\hat{X}$ and the $p$ terms with the operator $\hat{P}$. 
\Answerend
Note that the position operator and momentum operator are here one dimensional: if instead we were considering position and momentum in three directions then all we would do would be to replace the $x$ with the $y$ and $z$ variables for the $y$ and $z$ directional operators instead. Generalising up physical dimensions (as in, position dimensions) is only marginally more complicated because we have more terms to consider, and otherwise this rationale stays the same and everything is straightforward. Let us, to punch this in, consider the kinetic energy operator, which will become very useful. Note again that everything is happening in position space, or things would look quite different in another space with a different eigenbasis.
\subsubsection{Kinetic Energy Operator}
The classical formula for kinetic energy is of course
$$
KE=\frac{1}{2}mv^2.
$$
We want this in terms of position and momentum. It is important that mass is treated as a constant so does not interfere in this as a separate observable. The kinetic energy formula expressed in momentum and position is 
$$
KE=\frac{1}{2}mv^2=\biggl(\frac{m}{2m}\biggr)mv^2=\frac{m^2v^2}{2m}
$$
which is, 
$$
KE=\frac{p^2}{2m}
$$
where position does not appear in this particular observable expression, which is completely fine. Then, by the postulate, all we need to do is to replace the classical variable $p$ with the momentum operator:
$$
KE=\frac{p^2}{2m}\duac \hat{KE}=\frac{\hat{P}^2}{2m}.
$$
The notation $\hat{KE}$ looks foolish and so convention uses $\hat{T}$ instead. The expression $\hat{P}^2$, we know, means applying the same momentum operator twice.
\\\\
Therefore, in position space, where the momentum operator is 
$$
\hat{P}\Psi=-i\hbar\frac{\partial}{\partial x}\Psi,
$$
we get 
$$
\hat{T}\Psi=\frac{\hat{P}^2}{2m}\Psi=-\frac{i\hbar}{2m}\biggl(\frac{\partial}{\partial x}\biggl(-i\hbar\frac{\partial\Psi}{\partial x}\biggr)\biggr)
$$
which is, 
$$
\hat{T}{\Psi}=\frac{(i\hbar)^2}{2m}\frac{\partial^2 \Psi}{\partial x^2}=-\frac{\hbar^2}{2m}\frac{\partial^2 \Psi}{\partial x^2}.
$$
Thus we are done, and have the position space form of the kinetic operator:
$$
\hat{T}\Psi=-\frac{\hbar^2}{2m}\frac{\partial \Psi}{\partial x^2}.
$$
\subsection{Mass and Time}
There is a final note to make, on mass and time. Both of these are not treated as observables in quantum mechanics. This makes intuitive sense for time: in experiments we would measure the duration of an event, but this is first of all not something which pertains to the state problem but more importantly not something we expect to involve probability. The time at which we are measuring the time obviously should not be probabilistic; it is therefore treated more like an underlying variable just like the space of the universe. 
\\\\
With mass, the complete justification is more nuanced. It turns out that particle physics shows the existence of so-called ``fundamental particles", and that everything is made up of these particles. Then, we create a notion of mass simply based on the ``quantum number" of those particles: which turns out to be impossible to change, hence the idea of being `fundamental'. Since mass is built up concretely from this bedrock of fundamental properties, then, it is not subject to the probabilistic superpositions and uncertainties of other observables. Hence, it may also be treated as a constant in a given stationary equation.
\\\\
The best thing to do, then, is to treat $m$ as a constant and $t$ as a varying but non-probabilistic parameter. In both cases, they are not really observables and thus do not require an observable operator to be linked to them.
\section{Summary}
In this chapter, we finished addressing all the uniquely quantum behaviour the Stern-Gerlach experiment threw at us. We proved the incredible Heisenberg Uncertainty Principle, as well as the less explosive but equally insightful Compatibility Theorem, and showed that the mathematical manipulations of quantum mechanics, specifically herein with respect to the commutation relations between two observable operators, can really yield very rich conceptual and algebraic connections. Finally, we also explicitly showed how to construct observable operators without needing to postulate the form of every single one, even though this story is still to be completed more carefully in Chapter 7 and Chapter 8.

