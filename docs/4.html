<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>4</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <script>
    window.MathJax = {
      loader: { load: ['[tex]/newcommand', '[tex]/boldsymbol'] },
      tex: {
        packages: { '[+]': ['base', 'ams', 'newcommand', 'boldsymbol'] },
        macros: {
          bm: ["\\boldsymbol{#1}", 1]  // Define \bm{} using \boldsymbol{}
        },
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


      
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
    $$
    \newcommand{\Answer}{\begin{tcolorbox}}
    \newcommand{\Answerend}{\end{tcolorbox}}
    \newcommand{\ket}[1]{|#1\rangle}
    \newcommand{\bra}[1]{\langle#1|}
    \newcommand{\ip}[2]{\langle#1|#2\rangle}
    \newcommand{\bip}[2]{\left\langle#1\middle|#2\right\rangle}
    \newcommand{\qexp}[1]{\langle#1\rangle}
    \newcommand{\apos}[1]{``#1"}
    \newcommand{\sapos}[1]{`#1'}
    \newcommand{\elec}{e^{-}}
    \newcommand{\uspin}{(\uparrow)}
    \newcommand{\dspin}{(\downarrow)}
    \newcommand{\lspin}{(\leftarrow)}
    \newcommand{\rspin}{(\rightarrow)}
    \newcommand{\ulspin}{(\uparrow\leftarrow)}
    \newcommand{\urspin}{(\uparrow\rightarrow)}
    \newcommand{\dlspin}{(\downarrow\leftarrow)}
    \newcommand{\drspin}{(\downarrow\rightarrow)}
    \newcommand{\R}{\mathbb{R}}
    \newcommand{\stab}{\:\:}
    \newcommand{\mtab}{\:\:\:}
    \newcommand{\btab}{\:\:\:}
    \newcommand{\imp}{\Rightarrow}
    \newcommand{\doubimp}{\Leftrightarrow}
    \newcommand{\setof}[1]{\{#1\}}
    \newcommand{\infint}{\int_{-\infty}^{\infty}}
    \newcommand{\trans}[1]{\mathcal{T}(#1)}
    \newcommand{\dd}[2]{\delta(#1-#2)}
    \newcommand{\ipbig}[2]{\langle#1|#2\rangle}
    \newcommand{\talpha}{\tilde{\alpha}}
    \newcommand{\op}[2]{|#1\rangle\langle#2|}
    \newcommand{\sop}[1]{|#1\rangle\langle#1|}
    \newcommand{\prop}[2]{\mathcal{U}(#1,#2)}
    \newcommand{\propdagg}[2]{\mathcal{U}^{\dagger}(#1,#2)}
    \newcommand{\sip}[1]{\langle#1|#1\rangle}
    \newcommand{\optrip}[3]{\langle#1|\hat{#2}|#3\rangle}
    \newcommand{\nhoptrip}[3]{\langle#1|{#2}|#3\rangle}
    \newcommand{\northexp}[2]{\sum_{i=1}^{n}|#2\rangle\langle#2|#1\rangle}
    \newcommand{\orthexp}[4]{\sum_{#3=1}^#4|#2\rangle\langle#2|#1\rangle}
    \newcommand{\schrodeq}{i\hbar\frac{\partial \Psi(x,t)}{\partial t}=\hat{H}\Psi(x,t)}
    \newcommand{\nd}[2]{\frac{d#1}{d #2}}
    \newcommand{\snd}[2]{\frac{d^{2}#1}{d#2^2}}
    \newcommand{\pd}[2]{\frac{\partial#1}{\partial #2}}
    \newcommand{\spd}[2]{\frac{\partial^{2}#1}{\partial #2^2}}
    \newcommand{\duac}{\leftrightarrow}
    \newcommand{\oip}[2]{\left(#1,#2\right)}
    \newcommand{\obip}[2]{\left(#1,#2\right)}
    $$
<h1
id="chapter-4-state-space-operators-and-the-quantum-observables-problem">Chapter
4: State Space Operators and the Quantum Observables Problem</h1>
<p>The previous chapter introduced the concept of Hilbert space vectors,
called state vectors, representing physical states. The state space,
like any infinite-dimensional vector space, has infinite bases, and this
was claimed to be a motivating factor for the Hilbert space formulation
in the first place: because it allows for the state represented by the
state vector to be looked at from the perspective of different
observables like momentum and position. To complete our understanding of
how the state problem is approached in quantum mechanics we need to
understand exactly how observables are incorporated into this linear
algebraic vector space system, through very specific bases. This
question will be answered by an investigation into <strong>linear
operators</strong>.</p>
<h2 id="hilbert-space-operators">Hilbert Space Operators</h2>
<p>An operator is a function, but it is the lexicon used to describe a
sort of function which acts on functions or vectors as inputsâ€“ here, in
the state space, on state vectors. For a new student of quantum
mechanics it is paramount to understand that the distinction between an
operator and a function does not exist. The only reason the term
operator is used in conjunction with vector spaces is to avoid
exhausting the term function, since operators act on state vectors which
are themselves linked already to wavefunctions. If unused to the term,
there is a harmful tendency to give a mystic image to operators, but
this must not be done. An operator, just like a function, takes an input
and maps it to an output, and that is the extent of it.<br />
<br />
For example, one operator the reader will have seen already is the
differential operator: <span
class="math display">\[\frac{d}{dx}f(x)=f&#39;(x).\]</span> This takes
some input function <span class="math inline">\(f(x)\)</span>, and maps
it to an output function <span class="math inline">\(f&#39;(x)\)</span>.
In that sense, it is a function because it has an input and output, but
it is an operator because it acts on functions. If the input was <span
class="math inline">\(f(x)=x^2\)</span> then the output of the
differential operator would be <span class="math inline">\(2x\)</span>;
if it were <span class="math inline">\(e^x\)</span> then its output
would also be <span class="math inline">\(e^x\)</span>. A scalar can
also be an input of the operator: since an operator is a function of
functions, we could just define the input function to be <span
class="math inline">\(f(x)\equiv k\)</span> for that scalar <span
class="math inline">\(k\)</span>, and of course this would still be a
function whilst sharing no differences to the scalar. Here, if such a
scalar was an input of the differential operator, than the output would
be the null function: or, <span class="math inline">\(0\)</span>, but
for other operators, this may be different.<br />
<br />
If this is understood, the technical requirement for an operator which
is linear, here donated by <span class="math inline">\(\Omega\)</span>,
is that they are linear maps: <span class="math display">\[\Omega:
\mathscr{H} \mapsto \mathscr{H},\:\:\:\: \Omega(\Psi_{1}+\Psi_{2})\equiv
\Omega\Psi_{1}+\Omega\Psi_{2}.\]</span> There do exist non-linear
operators, but in this book, and much of quantum mechanics, we will only
ever encounter linear operators, and so we can take the latter linear
distributive property to be a given for our work. We also have:</p>
<ul>
<li><p>Associativity of scalar multiplication: <span
class="math display">\[(c\Omega)\Psi=c(\Omega\Psi).\]</span></p></li>
<li><p>Distributivity: <span
class="math display">\[(\Omega_{1}+\Omega_{2})\Psi=\Omega_{1}\Psi +
\Omega_{2}\Psi.\]</span></p></li>
<li><p>Associativity in operators: <span
class="math display">\[\Omega_{1}(\Omega_{2}\Psi)=\Omega_{1}\Omega_{2}\Psi\]</span></p></li>
</ul>
<p>We do not, however have commutativity: <span
class="math display">\[\Omega_{1}\Omega_{2}\Psi\neq\Omega_{2}\Omega_{1}\Psi\]</span>
for most pairs of operators <span
class="math inline">\(\Omega_{1}\)</span> and <span
class="math inline">\(\Omega_{2}\)</span> (some do by chance). Moreover,
there is in general no useful relation between <span
class="math inline">\(\Omega_{1}\Omega_{2}\)</span> and <span
class="math inline">\(\Omega_{2}\Omega_{1}\)</span>, unlike the inner
product, which is not commutative, but which follows the much nicer
relation <span class="math display">\[(\Psi_{1},\Psi_{2}) =
(\Psi_{2},\Psi_{1})^{\ast}.\]</span> The general irregularity of
commutation relations between linear operators will turn out to be
connected to an incredibly rich set of quantum phenomena!<br />
<br />
The rest of the operator facts again should, like the rules of vector
spaces, come mostly naturally. A useful point to remember is that for
any operator we work with in quantum mechanics, it maps from the state
space <span class="math inline">\(\mathscr{H}\)</span> to the state
space <span class="math inline">\(\mathscr{H}\)</span>: in other words,
performing that operator on any input state space vector will directly
create a new state space vector! Thus <span
class="math display">\[\Omega\Psi\]</span> is a new state space vector,
and so is <span
class="math display">\[\Omega_{1}\Omega_{2}\Psi=\Omega_{1}(\Omega_{2}\Psi)\]</span>
which is, the action of <span class="math inline">\(\Omega_{1}\)</span>
on the state space vector <span
class="math inline">\(\Omega_{2}\Psi\)</span> obtained after operating
with <span class="math inline">\(\Omega_{2}\)</span> on an original
state vector <span class="math inline">\(\Psi\)</span>. We have a
tendency, which is natural, to be suspicious when some arbitrary
function acts on an immensely complex input vector. However, in the
rules of quantum mechanics where linear operators dominate, we do not
need this suspicion. Fluency should soon dictate that we see <span
class="math inline">\(\Omega\Psi\)</span> as just another state space
vector.<br />
<br />
The caveat which must be noted is that general linear operators will not
send a unit ray to another unit ray: i.e, if the input vector is
normalised the transformation will usually give them a new unphysical
phase factor. Yet we know the Hilbert space can contain these
unphysically scaled state vectors, which are equivalent to the unit ray
states, so we are fine, as we can renormalise them later.<br />
<br />
The special class of operators which <em>do</em> preserve the
normalisation of unit ray states are called <strong>unitary</strong>
operators. We will cover these later.<br />
<br />
Now, the much more important qualities of operators in quantum mechanics
are that they possess quantities called eigenvalues and eigenvectors,
which is where the quantum mechanical formalism will really start to
come together.</p>
<h3 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h3>
<p>We have just studied operators, which are fundamentally crucial to
all quantum mechanics, but of the moment seem to have vague physical
interpretations. The other side of the same coin is
<strong>eigenvalues</strong>. Flip to the middle chapter of any quantum
mechanics text and you might find a large mixture of words with the
prefix â€œeigen-": eigenvector, eigenfunction, eigenvalue, eigenenergy,
eigenstate, eigenmomentum- et cetera. This will soon become natural,
though it may be initially daunting. Understanding why these words are
so commonplace will set us in a good position, so we do that now.<br />
<br />
Consider a three dimensional example first. We will pick a random
vector, which in this case can be a column vector, <span
class="math display">\[V=\begin{pmatrix}
\frac{47}{10}\\
5.69\\
242\\
\end{pmatrix}\]</span> and a random operator, <span
class="math display">\[\Omega \begin{pmatrix}
x_{1}\\
x_{2}\\
x_{3}\\
\end{pmatrix}=\frac{1}{2}\begin{pmatrix}
x_{2}\\
x_{3}\\
x_{1}\\
\end{pmatrix}\]</span> Upon applying this operator to the vector <span
class="math inline">\(V\)</span>, we are likely not going to get an
output vector which is anything like the original vector in terms of how
corresponding components are related.<br />
<br />
We are however given the grace from mathematics of sets of vectors,
belonging to specific operators, which behave much more stably. To each
operator <span class="math inline">\(\Omega\)</span> there exists a set
of vectors, called <strong>eigenvectors</strong>, such that: <span
class="math display">\[\Omega\omega = \lambda\omega\]</span> for some
constant <span class="math inline">\(\lambda\)</span> and vector <span
class="math inline">\(\omega\)</span>. In the above equation, which we
call an eigenvalue equation, <span class="math inline">\(\omega\)</span>
is an eigenvector of <span class="math inline">\(\Omega\)</span>, and
<span class="math inline">\(\lambda\)</span> is the corresponding
eigenvalue: a complex constant. This equation corresponds to the
operator <span class="math inline">\(\Omega\)</span> scaling the
eigenvector <span class="math inline">\(\omega\)</span> by a scale
factor <span class="math inline">\(\lambda\)</span>: a relatively very
trivial transformation compared to the nontrivial possibilities we
expect for general vectors under the transformation. In the above
example we just saw, for example, no such scaling exists at all, despite
the transformation looking somewhat simple!<br />
<br />
Let us consider a basic operator and try to solve for all possible
eigenvectors and eigenvalues. We could use the identity operator, which
is extremely trivial: <span class="math display">\[I\epsilon =
\epsilon\]</span> means that any vector <span
class="math inline">\(\epsilon\)</span> is an eigenvector of the
identity operator, with corresponding eigenvalue <span
class="math inline">\(1\)</span>. It is a rather uninteresting result,
which pertains to a uniquely simple vector.<br />
<br />
This eigenvalue equation was uniquely simple, but for more complicated
operators, it is clear that trying to find eigenvectors by inspection
will usually be futile. The more advanced methods of solving eigenvalue
problems will come later in this book; for now, only the theory is
important.<br />
<br />
A final note on the above: the prefix eigen-, which is derived from
German and means â€˜ownâ€™ (hence, each operator has its â€˜ownâ€™ set of
eigenvectors), is always used in mathematics when we are dealing with
the above cases. We therefore have eigenvalues, eigenvectors,
eigenfunctions: but also, later on, eigenenergies, eigenmomenta, and so
on, when the eigenvalues are energy and momentum values respectively.
The context and this explicit note should demystify such eigen- words in
the future.</p>
<h3 id="hermitian-operators">Hermitian Operators</h3>
<p>The next important definition central to quantum mechanics is of
<strong>Hermitian</strong> operators. Operators are Hermitian if they
possess the property: <span
class="math display">\[\oip{\bm{\alpha}}{\Omega\bm{\beta}}=\oip{\Omega\bm{\alpha}}{\bm{\beta}}.\]</span>
for <em>any</em> vectors <span
class="math inline">\(\bm{\alpha},\bm{\beta}\)</span>. This
â€˜Hermiticityâ€™ property is deceptively simple, but here are some profound
consequent facts for hermitian operators following this definition.</p>
<ol>
<li><p>Hermitian operators must have real eigenvalues.<br />
<br />
<u>Proof:</u><br />
<br />
For a hermitian operator the eigenvalue condition is the same <span
class="math display">\[\Omega \omega= \lambda \omega.\]</span> We have,
in taking the inner product with the eigenvector <span
class="math inline">\(\omega\)</span>: <span
class="math display">\[\oip{\omega}{\Omega\omega}=\oip{\Omega\omega}{\omega}\]</span>
by the definition of Hermiticity, so <span
class="math display">\[\begin{aligned}
    \oip{\omega}{\lambda\omega}=\oip{\lambda\omega}{\omega}
&amp;\Rightarrow\:\: \lambda\oip{\omega}{\omega} =
\lambda^{\ast}\oip{\omega}{\omega}\\
    \Rightarrow\:\: \lambda = \lambda^{\ast} &amp;\Rightarrow \:\:
\lambda \in \mathbb{R} \btab \square
    \end{aligned}\]</span></p></li>
<li><p>Eigenvectors <span class="math inline">\(\omega_{1}\)</span> and
<span class="math inline">\(\omega_{2}\)</span> of the same hermitian
operator corresponding to different eigenvalues are orthogonal to each
other.<br />
<br />
<u>Proof:</u><br />
<br />
To prove that <span class="math inline">\(\omega_{1}\)</span> and <span
class="math inline">\(\omega_{2}\)</span> are orthogonal we need to
prove that <span
class="math inline">\(\oip{\omega_{1}}{\omega_{2}}=0\)</span>. We can do
this by manipulating the hermitian property of the operator, here
denoted <span class="math inline">\(\Omega\)</span>. <span
class="math display">\[\begin{aligned}
    \oip{\omega_{1}}{\Omega\omega_{2}}&amp;=\oip{\Omega\omega_{1}}{\omega_{2}}\\
    \Rightarrow\:\:
\oip{\omega_{1}}{\lambda_{2}\omega_{2}}&amp;=\oip{\lambda_{1}\omega_{1}}{\omega_{2}}\\
    \Rightarrow\:\:
\lambda_{2}\oip{\omega_{1}}{\omega_{2}}&amp;=\lambda_{1}^{\ast}\oip{\omega_{1}}{\omega_{2}}\\
    \end{aligned}\]</span> but <span
class="math inline">\(\lambda_{1}\)</span> is an eigenvalue of a
hermitian operator so it is real: i.e, <span
class="math inline">\(\lambda_{1}^{\ast}=\lambda_{1}\)</span>. So above
we had <span class="math display">\[\begin{aligned}
    \lambda_{2}\oip{\omega_{1}}{\omega_{2}}&amp;=\lambda_{1}^{\ast}\oip{\omega_{1}}{\omega_{2}}\\
    \end{aligned}\]</span> which is, <span
class="math display">\[\lambda_{2}\oip{\omega_{1}}{\omega_{2}}=\lambda_{1}\oip{\omega_{1}}{\omega_{2}}.\\
\]</span> Therefore, if the eigenvectors do not have the same eigenvalue
then <span class="math inline">\(\lambda_{2}\neq\lambda_{1}\)</span> so
the above implies that <span
class="math inline">\(\oip{\omega_{1}}{\omega_{2}}=0\)</span> and so
these eigenvectors must be orthogonal. <span
class="math inline">\(\btab\btab\square\)</span><br />
<br />
There is a note for the above proof, however. The proof works on the
assumption that for different eigenvectors their eigenvalues are also
different. This is not always a correct assumption. Consider the
identity operator <span class="math inline">\(I\)</span>. It is clearly
hermitian: <span
class="math display">\[\oip{\Psi_{1}}{I\Psi_{2}}=\oip{\Psi_{1}}{\Psi_{2}}=\oip{I\Psi_{1}}{\Psi_{2}}.\]</span>
However, it has infinite different eigenvectors but they all have the
same eigenvalue: 1. Thus the proof above cannot apply to the identity
operator. In this case, the collapse of the point should be obvious
anywayâ€“ all vectors are eigenvectors of the identity operator, but
certainly not all vectors are orthogonal to each other. In general, an
operator where different eigenvectors can share the same eigenvalue is
said to be <strong>degenerate</strong>. In particular, we say particular
eigenvalues are degenerate if they can correspond to multiple different
eigenvectorsâ€“ but we do not say eigenvectors are degenerate because an
eigenvector can never correspond to multiple different
eigenvalues.<br />
<br />
There are a few proofs of theorems in this book which involve the
assumption that we are working with non-degenerate operators. These
proofs, when we incorporate degeneracy, are usually different â€“ and
unfortunately, more difficult. For every step where we assume
non-degeneracy, it would still be within the reaches of this book to
prove an alternative proof in the case of degeneracy, but at the same
time these would take labour and space. Therefore, I will not include
them in this book because they will not alter anything in the
fundamental understanding of a reader. Should the reader want to find
such proofs, they may turn to a more advanced textbook which has the
space and desire to cover this technicality. It should not make a
massive difference either way whether the reader is aware of the
degenerate case proof, so long as they understand when degeneracy makes
a difference to actual consequent theorem or result (and indeed when it
does not, which is quite common here). I will highlight these cases at
when they occur.</p></li>
<li><p>For an operator <span class="math inline">\(\Omega\)</span> with
real eigenvalues <span class="math inline">\(\lambda_{i}\)</span> and
eigenvectors <span class="math inline">\(\bm{\alpha}_{i}\)</span>, if
the eigenvectors constitute an orthonormal basis in the Hilbert space
then the operator is hermitian.<br />
<br />
<u>Proof:</u><br />
<br />
Take the component expressions for the two arbitrary vectors <span
class="math inline">\(\Psi_{1}\)</span> and <span
class="math inline">\(\Psi_{2}\)</span>. We know they can be both
expressed as a linear combination of the eigenvectors <span
class="math inline">\(\bm{\alpha}_{i}\)</span> since the set <span
class="math inline">\(\{\bm{\alpha}_{i}\}\)</span> is stated in the
conditions to be an orthonormal basis. So we have: <span
class="math display">\[\Psi_{1}=\sum_{\{i\}}c_{i}\bm{\alpha}_{i},\:\:\Psi_{2}=\sum_{\{j\}}\gamma_{j}\bm{\alpha}_{j}\]</span>
where the components <span class="math inline">\(c_{i}\)</span> and
<span class="math inline">\(\gamma_{j}\)</span> are found by <span
class="math inline">\((\bm{\alpha}_{i},\Psi_{1})\)</span> and <span
class="math inline">\(\oip{\bm{\alpha}_{j}}{\Psi_{2}}\)</span>
respectively. Then <span class="math display">\[\begin{aligned}
    (\Omega\Psi_{1},\Psi_{2})&amp;=\left(\Omega\sum_{\{i\}}c_{i}\bm{\alpha}_{i},\sum_{\{j\}}\gamma_{j}\bm{\alpha}_{j}\right)\\
    &amp;=\left(\sum_{\{i\}}c_{i}\Omega\bm{\alpha}_{i},\sum_{\{j\}}\gamma_{j}\bm{\alpha}_{j}\right)
    \end{aligned}\]</span> where the operator can be incorporated into
the sum term as it is a linear operator. Then this becomes <span
class="math display">\[\begin{aligned}
    (\Omega\Psi_{1},\Psi_{2}) &amp;=
\left(\sum_{\{i\}}c_{i}\lambda_{i}\bm{\alpha}_{i},\sum_{\{j\}}\gamma_{j}\bm{\alpha}_{j}\right)\\
    &amp;=\sum_{i,j}c^{\ast}_{i}\lambda_{i}^{\ast}\gamma_{j}(\bm{\alpha}_{i},\bm{\alpha}_{j})
    \end{aligned}\]</span> but <span
class="math inline">\(\{\bm{\alpha}_{i}\}\)</span> is an orthonormal
basis so <span
class="math inline">\((\bm{\alpha}_{i},\bm{\alpha}_{j})=\delta_{ij}\)</span>.
The above then becomes <span class="math inline">\(0\)</span> except for
when the two basis vectors are the same, so we are left with: <span
class="math display">\[(\Omega\Psi_{1},\Psi_{2})=\sum_{i}c^{\ast}_{i}\lambda^{\ast}_{i}\gamma_{i}.\]</span>
Now, considering <span
class="math inline">\((\Psi_{1},\Omega\Psi_{2})\)</span>, we have very
similarly: <span class="math display">\[\begin{aligned}
    (\Psi_{1},\Omega\Psi_{2})&amp;=\left(\sum_{\{i\}}c_{i}\bm{\alpha}_{i},\Omega\sum_{\{j\}}\gamma_{j}\bm{\alpha}_{j}\right)\\
    &amp;=\left(\sum_{\{i\}}c_{i}\bm{\alpha}_{i},\sum_{\{j\}}\gamma_{j}\Omega\bm{\alpha}_{j}\right)\\
    &amp;=
\left(\sum_{\{i\}}c_{i}\bm{\alpha}_{i},\sum_{\{j\}}\gamma_{j}\lambda_{j}\bm{\alpha}_{j}\right)\\
    &amp;=\sum_{i,j}c_{i}^{\ast}\lambda_{j}\gamma_{j}(\bm{\alpha}_{i},\bm{\alpha}_{j})=\sum_{i,j}c_{i}^{\ast}\lambda_{j}\gamma_{j}\delta_{ij}\\
    &amp;=\sum_{i}c_{i}^{\ast}\lambda_{i}\gamma_{i}.
    \end{aligned}\]</span> so we have <span
class="math display">\[(\Omega\Psi_{1},\Psi_{2})=\sum_{i}c^{\ast}_{i}\lambda^{\ast}_{i}\gamma_{i},\:\:\:\:(\Psi_{1},\Omega\Psi_{2})=\sum_{i}c_{i}^{\ast}\lambda_{i}\gamma_{i}\]</span>
but we have conditioned that the eigenvalues <span
class="math inline">\(\lambda_{i}\)</span> are real so we therefore see
that <span class="math inline">\(\lambda_{i}=\lambda_{i}^{\ast}\)</span>
and so <span
class="math display">\[(\Psi_{1},\Omega\Psi_{2})=(\Omega\Psi_{1},\Psi_{2})\]</span>
which is the definition of a Hermitian operator. This holds true for any
arbitrary <span class="math inline">\(\Psi_{1}\)</span> and <span
class="math inline">\(\Psi_{2}\)</span> so long as they are in the space
spanned by the orthonormal basis and can subsequently be expressed as a
linear combination of the orthonormal constituent vectors; therefore,
any operator with real eigenvalues whose eigenvectors can form an
orthonormal basis set is Hermitian. <span
class="math inline">\(\square\)</span><br />
<br />
This proof in fact goes both ways: more significantly, any Hermitian
operator possesses a set of eigenvectors which are an orthonormal basis
set of the state space! The proof is rather technical, so it will be
ignoredâ€“ but the profound consequences are clear. If an operator in the
state space is Hermitian it has a basis consisting eigenvectors, called
its <strong>eigenbasis</strong>, spanning the space; if we take any
basis of the state space we can take its inner product over all the
basis eigenvectors with any state vector to produce a wavefunction.
These representations will prove massively helpful.</p></li>
<li><p>The action of all Hermitian operators whose eigenvectors form an
orthonormal basis can be specified by their eigenvalues and
eigenvectors.<br />
<br />
<u>Proof:</u><br />
<br />
For a Hermitian operator <span class="math inline">\(\Omega\)</span>
with eigenvalues <span class="math inline">\(\lambda_{i}\)</span> and
eigenvectors <span class="math inline">\(\bm{\alpha}_{i}\)</span>, any
vector can be expressed in the orthonormal basis: <span
class="math display">\[\Psi=\sum_{i}(\bm{\alpha}_{i},\Psi)\bm{\alpha}_{i}\]</span>
so <span
class="math display">\[\Omega\psi=\Omega\sum_{i}(\bm{\alpha}_{i},\psi)\bm{\alpha}_{i}=\sum_{i}(\bm{\alpha}_{i},\psi)\Omega\bm{\alpha}_{i}=\sum_{i}(\bm{\alpha}_{i},\psi)\lambda_{i}\bm{\alpha}_{i}.\]</span>
This clearly requires no external knowledge other than understanding the
sets <span class="math inline">\(\{\lambda_{i}\}\)</span> and <span
class="math inline">\(\{\bm{\alpha}_{i}\}\)</span>. Conversely: if we
completely understand these sets then we can completely specify the
operator given an input vector <span class="math inline">\(\psi\)</span>
the operator is acting on.</p></li>
</ol>
<p>With all this knowledge about operators in vector spaces, and clear
signs that their eigenbases will be very useful, we now come to the
Second Postulate of quantum mechanics.</p>
<h2 id="observables-in-quantum-mechanics">Observables in Quantum
Mechanics</h2>
<p>The state problem is a question of information. The most relevant
information to a physicist about a state is the value of its
<em>observables</em>. The problem, we have seen, is that unlike with the
classical state, the quantum state cannot just be said to possess a
value for any observable, as it is instead a superposition of many
different possible states and it is thus far unclear which state will
emerge upon measurement. This problem was dealt with by placing states
in correspondence with vectors in a vector space which meant that all
possible states could be summed together in a superposition to create a
new state without forming something out of the space of possible states,
and we learnt how to create more â€˜tangibleâ€™ wavefunctions from those
state vectors. However, we still do not know why wavefunctions are so
tangible. The genius of the formalism comes when we incorporate
operators and their orthogonal eigenbases into the picture, where
wavefunctions in eigenbases will answer our problem.<br />
<br />
<br />
<br />
The most essential outcome to us here is that we want real values for
the results of physical measurements, as imaginary position or imaginary
energy for example would be nonsense; since the set of eigenvalues <span
class="math inline">\(\setof{A_{i}}\)</span> are the only possible
results of measurements, we therefore require real eigenvalues. By the
postulate, we have Hermitian operators representing observables, which
therefore must have real eigenvalues. The postulate is that these
eigenvalues are the only possible measurable results for that observable
for any states, so that they are real is critical. It is from the fact
we need the operator to have real eigenvalues if these are to be the
measured values for a physical operator, combined with the fact it is
postulated to have an orthonormal eigenbasis spanning the state space,
which guarantees the operators representing observables must be
Hermitian. That is precisely, we recall, <span
class="math display">\[\oip{\hat{A}\Psi_{1}}{\Psi_{2}}=\oip{\Psi_{1}}{\hat{A}\Psi_{2}}.\]</span>
We start a fuller discussion by noting the assertions of the postulate
in short-form:</p>
<ol>
<li><p>All physical observables are represented by hermitian operators
whose eigenvectors form an eigenbasis which spans the state space. For
brevity it is customary to call these operators which represent physical
operators â€˜observable operatorsâ€™ throughout the course of this
book.</p></li>
<li><p>Each measurement of a physical observable must yield one of the
real eigenvalues of the observable operator.</p></li>
<li><p>As it is possible for an operator to have a finite/and or
discrete number of eigenvalues, so can a physical observable have a
finite number of possible results after being measured if their operator
has a finite number of eigenvalues. Such a situation is rarer than one
would assume in quantum mechanics, considering we are working in an
infinite dimensional vector space where it is relatively unlikely there
are not infinite eigenvectors and eigenvalues to an arbitrary operator,
but it is far from a non-existent possibility. The physical phenomenon
resulting from discretely distributed eigenvalues is called
quantization; its implications, most famously perhaps in the discrete
energy levels of electrons, are important and may well have been already
known to the reader.</p></li>
</ol>
<p>Now we must consider the importance of time. Crucial is that
operators corresponding to physical observables never change with time,
and there is only one operator corresponding to each physical
observable. That does not mean that the measured values of the
observable will remain constant across any time period. That is because
clearly the measured value of the observable depends on the state vector
<span class="math inline">\(\Psi\)</span> representing the state of the
system which is the input vector we the observable operator is operating
on; we have already stated this state vector can evolve with time. The
precise nature of all these temporal considerations will be covered in
due course, but that observable operators do not evolve with time is
surely a great relief, especially if we need to think about their
eigenbases and eigenvalues and do not want to have to continually solve
what are not trivial eigenvalue equations.<br />
<br />
The second part of the postulate gives us an interesting and significant
link between the state vectors which represent states, the state space
operators representing physical observables, and the eigenvalues of
those observables representing possible results after measurement.<br />
<br />
To start off, note that we expect that most state space vectors will be
linear combinations of the eigenvectors of any observable operator,
since the set of (infinite) eigenvectors of a hermitian operator
constitutes an orthonormal basis of its space. We do not expect all
possible states to be pure scalar multiples of single eigenvectors since
there can be infinite linear combinations of the eigenvectors which are
not pure scalar multiples of single eigenvectors. The postulate now
states that the condition <span
class="math display">\[\hat{A}\Psi=A_{i}\bm{\alpha}_{i}\]</span> means
that <span class="math inline">\(A_{i}\)</span> is the measured value of
the physical observable represented by the operator <span
class="math inline">\(\hat{A}\)</span>. However, this condition is
clearly very singular if we are working with a wavefunction which is a
linear combination of the infinite eigenvectors of an observable
operatorâ€“ since the state <span class="math inline">\(\Psi\)</span> does
not naturally coincide with the eigenvector <span
class="math inline">\(\bm{\alpha}_{i}\)</span> alone. What should be
abundantly clear, however, is that, post-measurement, <span
class="math inline">\(\Psi\)</span> has changed from a linear
combination of eigenvectors to a multiple of only one of them, the
eigenvalue corresponding to which is the measured value of the
observable. So the act of measurement is clearly very important; indeed,
it forms one of the central pillars of quantum mechanics and especially
the mathematics which formulates it. We have seen this, already, in the
Stern Gerlach experiment! There, measuring the <span
class="math inline">\(x\)</span> spin led to irrevocable changes in the
<span class="math inline">\(y\)</span> spin even without physically
affecting it in that dimension. This measurement problem is the final
component of the quantum mechanical solution to the state problem, and
pulls everything together in an understandable way. Thus to complement
Postulate 2 on observables, we have Postulate 3, on Measurements.</p>
<h3 id="measurements">Measurements</h3>
<p><u><strong>Postulate 3: Measurements</strong></u><br />
<br />
After a measurement of a physical observable, the state vector is forced
into a specific eigenvector corresponding to the eigenvalue measured for
that observable. The probability that the (normalised) state vector is
forced into a state represented by a state vector <span
class="math inline">\(\bm{\alpha}_{i}\)</span>, which is called an
eigenstate, is given by <span
class="math display">\[P(\bm{\alpha}_{i})=|\oip{\bm{\alpha}_{i}}{\Psi}|^2,\]</span>
which is therefore also the probability of measuring the eigenvalue
<span class="math inline">\(A_{i}\)</span> as the final result of the
measurement for the observable. This Postulate now provides great
meaning to the discourse immediately preceding this section. By
<em>forcing</em> a state vector into specific eigenvector of an
observable operator after a measurement of that specific observable, we
guarantee several things:</p>
<ul>
<li><p>We do not restrict the state vector to being a pure scalar
multiple of one single eigenvector prior to measurement. This is
important as by Postulate 1, all state space vectors represent physical
states, and there certainly should be infinite Hilbert space vectors
which are linear combinations of any orthonormal basis vectors which
span it: including when that orthonormal basis is the eigenbasis of an
observable operator. This is the superposition of different possible
states which exists before a measurement.</p></li>
<li><p>We guarantee that after measurement, Postulate 2 has meaning:
since the measurement forces the wavefunction into a specific
eigenstate, we will indeed achieve after measurement <span
class="math display">\[\hat{A}\Psi\to
\hat{A}\bm{\alpha}_{i}=A_{i}\bm{\alpha}_{i}\]</span> and therefore we
guarantee that a measurement will always yield one single valueâ€“ the
eigenvalue <span class="math inline">\(A_{i}\)</span>: regardless of
what superposition of states it was in previously.</p></li>
</ul>
<p>This is all well and good, but what gives order to the chaos is the
probabilistic link of the postulate. Without it, we would be wondering
what to do in any arbitrary superposition in states, since intuition
tells us that just because we have a superposition it does not mean that
all the measurements must have equal probabilities. Fortunately, we have
the postulate: <span
class="math display">\[P(\bm{\alpha}_{i})=|\oip{\bm{\alpha}_{i}}{\Psi}|^2\]</span>
where <span class="math inline">\(P(\bm{\alpha}_{i})\)</span> is the
probability of measuring the state vector to be in the state <span
class="math inline">\(\bm{\alpha}_{i}\)</span>. Here we make a
clarification of similar type as that regarding the distinction between
state and wavefunction: the reader must understand that the state vector
being in an eigenstate <span
class="math inline">\(\bm{\alpha}_{i}\)</span> is not so interesting
itself as is the fact that when it is in that eigenstate we know <span
class="math inline">\(A_{i}\)</span> is the eigenvalue is the measured
result for the observable. Thus when we say the state is measured to be
the eigenstate <span class="math inline">\(\bm{\alpha}_{i}\)</span> we
really allude to the fact that a measurement will yield <span
class="math inline">\(A_{i}\)</span> as the value. The reason we do not
write <span class="math inline">\(P(A_{i})\)</span>, the probability of
measuring <span class="math inline">\(A_{i}\)</span>, is due to the fact
that in the face of degeneracy (say, eigenvalue <span
class="math inline">\(A_{1}\)</span> corresponding to two different
eigenstates <span class="math inline">\(\bm{\alpha}_{1}\)</span> and
<span class="math inline">\(\bm{\alpha}_{2}\)</span>), we have the
following problem: <span
class="math display">\[P(A_{1})\neq\oip{\bm{\alpha}_{1}}{\Psi}\neq\oip{\bm{\alpha}_{2}}{\Psi}\]</span>
in fact, here it would be <span
class="math display">\[P(A_{1})=\oip{\bm{\alpha}_{1}}{\Psi}+\oip{\bm{\alpha}_{2}}{\Psi}.\]</span>
So we see that defining the probability of a wavefunction being in an
eigenstate is slightly easier and more consistent. Next, consider the
state <span class="math display">\[\Psi=\bm{\alpha}_{n}.\]</span> In
this state a measurement will yield value <span
class="math inline">\(A_{n}\)</span> with probability <span
class="math display">\[P(A_{n}):=|\oip{\bm{\alpha}_{n}}{\Psi}|^2=|\oip{\bm{\alpha}_{n}}{\bm{\alpha}_{n}}|^2=1\]</span>
since the eigenvectors <span
class="math inline">\(\bm{\alpha}_{n}\)</span> are assumed to be
normalised. So if the state vector is a pure eigenstate then the
eigenvalue corresponding to the eigenstate it is in will be measured
with probability 1. When do pure eigenstates occur for state vectors?
They may occur organically for some arbitrary physical state which
happens to be a pure eigenstate of a physical observable, though we
expect this to be comparatively rare. More importantly: they also occur
after measurements, since by the first part of the postulate a
measurement will force a state vector into an eigenvector â€“ a pure
eigenstate â€“ of the observable operator. This now explains why
instantaneous successive measurements must yield the same answer: the
first measurement forces the state vector into a pure eigenstate <span
class="math inline">\(\bm{\alpha}_{n}\)</span> corresponding to the
eigenvalue <span class="math inline">\(A_{n}\)</span> measured, and then
the second measurement will give the same eigenvalue <span
class="math inline">\(A_{n}\)</span> with probability <span
class="math display">\[P(A_{n})=|\oip{\bm{\alpha}_{n}}{\bm{\alpha}_{n}}|^2=1\]</span>
since the state vector is now the pure eigenstate <span
class="math inline">\(\bm{\alpha}_{n}\)</span> after being forced into
this eigenstate by the first measurement. We saw this intuitive
consequence in the Stern Gerlach experiment, where successive magnetic
fields in the same axis yielded the same spin results each time!<br />
<br />
The disturbance to classical intuition comes when we make the same
observation we have already made. By Postulate 1, all state space
vectors represent physical states, and there are infinite state space
vectors which are linear combinations of any orthonormal basis vectors
which span it. Thus if the orthonormal basis is the eigenbasis of a
physical observable operator, there are infinite state vectors which are
not pure eigenvectors of the observable operator, but rather linear
combinations of the corresponding eigenvectors. Then, <span
class="math display">\[\Psi=\sum_{i}\oip{\bm{\alpha}_{i}}{\Psi}\bm{\alpha}_{i},\]</span>
by the expansion of Hilbert Space vectors in an orthonormal basis. But
then, for any <span class="math inline">\(A_{n}\)</span> in a
non-degenerate state (similar reasoning holds for degenerate states),
<span class="math display">\[\begin{aligned}
P(A_{n}):=|\oip{\bm{\alpha}_{n}}{\Psi}|^2=|\oip{\bm{\alpha}_{n}}{\sum_{i}\oip{\bm{\alpha}_{i}}{\Psi}\bm{\alpha}_{i}}|^2
\end{aligned}\]</span> so if this probability was zero then that would
imply the component <span
class="math inline">\(\oip{\bm{\alpha}_{i}}{\Psi}=0\)</span>. In a
nontrivial linear combination of eigenvectors, there will exist more
than one eigenvector <span
class="math inline">\(\bm{\alpha}_{i}\)</span> for which this is not
true, and therefore more than one eigenvalue <span
class="math inline">\(A_{i}\)</span> which can be measured with nonzero
probability.<br />
<br />
This is the famous probabilistic nature of quantum mechanics
encapsulated through our postulates. Do there exist such state vectors
which are linear combinations of multiple eigenvectors of an observable
operator? Certainly yes, by Postulate 1. Yet in such cases multiple
eigenvalues can be measured with nonzero probability: that is, multiple
values can be obtained for the same measurement. Thus the wording of the
postulateâ€“ the state vector is forced into a specific eigenvectorâ€“ is
relevant: the state vector in these cases does not possess a single
fixed value for an observable which must be revealed upon measurement so
it cannot be said, technically, to have a position or momentum or any
other observable value. We can only say that an eigenvalue is the
measured value of this specific measurement: or, the eigenvector the
state vector has been forced into was not necessarily the state vector
before at all; in another scenario, with defined probability, the state
vector may well have been forced into a different eigenstate and then
yielded a different value for an observable.<br />
<br />
We end this section with a summary on quantum states and inherent
probability:</p>
<ul>
<li><p>In a pure state with respect to a physical observable the state
vector is made up solely of one eigenvector of the observable operator
(up to some phase factor). A measurement will therefore yield the
eigenvalue corresponding to that eigenvector with probability 1. In such
cases (a rarity) a deterministic prediction can be made about the
results of a measurement.</p></li>
<li><p><em>After</em> measurement a state vector is forced into one of
the constituent pure eigenstates <span
class="math inline">\(\bm{\alpha}_{i}\)</span> with probability <span
class="math inline">\(|\oip{\bm{\alpha}_{i}}{\Psi}|^2\)</span>. Thereon
the above determinism of a pure state applies for successive
measurements of the same observable unless the system experiences
external perturbation which moves it out of the pure
eigenstate.</p></li>
<li><p>A state vector is in a mixed state with respect to a physical
observable if the state vector is a non-trivial linear combination of
more than one eigenvector of the observable operator. In those cases the
strongest predictive statement about the result of a measurement is that
a specific eigenstate <span
class="math inline">\(\bm{\alpha}_{i}\)</span> has probability <span
class="math inline">\(|\oip{\bm{\alpha}_{i}}{\Psi}|^2\)</span> of being
measured. We cannot make any deterministic predictions at all, and we do
not really think of a mixed state as having a value for that specific
observable. Most naturally occurring states in quantum mechanics are
indeed mixed states.</p></li>
</ul>
<h2 id="probability-mass-functions">Probability Mass Functions</h2>
<p>We conclude this chapter on measurements, and indeed conclude the
fundamental postulates dealing with the quantum state problem, with a
formalisation of how discrete wavefunctions encapsulate probabilities as
probability mass functions.<br />
<br />
We have already stated that the discrete wavefunction, which we denoted
<span class="math inline">\({\psi}_{\bm{\alpha}}(x)\)</span> with a
domain of orthonormal eigenvectors, is exactly the function which stores
the components corresponding to the eigenvectors we input. We therefore
call it a <strong>probability mass function</strong>. This is a formal
name for a very simple idea: it stores probabilities of discrete events-
here, the event is the state vector being forced into a certain
eigenstate by a measurementâ€“ and can be extracted as an output of the
probability mass function when we input the event (eigenstate). We know
that these components are probabilities, because of the measurement
postulate and the common expansion we have already proved!<br />
<br />
If the discrete wavefunction is a probability mass function, then
necessarily the modulus squared of its outputs (the probabilities of the
state taking each pure eigenstate) must sum to exactly <span
class="math inline">\(1\)</span>. Thus as we must get some result for a
measurement, the sum of probabilities is the probability that some
result will occur, and thus 1. There is an important clarification to
make to prove that our formalism works.<br />
<br />
<em>Claim: The modulus squared components of a normalised state vector
must sum to 1 in a discrete basis.</em><br />
<br />
The importance of this claim is clear, since it is equivalent to the
statement that the sums of the different probabilities for all the
possible measurements of an observable must sum to <span
class="math inline">\(1\)</span>, which must be true if they are to be
considered probabilities in the first place.<br />
<br />
<u>Proof:</u><br />
<br />
For some state vector <span
class="math display">\[\Psi:=\sum_{\{i\}}c_{i}\bm{\alpha}_{i}\]</span>
in some orthonormal basis <span
class="math inline">\(\setof{\bm{\alpha}_{i}}\)</span>, we need to prove
that <span
class="math display">\[\sum_{\{i\}}|(\bm{\alpha}_{i},\Psi)|^2=1\]</span>
given that the state vector is normalised. Well we know that <span
class="math display">\[(\Psi,\Psi)=1\]</span> so we know that <span
class="math display">\[\biggl(\sum_{\{i\}}c_{i}\bm{\alpha}_{i},\sum_{\{i\}}c_{i}\bm{\alpha}_{i}\biggr)=1.\]</span>
Then, by the rudimentary expansion this is <span
class="math display">\[\biggl(\sum_{\{i\}}(\bm{\alpha}_{i},\Psi)\bm{\alpha}_{i},\sum_{\{j\}}(\bm{\alpha}_{j},\Psi)\bm{\alpha}_{j}\biggr)=1.\]</span>
Due to linear distributivity this means that we get sum terms of the
form <span
class="math display">\[\oip{\bm{\alpha}_{i}}{\Psi}^{\ast}\oip{\bm{\alpha}_{j}}{\Psi}\oip{\bm{\alpha}_{i}}{\bm{\alpha}_{j}}\]</span>
for some <span class="math inline">\(i,j\)</span>. However, due to the
orthonormality of the basis, all terms when <span
class="math inline">\(i\neq j\)</span> disappear, so we have <span
class="math display">\[\biggl(\sum_{\{i\}}(\bm{\alpha}_{i},\Psi)\bm{\alpha}_{i},\sum_{\{j\}}(\bm{\alpha}_{j},\Psi)\bm{\alpha}_{j}\biggr)=\sum_{\{i\}}\oip{\bm{\alpha}_{i}}{\bm{\alpha}_{i}}^{\ast}\oip{\bm{\alpha}_{i}}{\bm{\alpha}_{i}}=1.\]</span>
But then this is simply <span
class="math display">\[\sum_{\{i\}}|\oip{\bm{\alpha}_{i}}{\Psi}|^2=1.\]</span>
and our proof is complete.<br />
<br />
Thus indeed, we have the result that the square modulus components of
the discrete state vector, which is, the square modulus of the outputs
of its discrete wavefunctions, are valid probabilities in themselves of
measurements. In this way, the encapsulation component of the state
problem is much better understood: a state vector represents a physical
state, we can then transform that state vector into wavefunctions in
another bijection to the state and state vector, and in the discrete
case the modulus squared of the outputs of these wavefunctions are the
probabilities we need if we want to make predictions about
measurements.</p>
<h2 id="exercises-from-chapter-4ast">Exercises from Chapter 4<span
class="math inline">\(\ast\)</span></h2>
<ol>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>
<hr>
<div style="text-align: center; font-size: 1.2em; padding-top: 20px;">
    <a href='3.html'>&larr; Previous Chapter</a> &nbsp;&nbsp; | &nbsp;&nbsp; <a href='5.html'>Next Chapter &rarr;</a>
</div>
</body>
</html>
