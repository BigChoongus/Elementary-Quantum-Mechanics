<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>6</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <script>
    window.MathJax = {
      loader: { load: ['[tex]/newcommand', '[tex]/boldsymbol'] },
      tex: {
        packages: { '[+]': ['base', 'ams', 'newcommand', 'boldsymbol'] },
        macros: {
          bm: ["\\boldsymbol{#1}", 1]  // Define \bm{} using \boldsymbol{}
        },
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


      
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
    $$
    \newcommand{\Answer}{\begin{tcolorbox}}
    \newcommand{\Answerend}{\end{tcolorbox}}
    \newcommand{\ket}[1]{|#1\rangle}
    \newcommand{\bra}[1]{\langle#1|}
    \newcommand{\ip}[2]{\langle#1|#2\rangle}
    \newcommand{\bip}[2]{\left\langle#1\middle|#2\right\rangle}
    \newcommand{\qexp}[1]{\langle#1\rangle}
    \newcommand{\apos}[1]{``#1"}
    \newcommand{\sapos}[1]{`#1'}
    \newcommand{\elec}{e^{-}}
    \newcommand{\uspin}{(\uparrow)}
    \newcommand{\dspin}{(\downarrow)}
    \newcommand{\lspin}{(\leftarrow)}
    \newcommand{\rspin}{(\rightarrow)}
    \newcommand{\ulspin}{(\uparrow\leftarrow)}
    \newcommand{\urspin}{(\uparrow\rightarrow)}
    \newcommand{\dlspin}{(\downarrow\leftarrow)}
    \newcommand{\drspin}{(\downarrow\rightarrow)}
    \newcommand{\R}{\mathbb{R}}
    \newcommand{\stab}{\:\:}
    \newcommand{\mtab}{\:\:\:}
    \newcommand{\btab}{\:\:\:}
    \newcommand{\imp}{\Rightarrow}
    \newcommand{\doubimp}{\Leftrightarrow}
    \newcommand{\setof}[1]{\{#1\}}
    \newcommand{\infint}{\int_{-\infty}^{\infty}}
    \newcommand{\trans}[1]{\mathcal{T}(#1)}
    \newcommand{\dd}[2]{\delta(#1-#2)}
    \newcommand{\ipbig}[2]{\langle#1|#2\rangle}
    \newcommand{\talpha}{\tilde{\alpha}}
    \newcommand{\op}[2]{|#1\rangle\langle#2|}
    \newcommand{\sop}[1]{|#1\rangle\langle#1|}
    \newcommand{\prop}[2]{\mathcal{U}(#1,#2)}
    \newcommand{\propdagg}[2]{\mathcal{U}^{\dagger}(#1,#2)}
    \newcommand{\sip}[1]{\langle#1|#1\rangle}
    \newcommand{\optrip}[3]{\langle#1|\hat{#2}|#3\rangle}
    \newcommand{\nhoptrip}[3]{\langle#1|{#2}|#3\rangle}
    \newcommand{\northexp}[2]{\sum_{i=1}^{n}|#2\rangle\langle#2|#1\rangle}
    \newcommand{\orthexp}[4]{\sum_{#3=1}^#4|#2\rangle\langle#2|#1\rangle}
    \newcommand{\schrodeq}{i\hbar\frac{\partial \Psi(x,t)}{\partial t}=\hat{H}\Psi(x,t)}
    \newcommand{\nd}[2]{\frac{d#1}{d #2}}
    \newcommand{\snd}[2]{\frac{d^{2}#1}{d#2^2}}
    \newcommand{\pd}[2]{\frac{\partial#1}{\partial #2}}
    \newcommand{\spd}[2]{\frac{\partial^{2}#1}{\partial #2^2}}
    \newcommand{\duac}{\leftrightarrow}
    \newcommand{\oip}[2]{\left(#1,#2\right)}
    \newcommand{\obip}[2]{\left(#1,#2\right)}
    $$
<h1 id="chapter-6-time-evolution-and-problem-solving-techniques">Chapter
6: Time Evolution and Problem Solving Techniques</h1>
<p>We will complete our study of the postulates of quantum mechanics in
this chapter, which will be less cumbersome than the preceding two
chapters where we had to deal with abstract mathematical theory in
vector spaces and operators. This will provide quantum mechanics’ answer
to the other major problem of a Physical Model: the Time-Evolution
Problem.<br />
<br />
The quantum time-evolution problem will prove to be a double-edged
challenge. On one hand, the time evolution postulate is completely
trivial conceptually, especially compared to the state vector,
observable and measurement postulates, so the reader can breathe a sigh
of relief and know they won’t have to deal with learning a new field of
mathematics just to tackle this problem. All it will include, indeed, is
learning one equation: the all-famous Schrödinger Equation postulated by
Erwin Schrödinger, and this equation will not be difficult to
understand.<br />
<br />
On the other hand, the time evolution problem has by far the most
potential to be complex, and in many advanced cases, <em>impossible</em>
to solve without new intensely demanding mathematical techniques for
manipulation and indeed often (though still very accurate)
approximation. This should not be disheartening, however. The great
physicists of the <span class="math inline">\(20\)</span>th century were
not immortalised for only reaching the depth of an introductory book
like this one. The Schrödinger Equation is still always valid, even when
it is difficult, or impossible, for us to practically solve the actual
complete solutions of it.</p>
<h2 id="time-evolution-and-schrödingers-equation">Time Evolution and
Schrödinger’s Equation</h2>
<p>Now that we have established the stationary properties of quantum
states, observables and measurements, we are done with the quantum
mechanical state problem. The second paramount question of Physics is
the question of time evolution. One might be relieved to find that,
theoretically speaking, the time evolution problem is much simpler and
will not require us to do so much complex postulating as the state
problem did. In fact, we only need one more postulate to introduce time
evolution; this is the famous Schrödinger Equation (which is a
postulate, not a derivation!), which will be important to quantum
mechanics much similar to the way <span
class="math inline">\(F=ma\)</span> is ubiquitous in classical
mechanics.</p>
<div class="tcolorbox">
<p><u><strong>Postulate 5: Schrödinger Equation and the
Hamiltonian</strong></u><br />
<br />
In quantum mechanics, there exists the Hamiltonian operator, written
<span class="math inline">\(\hat{H}\)</span>, which corresponds to the
total energy of the system. It is also hermitian, and it plays an
integral role in the time-evolution equation in quantum mechanics, the
Schödinger Equation: <span class="math display">\[i\hbar\frac{\partial
\Psi(t)}{\partial t}=\hat{H}\Psi(t)\]</span> with the Hamiltonian
Operator <span class="math inline">\(\hat{H}\)</span> defined as <span
class="math display">\[H = \frac{\hat{P^2}}{2m} + V(\hat{X})\]</span>
for some function <span class="math inline">\(V\)</span> of the position
operator <span class="math inline">\(\hat{X}\)</span> which is called
the potential. This equation determines how the wavefunction will evolve
in time provided there are no perturbations to the system.</p>
</div>
<p>Note that we have moved from <span
class="math inline">\(\Psi_{t}\)</span> to <span
class="math inline">\(\Psi(t)\)</span> which is a better shorthand
notation now that we are not discussing stationary states. This function
notation doesn’t clash with the fact that the state vector is a state
vector: it just means that the input is a time value and the output is
the state vector corresponding to the state at that time.<br />
<br />
Now, we once more start by listing the assertions of this postulate for
clarity.</p>
<ol>
<li><p>This equation is <em>basis-independent</em>, because the
derivative which appears is with respect to time, which is a parameter
of the wavefunction regardless of which basis we are working in.<br />
<br />
For example, in position space, the Schrödinger Equation is: <span
class="math display">\[i\hbar\frac{\partial \psi(x,t)}{\partial t}=
\biggl(-\frac{\hbar^2}{2m}\frac{\partial^2 \psi(x,t)}{\partial x^2}+
V(x)\biggr)\psi(x,t)\]</span> after substituting the position space
forms <span class="math display">\[\hat{X}=x, \:\:
\hat{P}=-i\hbar\frac{\partial}{\partial x}, \:\: \Psi(t) =
\psi(x,t)\]</span> Note that as opposed to previous chapters, we now
consider <span class="math inline">\(\psi(x,t)\)</span> instead of <span
class="math inline">\(\psi(x)\)</span> for the position space
wavfunction just as we consider <span
class="math inline">\(\Psi(t)\)</span> instead of <span
class="math inline">\(\Psi\)</span>, since we are no longer working with
the properties of the state vector at one single time only.</p></li>
<li><p>There is a total energy, regardless of the specific qualities of
the potential, which is integral to quantum mechanics. Clearly it is a
physical observable since it is represented by a hermitian operator, the
Hamiltonian.</p></li>
<li><p>The Hamiltonian operator is the operator form of the classical
Hamiltonian- that is, the quantum mechanical version of <span
class="math display">\[\mathscr{H}(x,p)=\frac{p^2}{2m}+V(x).\]</span>
which is obtained merely by replacing the classical <span
class="math inline">\(p\)</span> with <span
class="math inline">\(\hat{P}\)</span> and the function <span
class="math inline">\(V(x)\)</span> of the classical <span
class="math inline">\(x\)</span>, called the potential, with the same
function <span class="math inline">\(V\)</span> but of <span
class="math inline">\(\hat{X}\)</span>.</p></li>
<li><p>The potential <span class="math inline">\(V(x)\)</span> may not
be familiar to a reader who has not studied classical mechanics.
However, note that the kinetic energy operator <span
class="math display">\[\hat{T}= \hat{H}-V(\hat{X})=
\frac{\hat{P^2}}{2m}\]</span> is a very rigid term. Therefore, it is
really the potential which makes <span
class="math inline">\(\hat{H}\)</span> a complicated operator.<br />
<br />
The potential make take a variety of forms, depending on the setting of
our quantum system which we are trying to solve. Some examples might
be:</p>
<ol>
<li><p>The free potential (no potential at all): <span
class="math display">\[V(x)\equiv 0 \implies V(\hat{X}) \equiv 0
\implies \hat{H} = \frac{\hat{P}^{2}}{2m}\]</span></p></li>
<li><p>A linear potential, <span class="math display">\[V(x)= -\gamma
x\implies V(\hat{X}) = -\gamma\hat{X}\implies \hat{H} =
\frac{\hat{P}^{2}}{2m}-\gamma\hat{X}\]</span> for some real number <span
class="math inline">\(\gamma\)</span>.</p></li>
<li><p>The oscillator potential: <span class="math display">\[V(x) =
\frac{1}{2}m\omega^{2}x^2 \implies V(\hat{X}) =
\frac{1}{2}m\omega^{2}\hat{X}^2\]</span></p></li>
</ol>
<p>Note that we are showing explicitly that these potentials come from
classical mechanics by transforming them into quantum potentials with
the substitution <span class="math inline">\(x\to
\hat{X}\)</span>.<br />
<br />
It is hard to provide in short words an intuition for what the potential
is, but for our purposes it can be thought of as defining the “backdrop"
of our problem– almost a “shape" our solution must fit into– given its
spacial dependence, which stretches through all values of position <span
class="math inline">\(x\)</span>.<br />
<br />
Importantly, it is the form of this potential, which can be very exotic,
which determines whether our Hamiltonian will be <em>solvable</em>, that
is, if we can determine its eigenstates analytically. It turns out that
most complicated potentials such as <span
class="math display">\[V(\hat{X}) = \hat{X}^4\]</span> make the
Hamiltonian unsolvable.</p></li>
<li><p>Since we say that this Hamiltonian operator exists, there must be
eigenvalues (also called eigenenergies for obvious reasons) which are
the possible measured values of energy and corresponding eigenvectors of
the Hamiltonian- or, energy eigenstates. In fact we often get that the
eigenvalues are discretely distributed for the Hamiltonian operator:
which means energy is quantised. Bohr’s famous electron model results
from this energy quantisation.<br />
<br />
Finding the eigenstates of the Hamiltonian is sufficient to solve
Schrödinger’s Equation. We will see why below.</p></li>
<li><p>Given a state vector at time <span
class="math inline">\(0\)</span> it evolves in a completely
deterministic way. This is surely a great relief. The state may not be
deterministic– in which case it is a mixed state for which the strongest
predictive statements which can be made are those detailed in Postulate
3. However, it will evolve into a new state vector in a predictable way.
That is not to say at all that after the evolution it will not still be
in a mixed state, as the new state it has evolved over time into may
well still be a superposition of eigenstates. It is just to say that we
can determine future state vectors (not necessarily measurements) well
given the starting one and the Hamiltonian for the given
system.</p></li>
</ol>
<p>For all quantum mechanics problems, solving the Schrödinger equation
is the most difficult part of the problem. The reason for this is that
the Hamiltonian operator is the only major operator which will change
depending on the conditions of the problem. The position operator,
momentum operator, spin operators all remain the same, but the
Hamiltonian is subject to great variation and even variation over time
if the potential of the system is varying over time. This means that for
different physical problems we always have to go through the
considerable difficulty of finding the form of the Hamiltonian given the
different conditions, and then solving the eigenvalue equation for that
Hamiltonian, which is rarely not incredibly difficult. With all that
said, this next section details why such painstaking effort is worth it:
with the energy eigenvalue, or Hamiltonian eigenvalue, equation solved,
Schrödinger’s Equation is also immediately solved.</p>
<h3 id="solving-with-energy-eigenstates">Solving with Energy
Eigenstates</h3>
<p>This section will require a difficult mixing of the rules we have
thus far learnt and to be able to follow the developments we make will
be tantamount to truly consolidating our grasps on the postulates and
mathematics up to this point. It also gives many procedural insights
into how one should approach problems (not just subproblems, but full
problems) in quantum mechanics, by giving the most elementary way to
solve the Schrödinger equation. This method is through considering
<strong>energy eigenstates</strong>: that is, the eigenstates of the
Hamiltonian operator and the state vectors in the orthonormal basis they
form which spans the Hilbert space.<br />
<br />
To start recall that for any state vector in the state space and
orthonormal basis <span
class="math inline">\(\{\bm{\alpha}_{i}\}\)</span> the state vector can
be expressed in terms of how it acts on those eigenvectors: <span
class="math display">\[\Psi_{t}=\sum_{i=1}^{k}\oip{\bm{\alpha}_{i}}{\Psi_{t}}\bm{\alpha}_{i}.\]</span>
where <span class="math inline">\(k\)</span> is the dimensionality of
the space: in the state space, we would sum to infinity. There are
infinite orthornormal bases which can we can choose to span the state
space, but we have already seen that, for the action of an operator on
the state vector, considering that state vector as a combination in the
form above but with eigenvectors from the eigenbasis of that operator is
natural and fruitful because we get simplifications involving
eigenvalues, which also have clearer physical meaning. Now the
Schrödinger Equation clearly puts the Hamiltonian to the forefront of
our focus, and therefore we might like to consider the state vector
<span class="math inline">\(\Psi\)</span> when it is expressed in the
energy eigenbasis. The Schrödinger Equation states that <span
class="math display">\[i\hbar\frac{\partial \Psi(t)}{\partial
t}=\hat{H}\Psi(t).\]</span> If we take the eigenbasis of the Hamiltonian
to be <span class="math inline">\(\{\bm{\bm{\varepsilon}}_{n}\}\)</span>
and the corresponding energy eigenvalues to be <span
class="math inline">\(\{E_{n}\}\)</span> then the state vector can be
expressed as <span
class="math display">\[\Psi_{t}(x)=\sum_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\bm{\varepsilon}_{n}\]</span>
and the Schrödinger Equation now takes the form <span
class="math display">\[i\hbar\frac{\partial}{\partial
t}\sum_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\bm{\varepsilon}_{n}=\hat{H}\sum_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\bm{\varepsilon}_{n}.\]</span>
Continue by using the linear distributivity of Hermitian operators (fact
H4). This tells us that the right hand side can be written <span
class="math display">\[\hat{H}\sum_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\bm{\varepsilon}_{n}=\sum_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\hat{H}\bm{\varepsilon}_{n}=\sum_{n}E_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\bm{\varepsilon}_{n}.\]</span>
Then consider the time derivative of the quantities <span
class="math inline">\(\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\)</span>. The
eigenvectors of the Hamiltonian in the state space must be independent
of time, presuming the Hamiltonian itself isn’t varying over time (cases
with time-varying Hamiltonians are far trickier to solve and thus will
not be considered in this book). Therefore, they have 0 time derivative,
which means we have <span
class="math display">\[\frac{\partial}{\partial
t}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}=\oip{\bm{\varepsilon}_{n}}{\frac{\partial}{\partial
t}\Psi_{t}}.\]</span> One can be assured of this fact by explicitly
writing out the summation form of the inner product. Next, by
rearrangement of Schrödinger’s Equation, <span
class="math display">\[\frac{\partial}{\partial
t}\Psi_{t}=-\frac{i}{\hbar}\hat{H}\Psi_{t}\]</span> so we can substitute
this into the above expression: <span
class="math display">\[\frac{\partial}{\partial
t}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}=\oip{\bm{\varepsilon}_{n}}{-\frac{i}{\hbar}\hat{H}\Psi_{t}}=-\frac{i}{\hbar}\oip{\bm{\varepsilon}_{n}}{\hat{H}\Psi_{t}}.\]</span>
Then, substituting the energy eigenbasis expression of <span
class="math inline">\(\Psi_{t}\)</span>, <span
class="math display">\[\begin{aligned}
\frac{\partial}{\partial
t}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}&amp;=-\frac{i}{\hbar}\oip{\bm{\varepsilon}_{n}}{\hat{H}\sum_{m}\oip{\bm{\varepsilon}_{m}}{\Psi_{t}}\bm{\varepsilon}_{m}}
=-\frac{i}{\hbar}\oip{\hat{H}\bm{\varepsilon}_{n}}{\sum_{m}\oip{\bm{\varepsilon}_{m}}{\Psi_{t}}\bm{\varepsilon}_{m}}\\
&amp;=-\frac{i}{\hbar}\oip{E_{n}\bm{\varepsilon}_{n}}{\sum_{m}\oip{\bm{\varepsilon}_{m}}{\Psi_{t}}\bm{\varepsilon}_{m}}
=-\frac{i}{\hbar}E^{\ast}_{n}\oip{\bm{\varepsilon}_{n}}{\sum_{m}\oip{\bm{\varepsilon}_{m}}{\Psi_{t}}\bm{\varepsilon}_{m}}\\
&amp;=-\frac{i}{\hbar}E_{n}\oip{\bm{\varepsilon}_{n}}{\sum_{m}\oip{\bm{\varepsilon}_{m}}{\Psi_{t}}\bm{\varepsilon}_{m}}
\end{aligned}\]</span> where the fact that <span
class="math inline">\(\hat{H}\)</span> is hermitian is used in the
algebraic manipulations. Then, as the inner product is linearly
distributive across the sum term, this becomes <span
class="math display">\[\frac{\partial}{\partial
t}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}=-\frac{i}{\hbar}E_{n}\sum_{m}\oip{\bm{\varepsilon}_{n}}{\oip{\bm{\varepsilon}_{m}}{\Psi_{t}}\bm{\varepsilon}_{m}}\]</span>
and as the inner product <span
class="math inline">\(\oip{\bm{\varepsilon}_{m}}{\Psi_{t}}\)</span> is
some constant for fixed <span class="math inline">\(m\)</span> and t, we
can pull it out to write the expression as <span
class="math display">\[\frac{\partial}{\partial
t}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}=-\frac{i}{\hbar}E_{n}\sum_{m}\oip{\bm{\varepsilon}_{m}}{\Psi_{t}}\oip{\bm{\varepsilon}_{n}}{\bm{\varepsilon}_{m}}=-\frac{i}{\hbar}E_{n}\sum_{m}\oip{\bm{\varepsilon}_{m}}{\Psi_{t}}\delta_{nm},\]</span>
which is, <span class="math display">\[\frac{\partial}{\partial
t}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}=-\frac{i}{\hbar}E_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\]</span>
as the Kronecker delta resulting from the orthogonality of the
eigenvectors cancels out all other sum terms except for when the index
<span class="math inline">\(m\)</span> matches up with <span
class="math inline">\(n\)</span>. This is clearly equivalent to the
differential equation <span class="math display">\[\frac{\partial
y}{\partial t}=ky\]</span> which has general solution <span
class="math inline">\(x=Ce^{kt}\)</span> for some constant of
integration <span class="math inline">\(C\)</span>. One might consider
that the inner product is a constant and therefore not a traditional
function one might find in differential equations of this form, but we
recall that constants can be seen as functions of the form <span
class="math inline">\(f(x)\equiv c\)</span>. Substituting <span
class="math inline">\(x:=\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\)</span>
and <span class="math inline">\(k:=-\frac{i}{\hbar}E_{n}\)</span>
analogously leaves us with the solution <span
class="math display">\[\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}=Ce^{-\frac{iE_{n}t}{\hbar}}.\]</span>
The final step is to realise the constant <span
class="math inline">\(C\)</span> is not random: at <span
class="math inline">\(t=0\)</span> we should have <span
class="math inline">\(\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}=C\)</span>,
which implies that the constant is <span
class="math inline">\(C=\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}\)</span>.
Thus we conclude that the rule for time-evolution is <span
class="math display">\[\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}=\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}e^{-\frac{iE_{n}t}{\hbar}}
\:\:\:\:\square\]</span><br />
<br />
All the above suggest the critical importance of the energy eigenstates.
Finding these energy eigenstates essentially amounts to solving the SE!
So we therefore call the eigenvalue equation of the Hamiltonian its own
name: the <strong>time-independent Schrödinger Equation (TISE)</strong>:
<span class="math display">\[\hat{H}\Psi_{n}=
\biggl(\frac{\hat{P}^2}{2m}+V(\hat{X})\biggr)\Psi_{n}=E_{n}\Psi_{n}\]</span>
for the energy eigenstates <span
class="math inline">\(\Psi_{n}\)</span>. We will continue to refer to
this as the TISE, and we will refer to the original Schrödinger Equation
<span class="math display">\[i\hbar \frac{\partial \Psi}{\partial t} =
\hat{H}\Psi\]</span> as the time-dependent Schrödinger Equation, as it
is time-dependent.<br />
<br />
Many consequences for the solution of Schrödinger’s Equation derive
themselves promptly. We list them in the taxonomical format again.<br />
</p>
<ol>
<li><p>We first note what the fact above actually means. By the rule S8,
the term <span
class="math inline">\(\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\)</span> is
the component of <span class="math inline">\(\Psi_{t}\)</span> in the
eigenbasis <span class="math inline">\(\{\bm{\varepsilon}_{i}\}\)</span>
corresponding to eigenvector <span
class="math inline">\(\bm{\varepsilon}_{n}\)</span>: <span
class="math display">\[\Psi_{t}=\sum_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\bm{\varepsilon}_{n}.\]</span>
The eigenvector <span
class="math inline">\(\bm{\varepsilon}_{n}\)</span> itself does not
evolve with time. Therefore, by determining how the component <span
class="math inline">\(\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\)</span>
evolves with time, we get: <span
class="math display">\[\Psi_{t}=\sum_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}\bm{\varepsilon}_{n}=\sum_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}e^{-\frac{iE_{n}t}{\hbar}}\bm{\varepsilon}_{n}.\]</span>
We cannot fall for the common deception of thinking we can pull out the
term <span class="math inline">\(e^{-{iE_{n}t}/{\hbar}}\)</span> from
the sum, since the eigenvalues <span
class="math inline">\(\setof{E_{n}}\)</span> corresponding to the
eigenvectors <span
class="math inline">\(\setof{\bm{\varepsilon}_{n}}\)</span> change for
each <span class="math inline">\(n\)</span> so it is not a constant.
However, we do know now that, if we can determine the eigenvectors of
the Hamiltonian for a given system, and the corresponding eigenvalues,
and know the initial state (and therefore the components <span
class="math inline">\(\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}\)</span> of
the initial state vector) then we have a fully defined state <span
class="math inline">\(\Psi_{t}\)</span> for any <span
class="math inline">\(t\)</span> as we can track how each of its
components evolve very easily. This amounts, of course, to solving the
Schrödinger Equation: or, more satisfyingly put, solving the problem of
time evolution and solving both the central problems of quantum
mechanics.</p></li>
<li><p>An important clarification which has been alluded to but not
explicitly stated thus far must now be emphasised. There is no physical
operator which changes form over time. Most operators, like the position
and momentum operators, do not change even if there is a perturbation to
the system. However, the Hamiltonian is not the same for all systems.
Like all other observables operators, its formulation does not change-
but, unlike most other operators, this formulation consists of something
which can vary through perturbations. Specifically, the Hamiltonian is
expressed as <span
class="math display">\[\hat{H}=\frac{\hbar^{2}}{2m}\frac{\partial^2}{\partial
x^2}+V(\hat{X})\]</span> where <span class="math inline">\(x\)</span>
here is the position variable and the function <span
class="math inline">\(V(\hat{X})\)</span> is the potential of the
system. It is the potential which changes the form of the operator, as
different systems have different potentials; moreover, these potentials
can shift over time so in that way the form of the Hamiltonian changes
over time. Note the difference: the formula to construct the Hamiltonian
doesn’t change, but the Hamiltonian itself does– when <span
class="math inline">\(V(\hat{X})\)</span> changes.<br />
<br />
The role of energy eigenbases as the easiest way to solve the
Schrödinger Equation is now however fully clear to us. For a given
system, if we can formulate the Hamiltonian accurately, then we can
attempt to find its eigenvalues and corresponding eigenvectors using the
characteristic equation, and then by SE1 we can theoretically determine
the answer to the time evolution problem for the state with that
Hamiltonian. This is why almost invariably, for a given quantum system
of interest for which we are trying to understand the dynamics (that is,
the time evolution) the goal of the physicists are:</p>
<ol>
<li><p>Formulate the Hamiltonian. This can be exceptionally difficult,
for example if you have a group of electrons interacting with each other
for which the form of the potential is very difficult to derive. Many
problems in ongoing research are to find the Hamiltonian of relevant
systems.</p></li>
<li><p>Try to solve, either analytically or by numerical approximation,
what the eigenstates of the given Hamiltonian are. This can also be
essentially impossible for some problems, which is why considerably
ingenuity in algebraic manipulation and improved approximation schemes
is needed. Looking for such techniques is also a very important part of
modern-day quantum research.</p></li>
</ol></li>
<li><p>By Postulate 3, the value <span
class="math inline">\(|\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}|^2\)</span>
is the probability energy value <span
class="math inline">\(E_{n}\)</span> is measured to be the value of
energy for the system at time <span class="math inline">\(t\)</span>. In
computing this amplitude we achieve very interesting results: <span
class="math display">\[\begin{aligned}
    |\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}|^2
&amp;=  |\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}e^{-\frac{iE_{n}t}{\hbar}}|^2\\
    &amp;=|\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}|^2e^{-\frac{iE_{n}t}{\hbar}}e^{\frac{iE_{n}t}{\hbar}}\\
    &amp;=|\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}|^2\times 1\\
    &amp;=|\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}|^2
    \end{aligned}\]</span> In other words, the probability of measuring
the energy <span class="math inline">\(E_{n}\)</span> at time <span
class="math inline">\(t\)</span>, represented by <span
class="math inline">\(|\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}|^2\)</span>,
is the exact same as the probability of measuring that energy at time 0,
which is <span
class="math inline">\(|\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}|^2\)</span>.
In another sense this means that, unless there is some perturbation to
our system, there is no change in the probability of a certain energy
value being measured; this rule of time evolution essentially amounts to
the energy conservation law for a closed unperturbed system as whatever
energy we measure it to have at time 0 stays the same for all time <span
class="math inline">\(t\)</span>.<br />
<br />
This is not the last of this important result! We once more see that the
above holds for any observables with the eigenbasis <span
class="math inline">\(\{\bm{\varepsilon}_{i}\}\)</span> which is the
same as the energy eigenbasis, as the energy values themselves cancel
out, leaving the component amplitudes purely in terms of the
eigenvectors. This in fact means that for other observables compatible
with the energy- for other observables whose operators commute with the
Hamiltonian– the probability of making a measurement of the eigenvalue
corresponding to a certain eigenvector is also constant over time. This
is significant, as we have just proven the quantum mechanical
requirement for some observable to be a <strong>constant of
motion</strong>.</p></li>
<li><p>Consider the case when the state vector is in a pure energy
eigenstate- when <span
class="math display">\[\Psi_{t}=\bm{\varepsilon}_{k}\]</span> for some
<span class="math inline">\(k\)</span>. Then the probabilities of
measuring the eigenvalues <span class="math inline">\(E_{n}\)</span> are
<span
class="math display">\[|\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}|^{2}=|\oip{\bm{\varepsilon}_{n}}{\bm{\varepsilon}_{k}}|^{2}=\delta_{nk},\]</span>
which means that the probability of measuring the energy <span
class="math inline">\(E_{k}\)</span> is 1 and the probability of
measuring all other energies <span class="math inline">\(E_{n\neq
k}\)</span> is 0. This is the deterministic energy pure state, whose
relevance is clear due to the heavy discussion following Postulate 3 on
measurement and quantum states. Yet there is more to be said: <span
class="math display">\[\oip{\bm{\varepsilon}_{n}}{\Psi_{t}}=\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}e^{-{iE_{n}t}/{\hbar}}\]</span>
so the state vector would be <span
class="math display">\[\Psi_{t}=\sum_{n}\oip{\bm{\varepsilon}_{n}}{\bm{\varepsilon}_{k}}e^{-{iE_{n}t}/{\hbar}}\bm{\varepsilon}_{n}=\sum_{n}\delta_{nk}e^{-{iE_{n}t}/{\hbar}}\bm{\varepsilon}_{n}=e^{-{iE_{k}t}/{\hbar}}\bm{\varepsilon}_{k}.\]</span>
However, this is the same as the system at time <span
class="math inline">\(0\)</span> because the exponential has modulus 1
and therefore does not change the eigenvector in the Hilbert space from
the ray <span class="math inline">\(\bm{\varepsilon}_{k}\)</span> which
represents the initial state. Thus the whole system does not change at
all if it starts in a pure energy eigenstate; therefore, all observables
remain constant under time evolution so long as the Hamiltonian remains
the same.<br />
<br />
The consequence of this is that we can define the vectors: <span
class="math display">\[\forall
n\in\mathbb{Z}^+,\:\:\:\:\Psi_{t}^{(n)}:=e^{-iE_{n}t/\hbar}\bm{\varepsilon}_{n}.\]</span>
and these vectors <span class="math inline">\(\Psi_{t}^{(n)}\)</span>
are the deterministic forms of the state vector if at initial state for
<span class="math inline">\(t=0\)</span> the state vector is coincident
with the eigenvector <span
class="math inline">\(\bm{\varepsilon}_{n}\)</span>. We call these
<strong>stationary states</strong>, which belong to the specific system,
as their value is contingent on the eigenbasis of the Hamiltonian which
describes the system. Now, if at time 0 the system is in a pure energy
eigenstate <span class="math inline">\(\bm{\varepsilon}_{n}\)</span>,
then: <span class="math display">\[\Psi_{t}=\Psi_{t}^{(n)}.\]</span>
Otherwise, if it is in a mixed state at time 0, then <span
class="math display">\[\Psi_{t}(x)=\sum_{n}e^{-iE_{n}t/\hbar}\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}\bm{\varepsilon}_{n}\]</span>
by SE1, which can then be written as <span
class="math display">\[\Psi_{t}=\sum_{n}\oip{\bm{\varepsilon}_{n}}{\Psi_{0}}\Psi_{t}^{(n)}\]</span>
since <span class="math inline">\(\Psi_{0}^{(n)}\)</span> (the
stationary state at time <span class="math inline">\(0\)</span>) is the
initial pure state corresponding to stationary state <span
class="math inline">\(n\)</span>, which is the eigenvector <span
class="math inline">\(\bm{\varepsilon}_{n}\)</span>. This gives us a
physically meaningful method of describing the solution to the
Schrödinger Equation: finding its stationary states along with knowledge
of the initial state is sufficient to solve the Schrödinger
Equation.<br />
<br />
Note that the solution now does not give us a deterministic state if the
initial state was not a pure state; we get a mixed state in terms of the
stationary states. Thus we might be confused as to how it amounts to a
solution: but we recall that a fully defined mixed state is most of the
time tantamount to achieving the highest level of understanding
physically possible, by the measurement postulates– so it does count as
a solution since it represents the precise superposition of possible
states we get in reality. Finally, the solution of finding the
stationary states is absolutely the same as the solution detailed in SE2
and SE3– we’ve just labelled vectors to be stationary states, but the
underlying procedure is still simple. This is:</p>
<ol>
<li><p>Formulate the Hamiltonian for the system</p></li>
<li><p>Solve the time-independent Schrödinger, or energy eigenvalue
equation.</p></li>
<li><p>Formulate the state vector as a function of time and the initial
state as shown above.</p></li>
</ol></li>
</ol>
<h3 id="free-particle">Free Particle</h3>
<p>The first problem we can easily solve is the solution of the time
evolution of a free particle. A free particle is a particle in a zero
potential: so <span class="math inline">\(V(\hat{X})=0\)</span>. We
begin by listing out our procedure:</p>
<ol>
<li><p>List the boundary conditions.</p></li>
<li><p>Formulate the Hamiltonian and solve its eigenvalue
equation.</p></li>
<li><p>Use the eigenvectors and their eigenvalues to formulate the
stationary states and therefore the state vector.</p></li>
</ol>
<p>An important thing to note is that this procedure only applies for
systems where the Hamiltonian remains constant. This occurs if and only
if the potential of the system does not vary with time– we recall that
the Hamiltonian consists of the kinetic energy operator and therefore is
not affected by the kinetic energy of the particle changing! The energy
of the state will be affected, but the operator representing all
possible energy states will not because the kinetic energy operator is
constant. For the questions in these chapters, as well as for most
introductory quantum mechanical problems, time-varying potentials will
not be considered, as they are extremely complex and do not have any
place in the rudimentary foundations of quantum mechanics.<br />
<br />
Let us now solve this simplest problem in quantum mechanics with this
procedure.</p>
<h4 class="unnumbered" id="list-the-boundary-conditions">List the
Boundary Conditions</h4>
<p>The “boundary conditions" of a problem are simply all the conditions
the problem sets up for a solution. Listing them at this stage can be
very useful because there are many times, for example when evaluating
integrals or probabilities, that the boundary conditions can provide
insights to shortcuts where there seem like there are too many obstacles
to a solution. For now, though, this as a separate step will not be
vindicated considering there is one boundary condition specified here
only.</p>
<ol>
<li><p>The potential energy is <span
class="math inline">\(0\)</span>.</p></li>
</ol>
<h4 class="unnumbered" id="formulate-the-hamiltonian">Formulate the
Hamiltonian</h4>
<p>This is also quite easy, as we do not have to consider the potential.
<span
class="math display">\[\hat{H}:=-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial
x^2}+V(\hat{X}), \stab V(\hat{X})=0 \Rightarrow
\hat{H}=-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\]</span>
Solving the eigenvalue equation will take more work since this is the
first time we have done it for such a complicated problem. We want to
find eigenstates and eigenvalues which satisfy the eigenvalue equation
<span
class="math display">\[\hat{H}\bm{\varepsilon}_{n}=E_{n}\bm{\varepsilon}_{n}\]</span>
for real <span class="math inline">\(E_{n}\)</span>. This equation is,
as aforementioned, also known as the time-independent Schrödinger
Equation in literature. We can solve it for the free particle relatively
easily. The Hamiltonian is <span
class="math display">\[\hat{H}\Psi(x)=\frac{\hat{P}^2}{2m}\Psi(x)\]</span>
and so the eigenvalue equation (time-independent Schrödinger) is to find
eigenvectors <span
class="math inline">\(\setof{\bm{\varepsilon}_{n}}\)</span> which
satisfy: <span
class="math display">\[\frac{\hat{P}^2}{2m}\bm{\varepsilon}_{n}=E_{n}\bm{\varepsilon}_{n}\]</span>
for some real constant eigenvalues <span
class="math inline">\(E_{n}\)</span>. The crucial insight for this
eigenvalue equation is that the Hamiltonian operator of the system
commutes with the momentum operator! This is intuitively obvious, since
the Hamiltonian operator is the momentum operator squared divided by a
constant, and the momentum operator is just the momentum operator, so
the commutator <span
class="math display">\[\left[\frac{\hat{P}^2}{2m},\hat{P}\right]\]</span>
consists of only the operator <span
class="math inline">\(\hat{P}\)</span>, somewhat modified on one side.
Constants clearly do not affect commutators and operators commute with
themselves, so we would expect the commutator to be <span
class="math inline">\(0\)</span>. It can be verified by the reader as
well, if we put in <span
class="math inline">\(\hat{P}=-i\hbar\nd{}{x}\)</span>, but this is not
greatly necessary.<br />
<br />
From our vigilance, however, in checking the commutation relation
between the Hamiltonian and momentum operators (over time, one gains an
intuitive feeling for this vigilance) we achieve something much greater.
As their operators commute, energy and position are compatible
observables. And as they are compatible observables, they must possess a
common eigenbasis! Therefore, to any energy eigenvector <span
class="math inline">\(\bm{\varepsilon}_{n}\)</span> there also exists a
momentum eigenvector <span class="math inline">\(\phi_{n}\)</span> which
is the same function! Let us therefore try to put that in instead, with
this information. For all energy eigenvectors <span
class="math inline">\(\bm{\varepsilon}_{n}\)</span> such that <span
class="math display">\[\hat{H}\bm{\varepsilon}_{n}=\frac{\hat{P}^2}{2m}\bm{\varepsilon}_{n}=E_{n}\bm{\varepsilon}_{n},\]</span>
there must exist a momentum eigenvector such that <span
class="math display">\[\phi_{n}\equiv\bm{\varepsilon}_{n}\implies
\frac{\hat{P}^2}{2m}\phi_{n}=E_{n}\phi_{n}.\]</span> Now, if the
eigenvalue of momentum corresponding to the eigenvector <span
class="math inline">\(\phi_{n}\)</span> is <span
class="math inline">\(P_{n}\)</span>, then the eigenvalue corresponding
to <span class="math inline">\(\hat{P}^2\)</span> on <span
class="math inline">\(\phi_{n}\)</span> is <span
class="math inline">\(P_{n}^{2}\)</span>. So the above can be written
<span class="math display">\[\begin{aligned}
\frac{\hat{P}^2}{2m}\phi_{n}=E_{n}\phi_{n}\iff\frac{P_{n}^{2}}{2m}\phi_{n}=E_{n}\phi_{n}.
\end{aligned}\]</span> The eigenfunction <span
class="math inline">\(\phi_{n}\)</span> is clearly not the null vector,
and therefore the above implies that the <span
class="math inline">\(n\)</span>’th eigenmomenta and eigenenergies are
related by: <span
class="math display">\[\frac{P_{n}^{2}}{2m}=E_{n}\implies
P_{n}=\pm\sqrt{2mE}.\]</span> We can now try to find the free particle
wavefunction. Replacing the momentum operator with its algebraic
formulation (in position space, as we have been working thus far), we
have <span
class="math display">\[-{\frac{\hbar^2}{2m}}\frac{d^2\Psi}{dx^2} = E\Psi
\Rightarrow\:\: \frac{d^2\Psi}{dx^2} =-{\frac{2mE}{h^2}}\Psi.\]</span>
We could also express this as <span
class="math display">\[\frac{d^2\Psi}{dx^2}
=-{\frac{p^2}{h^2}}\Psi\]</span> since we know that for the free
particle <span class="math inline">\(E=p^2/2m\)</span> as just shown. We
have already seen that momentum has extra importance in the free
particle problem as it is compatible with energy (though that is not to
say this is the only such question where this may be true); therefore,
the momentum eigenstates are also the energy eigenstates, which we know
are very important due to their intimate relationship with time
evolution. It is also very clear that the free particle problem is a
fundamental conceptual problem. De Broglie therefore defined a
relationship <span class="math display">\[k=p/\hbar \implies p = \hbar
k\]</span> (where the latter form is far more commonly seen) for a
constant <span class="math inline">\(k\)</span> which we will now plug
into the equation we have above: <span
class="math display">\[\frac{d^2\Psi}{dx^2} =-{\frac{p^2}{h^2}}\Psi,
\stab p=\hbar k \implies \frac{d^2\Psi}{dx^2} =-k^2\Psi.\]</span> The
physical meaning of the constant <span class="math inline">\(k\)</span>
is somewhat tangential for this discussion, though it must be remarked
that the De Broglie relations concern all the important aspects of a
classical wave and are not simply random definitions. Nevertheless, for
our purposes working with <span class="math inline">\(k\)</span> instead
of <span class="math inline">\(p/\hbar\)</span> will be cleaner for the
algebra to follow. A reader should clearly see that the equation we have
just reached is perfectly analogous to the rudimentary differential
equation <span class="math display">\[\frac{d^2y}{dx^2}=-k^2y.\]</span>
Which has the general solution <span
class="math display">\[y=Ae^{-ikx}+Be^{ikx}:=\Psi.\]</span> This is
therefore the general solution to the free particle wavefunction where
constants <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are to be determined based on further
boundary conditions of the physical problem! It looks generic, but when
we do set bounds: whether these be positional bounds or any other
bounds, we will see that we can suddenly get quite interesting and
specific forms for the wavefunction. Such is for example shown in part
8.2, where we see the free particle confined to an ellipse and get a
very clear reult.</p>
<h2 id="a-holistic-summary">A Holistic Summary</h2>
<p>The meaning of these last three chapters has always been to provide a
robust defence against the many conceptual pitfalls which one can fall
into if they try to educate themselves on quantum mechanics purely
mathematically without grasping the underlying ideas fully. Certainly,
the rest of the book will be far more dense mathematically- though
effort will still be made to be extremely clear with that discourse. It
is therefore important, given that I have offered such a verbose
discourse on the relationship between quantum mechanics and physical
reality, to summarise everything we have learnt in a concise way; by
revising this section, the reader should be able to answer all the
physical questions they have about quantum mechanics.<br />
<br />
The problem of a physical model consists of two components, the state
problem and the time-evolution problem. The state problem consists of
two main problems: the first, how we represent physical information; the
second, how we extract physical information.<br />
<br />
<strong><u>The representation of physical information</u></strong></p>
<ul>
<li><p>The representation of physical information is carried out by the
state vector and its wavefunctions in quantum mechanics. It is
represented by a normed Hilbert space vector; all unphysically scaled
multiples are considered the same ray so represent the same
state.</p></li>
<li><p>In the book we have referred to the state vector at times as
“storing information", which is somewhat of a shorthand and can be
confusing. What really is true is that the state vector is given meaning
by the relation <span
class="math display">\[P(\bm{\alpha}_{i})=|\oip{\bm{\alpha}_{i}}{\Psi}|^{2}\]</span>
for any arbitrary eigenstate <span
class="math inline">\(\bm{\alpha}_{i}\)</span>, which means we can
convert it to wavefunctions which are probability distribution
functions. Subsequently, the wavefunction does store information because
it is defined by its components, which must correspond to specific
eigenvectors and therefore are probability amplitudes. As every
arbitrary state vector must have a unique expansion in every basis
spanning the space, every state vector therefore inherently can be
converted to its expression in any space, and therefore every state
vector possesses components in observable eigenspaces- which are,
probability amplitudes of measurements with respect to those physical
observables whose eigenspace it is in. That is how a state vector
encapsulates information.</p></li>
<li><p>Physical observables are represented by hermitian operators,
which do not change over time. The distinct eigenvectors of these
observable operators and the corresponding eigenvalues are also
therefore unchanging over time. The Hamiltonian is an exception, but not
because its formulation changes over time, but because its form changes
over time as the potential <span
class="math inline">\(V(\hat{X})\)</span> may change over time.</p></li>
<li><p>The vector space expressed as being spanned by an observable’s
eigenbasis is called its eigenspace. It is especially useful when
considering measurements of that observable, since the components of the
state vector in that eigenbasis are the probability amplitudes
corresponding to measurements of the observable.</p></li>
</ul>
<h2 id="exercises-from-chapter-5ast">Exercises from Chapter 5<span
class="math inline">\(\ast\)</span></h2>
<ol>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>
<hr>
<div style="text-align: center; font-size: 1.2em; padding-top: 20px;">
    <a href='5.html'>&larr; Previous Chapter</a> &nbsp;&nbsp; | &nbsp;&nbsp; <a href='7.html'>Next Chapter &rarr;</a>
</div>
</body>
</html>
