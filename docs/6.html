<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>6</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <script>
      window.MathJax = {
        loader: { load: ['[tex]/newcommand'] },
        tex: {
          packages: { '[+]': ['base', 'ams', 'newcommand'] },
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
      };
      </script>
      <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
    $$
    \newcommand{\Answer}{\begin{tcolorbox}}
    \newcommand{\Answerend}{\end{tcolorbox}}
    \newcommand{\ket}[1]{|#1\rangle}
    \newcommand{\bra}[1]{\langle#1|}
    \newcommand{\ip}[2]{\langle#1|#2\rangle}
    \newcommand{\bip}[2]{\left\langle#1\middle|#2\right\rangle}
    \newcommand{\qexp}[1]{\langle#1\rangle}
    \newcommand{\apos}[1]{``#1"}
    \newcommand{\sapos}[1]{`#1'}
    \newcommand{\elec}{e^{-}}
    \newcommand{\uspin}{(\uparrow)}
    \newcommand{\dspin}{(\downarrow)}
    \newcommand{\lspin}{(\leftarrow)}
    \newcommand{\rspin}{(\rightarrow)}
    \newcommand{\ulspin}{(\uparrow\leftarrow)}
    \newcommand{\urspin}{(\uparrow\rightarrow)}
    \newcommand{\dlspin}{(\downarrow\leftarrow)}
    \newcommand{\drspin}{(\downarrow\rightarrow)}
    \newcommand{\R}{\mathbb{R}}
    \newcommand{\stab}{\:\:}
    \newcommand{\mtab}{\:\:\:}
    \newcommand{\btab}{\:\:\:}
    \newcommand{\imp}{\Rightarrow}
    \newcommand{\doubimp}{\Leftrightarrow}
    \newcommand{\setof}[1]{\{#1\}}
    \newcommand{\infint}{\int_{-\infty}^{\infty}}
    \newcommand{\trans}[1]{\mathcal{T}(#1)}
    \newcommand{\dd}[2]{\delta(#1-#2)}
    \newcommand{\ipbig}[2]{\langle#1|#2\rangle}
    \newcommand{\talpha}{\tilde{\alpha}}
    \newcommand{\op}[2]{|#1\rangle\langle#2|}
    \newcommand{\sop}[1]{|#1\rangle\langle#1|}
    \newcommand{\prop}[2]{\mathcal{U}(#1,#2)}
    \newcommand{\propdagg}[2]{\mathcal{U}^{\dagger}(#1,#2)}
    \newcommand{\sip}[1]{\langle#1|#1\rangle}
    \newcommand{\optrip}[3]{\langle#1|\hat{#2}|#3\rangle}
    \newcommand{\nhoptrip}[3]{\langle#1|{#2}|#3\rangle}
    \newcommand{\northexp}[2]{\sum_{i=1}^{n}|#2\rangle\langle#2|#1\rangle}
    \newcommand{\orthexp}[4]{\sum_{#3=1}^#4|#2\rangle\langle#2|#1\rangle}
    \newcommand{\schrodeq}{i\hbar\frac{\partial \Psi(x,t)}{\partial t}=\hat{H}\Psi(x,t)}
    \newcommand{\nd}[2]{\frac{d#1}{d #2}}
    \newcommand{\snd}[2]{\frac{d^{2}#1}{d#2^2}}
    \newcommand{\pd}[2]{\frac{\partial#1}{\partial #2}}
    \newcommand{\spd}[2]{\frac{\partial^{2}#1}{\partial #2^2}}
    \newcommand{\duac}{\leftrightarrow}
    \newcommand{\oip}[2]{\left(#1,#2\right)}
    \newcommand{\obip}[2]{\left(#1,#2\right)}
    $$
<h1 id="dirac-notation-and-matrix-mechanics">Dirac Notation and Matrix
Mechanics</h1>
<p>The second part of this book, starting now, will be far more
mathematically demanding. It seeks to complete the original aim of this
pragmatic book: to allow a reader to segue onto mathematically advanced
quantum mechanics texts without chronic confusions of what the
mathematics <em>means</em>. Particularly, we will now tackle the
challenging case of extending our quantum mechanics to cover
continuously distributed observables.<br />
<br />
We have covered now all of the quantum mechanical postulates we need to
cover. On the other hand, our problem solving skills, which are very
different in quantum mechanics to just standard algebra, are still very
limited in the wider scope of things. There are many manipulative
techniques we have not covered which are fundamental, but far from
trivial. Furthermore, learning these techniques will require a
reformulation of almost everything we have done so far into a new
notation. This switch will be strange and unwelcome at first, but sooner
than later the reader will find that the new notation is far more useful
than confusing, and at that point they will be on a very good path
indeed.<br />
<br />
The benefit of this somewhat counterintuitive approach of learning one
way and then reformulating into a new notation is not only that we don’t
have several factors affecting our understanding at once and causing
trouble, but that the reader gets training to an undoubtedly
undergraduate level without wrecking their ability to appreciate nuances
as they grapple with difficult problems. Without a wider context of
measurements and probabilities and state evolution within which to frame
the mathematics in this book, it would seem like an endless barrage of
symbols for which we have no use and no appreciation. On the other hand,
after reading the previous chapters, we are capable of understanding why
components are important– they are probability amplitudes; how the state
vector is both a label and can be used as a substitute for a physical
state; how operators are central to the measurements we achieve for
different operators. With all this conceptual learning done, in many
ways our task from hereon is relatively simple– change notation and
crunch algebra without any other concerns any more! Remember what
started things off. That <span class="math inline">\(\uspin\)</span>
notation, which was designed to show how unimportant symbols themselves
are, was where our discussion started, and we now come full circle to
this idea that notation is far from sacred and must be evaluated simply
based on pragmatic convenience. This new Dirac notation we are switching
too will certainly make things more convenient, even though the state
vector is still a state vector and the wavefunctions are still
wavefunctions and the operators are still operators. Subsequently,
looking in greater detail at the mathematical processes of quantum
mechanics will become easier with our increased ability to engage in
more efficient algebra. Nothing has changed. We are just switching from
more intuitive symbols to the proper quantum mechanical convention of
Dirac notation, which one might not have seen before but which needn’t
be mystified.<br />
<br />
In addition, it is simply necessary to cover the conventional notation,
called Dirac notation for its founder, and the pertinent techniques
which follow it, because there does not exist a textbook whose level is
a step up from this one and which does not use such conventions and even
in many cases assume it is prior mastered. We will also see that it
would have been difficult to understand what it means outright before we
understood what a state vector is, for example, but on the other hand
now that we do understand what the state vector is, it will make problem
solving more powerful without any danger of confusions.<br />
<br />
In this chapter, we will set forwards some new limitations so we can
focus on the algebraic content (given that we understand the concepts of
the postulates quite well now). The primary limitation will be the one
maintained in previous chapters: that we are working in discrete cases.
The next chapter will be all dealing with continuity, which contains
some extremely difficult mathematics resulting from the mathematics in
this chapter, so this limitation will be soon loosened. We will also be
avoiding any thoughts about the wavefunctions being the primary
representations of abstract state vectors, because we will see that with
Dirac’s notation comes another natural and powerful idea of matrix
representations! We can also feel free to ignore measurements and
related probabilities so that we can focus on developing algebraic
techniques which will make problem solving more efficient and workable,
because we already understand the postulates we need to therein. So for
now, the job is mainly to forget the wavefunction, and focus on the more
explicitly state vector (and matrix) focused approach we will start
below. We appreciate throughout that wavefunctions (and therefore
measurement probabilities) are still obtainable, by taking the inner
product of a state vector with the relevant input basis vectors in that
component function transformation we have seen, so its absence is far
from its demise.</p>
<h2 id="dirac-bra-ket-notation">Dirac Bra-Ket Notation</h2>
<p>Our new notation begins here. Like any new notation, our job will be
changing the way we label the objects we want to consider. The starting
point will be the state vector, and its notation. We will not change any
properties of the state vector! Things may look different because the
notation looks nothing like we have ever seen before, but that does not
mean at all that the characteristics are different.<br />
<br />
The state vector will now be written as <span
class="math inline">\(\ket{\Psi}\)</span>. This called
<strong>ket</strong> psi. We simply call it a ket instead of vector
because Dirac notation is so ubiquitous in quantum mechanics that its
authority over the discourse of quantum mechanics has turned the word
ket into a common noun in itself. The ket and the state vector are one
and the same! The only difference is that we now use the notation <span
class="math inline">\(\ket{\Psi}\)</span> instead of the notation <span
class="math inline">\(\Psi\)</span>.<br />
<br />
The space of possible physical states will now be called the ket space
instead of the state space. Obviously, since kets are the same as state
vectors, kets are still in bijection with physical states. The vectors
which span the ket space are also kets themselves. Therefore, an
arbitrary ket can be expressed with components <span
class="math inline">\(\setof{c_{i}}\)</span> as <span
class="math display">\[\ket{X}:=\sum_{i=1}^{n}c_{i}\ket{\alpha_{i}}\]</span>
for a basis set <span
class="math inline">\(\setof{\ket{\alpha_{i}}}\)</span>. We can define
scalar multiplication exactly as we have before: <span
class="math display">\[k\times\ket{X}=k\sum_{i=1}^{n}c_{i}\ket{\alpha_{i}}=\sum_{i=1}^{n}(kc_{i})\ket{\alpha_{i}}\]</span>
where the scalar multiplies each component of the vector to give it new
components. Addition is defined by summing corresponding components as
well: <span
class="math display">\[\ket{X}:=\sum_{i=1}^{n}c_{i}\ket{\alpha_{i}},
\ket{Y}:=\sum_{i=1}^{n}c&#39;_{i}\ket{\alpha_{i}}\implies
\ket{X}+\ket{Y}=\sum_{i=1}^{n}(c+c&#39;_{i})\ket{\alpha_{i}}\]</span><br />
<br />
An extremely important point, again, is that the ket is an abstract
object. Recall that the state vector could not be given a form until we
chose a specific basis. Since the ket is the same, we will here need to
chose a basis for the ket to take a form. The key novelty of our new way
of approaching the same ideas is that, in our bid to make our state
vector, now ket, more , we will this time try to divorce the ket from
the idea of a wavefunction representation and replace this with yet
another representation in bijection to the state vector– a matrix
representation! This is because ideas of matrices will be very useful to
us in our work and manipulation. To do this, recall that in any basis a
constituent of any vector space has unique components in that basis,
most often referred to as its unique expansion in that basis. This
allows us to create a unique matrix expression of any ket: for any <span
class="math display">\[\ket{X}:=\sum_{i=1}^{n}c_{i}\ket{\alpha_{i}}\]</span>
in the basis <span
class="math inline">\(\setof{\ket{\alpha_{i}}}\)</span>, we can express
it as a column vector of its components <span
class="math display">\[\ket{X}=\begin{bmatrix}
c_{1}\\
c_{2}\\
\vdots\\
c_{n}\\
\end{bmatrix}\]</span> and this is a unique way to express the ket in
that basis. As all kets have the same dimensions in this matrix
expression (since the dimension of the ket space is the same so the
number of components specifying them are the same), we can form any
linear combinations of them since we are just summing matrices of the
same dimensions, which is acceptable, and this matrix expression will
not be meaningless.<br />
<br />
Another important point is for the inner labelling of kets: i.e- <span
class="math display">\[\ket{\textbf{Inner Label}}.\]</span> The inner
label of a ket never has any mathematical meaning. We can label kets
<span class="math display">\[\ket{1}, \ket{2}, ... \ket{5}\]</span> if
we want to. They are absolutely nothing to do with the numbers <span
class="math inline">\(1\)</span> to <span
class="math inline">\(5\)</span>. Instead, the inner label serves to
<strong>organise</strong> different kets: above, we have ordered ket
<span class="math inline">\(1\)</span>, ket <span
class="math inline">\(2\)</span>, ket <span
class="math inline">\(3\)</span>, and so on in some arbitrary way which
is meaningful to us. We could also label them <span
class="math display">\[\ket{\text{Dirac}}, \ket{\text{Heisenberg}},
\ket{\text{Feynman}}\]</span> except this provides no logical order for
us and is long to write. Sometimes one will see some long-winded labels,
such as <span
class="math display">\[\ket{P=\sqrt{{2mE}/{\hbar}}\:}\]</span> which is
an example taken from Chapter 8, where we needed to label a ket by its
momentum value <span class="math inline">\(\sqrt{{2mE}/{\hbar}}\)</span>
to distinguish it from another ket with another momentum value (but
otherwise identical characteristics). One should not be confused if
there are numerical numbers as inner labels of kets: they are part of a
taxonomy we have chosen ourselves. For example, in this book and
ubiquitously in all books, convention is to denote eigenkets by the
eigenvalue they represent. This is not to say that the label is
physically meaningful– we could easily label each eigenket by the square
of its eigenvalue, for example- but it is useful to remember and use
because it makes things clear for us when we have several lines of
algebra simultaneously. Next, we need to complete the new notation for
the inner product.</p>
<h3 id="bra-space-and-inner-products">Bra space and Inner Products</h3>
<p>One operation we cannot perform with a ket alone is the all-crucial
inner product. To define this, we first define a new vector space. This
vector space is the bra space, and is made by a simple transformation:
for every ket <span class="math inline">\(\ket{X}\)</span>, the
corresponding bra is written <span
class="math inline">\(\bra{X}\)</span> and is defined by <span
class="math display">\[\bra{X}=\ket{X}^{\dagger}.\]</span> The notation
means the <strong>Hermitian adjoint</strong> of the ket <span
class="math inline">\(\ket{X}\)</span>. This is the formal way to refer
to the transpose conjugate of any matrix (see the matrix preliminary) –
that is, complex conjugate all the matrix element values and then
transpose the matrix. For the column vector kets, this therefore turns
them to row vectors with the complex conjugate entries: <span
class="math display">\[\ket{X}=\begin{bmatrix}
c_{1}\\
c_{2}\\
\vdots\\
c_{n}\\
\end{bmatrix}\Rightarrow\stab\bra{X}=\begin{bmatrix}
c^{\ast}_{1}\stab c^{\ast}_{2} \dots c^{\ast}_{n}.
\end{bmatrix}\]</span> One sees that the relationship exhibited between
the two vector spaces of the ket space of kets and the bra space of bras
is bijection. This is because every ket is unique as a column vector–
due to its unique expansion in any basis set– and therefore taking the
Hermitian adjoint will give us one single unique bra. For every element
of the ket space there is therefore a single element in the bra space,
and vice versa. This exists precisely because of the way we have defined
the bra space.<br />
<br />
Why define such a bra space? Bras never represent physical states
directly- even though they are in bijection with the state vectors
(kets) which are in bijection with physical states so each bra is in
bijection with physical states. The answer is simple– because we need
the inner product to be a real scalar given two input states: as we have
already seen for the last few chapters. We cannot simply multiply two
kets because we cannot multiply two column vectors by the rules of
matrix multiplication! Therefore, we define the bra, which is a row
vector with the complex conjugate entries, and now if we multiply them:
<span class="math display">\[\bra{X} \times \ket{X}\]</span> we should
get a <span class="math inline">\(1\times n\)</span> row vector
multiplying an <span class="math inline">\(n\times 1\)</span> column
vector, and therefore a <span class="math inline">\(1\times 1\)</span>
result: eg, a scalar, as desired.<br />
<br />
We should be convinced that this new inner product is exactly the same
as the old inner product. Take any two state vectors <span
class="math display">\[\forall\stab\ket{\beta}:=\sum_{i=1}^{n}b_{i}\ket{\alpha_{i}},
\:\: \ket{\gamma}:=\sum_{i=1}^{n}c_{i}\ket{\alpha_{i}},\]</span> By the
rules of matrix multiplication, the <span
class="math inline">\(i\)</span>’th column component of the row vector
multiplies the <span class="math inline">\(i\)</span>’th row component
of the column vector. Thus we have <span
class="math display">\[\bra{\beta}\times\ket{\gamma}:=\ip{\beta}{\gamma}=\sum_{i=1}^{n}b^{\ast}_{i}c_{i}.\]</span>
as the components of the ket column vector, here <span
class="math inline">\(\ket{\beta}\)</span> are replaced by their complex
conjugates in the bra row vector. There is absolutely no difference to
the inner product operation: only, we consider it from a new perspective
due to the matrix representations we have introduced.<br />
<br />
Several properties we have established previously can now be written in
the new notation:</p>
<ul>
<li><p><span
class="math inline">\(\ip{X}{Y}=\ip{Y}{X}^{\ast}\)</span></p></li>
<li><p><span class="math inline">\(\ip{X}{X}\geq0\)</span></p></li>
<li><p>A normalised ket <span
class="math inline">\(\ket{\tilde{\alpha}}\)</span> is such that <span
class="math inline">\(\ip{\tilde{\alpha}}{{\tilde{\alpha}}}=1\)</span>.</p></li>
<li><p>Two kets <span class="math inline">\(\ket{\alpha}\)</span> and
<span class="math inline">\(\ket{\beta}\)</span> are orthogonal if <span
class="math inline">\(\ip{\alpha}{\beta}=0\)</span>.</p></li>
</ul>
<p>To further the bijection between the ket space and bra space, as bras
will prove more useful than only to appear in inner products, we have,
using the conventional <span
class="math inline">\(\leftrightarrow\)</span> symbol to mean : <span
class="math display">\[\begin{aligned}
\ket{X}&amp;\duac\bra{X}\\
c\ket{X}&amp;\duac c^{\ast}\bra{X}\\
\ket{X}+\ket{Y}&amp;\duac \bra{X}+\bra{Y}.\\
\end{aligned}\]</span> The way we obtain this is an important algebraic
point: by performing the operation on any linear combination of kets, we
immediately create a new set of bras in bijection with linear
combinations of kets of that type, in much of a similar argument to the
one we used originally to show the bra space is in bijection to the ket
space.<br />
<br />
The expansion of bras in their basis should be clear following the above
facts. For <span
class="math display">\[\ket{X}:=\sum_{i=1}^{n}c_{i}\ket{\alpha_{i}}\]</span>
we have <span
class="math display">\[\bra{X}=\sum_{i=1}^{n}c_{i}^{\ast}\bra{\alpha_{i}}.\]</span>
Thus completes our investigation of the bra space, and now we can
continue to reformulate old ideas in this new notation to familiarise
ourselves with it.<br />
<br />
Further with inner label conventions: in general, for scalars we write
<span class="math inline">\(\ket{aX}\)</span> to mean <span
class="math inline">\(a\ket{X}\)</span>, and <span
class="math inline">\(\bra{aX}\)</span> to mean <span
class="math inline">\(a^{\ast}\bra{X}\)</span>. It is up to the reader
to distinguish which are the scalars and which the placeholder letters
for kets and bras in question, though the context should never make this
in any real doubt if one follows the steps through. Similarly, we want
to ensure brevity with this notation as one (but not the only) of the
benefits of employing bra ket notation, so we also often write <span
class="math inline">\(\ket{aX+bY}\)</span> instead of <span
class="math inline">\(\ket{aX}+\ket{bY}\)</span> or <span
class="math inline">\(a\ket{X}+b\ket{Y}\)</span>. Using the ket <span
class="math inline">\(\ket{aX+bY}:=\ket{aX}+\ket{bY}\)</span> is
somewhat easier than for example defining <span
class="math display">\[\ket{Z}:=\ket{aX}+\ket{bY}\]</span> every time we
sum two kets, where a new letter in the inner label would just be
confusing.<br />
<br />
The inner product short-form facts are always helpful; indeed, we have
<em>linearity in ket</em>: <span
class="math display">\[\ip{V}{aX+bY}\equiv\bra{V}\times(\ket{aX}+\ket{bY})\equiv\ip{V}{aX}+\ip{V}{bY}\equiv
a\ip{V}{X}+b\ip{V}{Y}.\]</span> This fact is one we have seen already
(fact S7 in Chapter 3) for inner products, but expressed in Dirac
notation. The comparable idea exists in bra: <span
class="math display">\[\ip{aX+bY}{V}=(\bra{aX}+\bra{bY})\times\ket{X}=\ip{aX}{V}+\ip{bY}{V}=a^{\ast}\ip{X}{V}+b^{\ast}\ip{Y}{V}.\]</span><br />
Next, we can also recall the ubiquitous expansion. Take an arbitrary ket
<span class="math inline">\(\ket{X}\)</span> in the orthonormal basis
<span class="math inline">\(\setof{\ket{\tilde{\alpha}_{i}}}\)</span>
and the following will always hold: <span
class="math display">\[\begin{aligned}
\ket{X}&amp;:=\sum_{i=1}^{n}c_{i}\ket{\tilde{\alpha}_{i}} \implies
\forall j, \stab
\ip{\tilde{\alpha}_{j}}{X}=\bra{\tilde{\alpha}_{j}}\times\sum_{i=1}^{n}c_{i}\ket{\tilde{\alpha}_{i}}.
&amp;
\end{aligned}\]</span> By linearity in ket, the outside bra is absorbed
into the sum notation so we get <span
class="math display">\[\ip{\tilde{\alpha}_{j}}{X}=\sum_{i=1}^{n}c_{i}\ip{\tilde{\alpha}_{j}}{\tilde{\alpha}_{i}}=\sum_{i=1}^{n}c_{i}\delta_{ij}\]</span>
by the orthonormality of the basis. Then, this is simply <span
class="math display">\[\ip{\tilde{\alpha}_{j}}{X}=c_{j},\]</span> which
should certainly be familiar now, though perhaps not before in Dirac
notation. It tells us that, given a basis, if we orthonormalise it (with
the Gram-Schmidt process), the expansion coefficients of any ket are not
random: they are the inner products of the ket being expanded with the
corresponding basis kets. So for any ket <span
class="math inline">\(\ket{X}\)</span>, <span
class="math display">\[\ket{X}=\sum_{i=1}^{n}\ket{\tilde{\alpha}_{i}}\ip{\tilde{\alpha}_{i}}{X}.\]</span>
It is important to represent it explicitly here because the above will
be used universally for problems to follow, and without explicitly
stating that this is the expansion we already know it could otherwise be
confusing. The reader should find it simple to produce a bra expansion
in an orthonormal bra basis, though this is seldom as useful or commonly
used.</p>
<h3 id="operators-and-eigenkets">Operators and Eigenkets</h3>
<p>The action of an operator <span class="math inline">\(\Omega\)</span>
on a ket <span class="math inline">\(\ket{X}\)</span> is written <span
class="math inline">\(\Omega\ket{X}\)</span>. This is perhaps why we
incorporate scalars into the ket inner label, as noted above– so that
there is no mixup between scalar multiplication and the action of an
operator on a ket. The action of an operator on a bra is conversely
written <span class="math inline">\(\bra{X}\Omega\)</span>.<br />
<br />
We always focus on linear operators which map one ket onto another ket
in the vector space. This makes sense: the application of observable
operators for example we do not expect to make an impossible state, and
any possible state is represented by some superposition of the state
vectors which constitute the ket space. It is good to keep in mind very
explicitly the notion that an operator acting on a ket always produces a
new ket. More properties of linear operators can be expressed in bra-ket
notation:</p>
<ul>
<li><p><span class="math inline">\(a\Omega\ket{X}=\Omega
a\ket{X}\)</span></p></li>
<li><p><span class="math inline">\(\bra{X}a\Omega=\bra{X}\Omega
a\)</span></p></li>
<li><p><span class="math inline">\(\Omega_{1}\ket{X}=\Omega_{2}\ket{X}
\forall\ket{X}\implies\Omega_{1}=\Omega_{2}\)</span></p></li>
<li><p><span
class="math inline">\(\Omega\{a\ket{X}+b\ket{Y}\}=a\Omega\ket{X}+b\Omega\ket{Y}\)</span></p></li>
<li><p><span
class="math inline">\(\Omega\{a\bra{X}+b\bra{Y}\}=a\bra{X}\Omega+b\bra{Y}\Omega\)</span></p></li>
</ul>
<p>The product of two operators means to apply the operator to the ket
or bra respectively and then apply the second, operator to the resulting
ket or bra. We already know from our study of compatibility in
particular that the assumption that two given operators will commute is
false. The commutator is denoted the same way: <span
class="math display">\[[\Omega,
\Lambda]:=\Omega\Lambda-\Lambda\Omega\]</span> and so is the
anticommutator: <span class="math display">\[\{\Omega,
\Lambda\}:=\Omega\Lambda+\Lambda\Omega.\]</span> The eigenvectors of the
operators will now often be called eigenkets, for obvious reasons, and
the name eigenbras follows for bras in bijection with eigenkets.<br />
<br />
We must now introduce the unique way to specify an operator as a matrix.
This will allow us to perform some important operations in the
future.<br />
<br />
We start by noting that the action of an operator on the basis vectors
of a ket space (any basis, not just its own eigenbasis!) is sufficient
knowledge to specify its actions on all kets in that basis: <span
class="math display">\[\Omega\ket{\alpha_{i}}=\ket{\alpha&#39;_{i}}\imp\:\:
\forall \stab\ket{X}:=\sum_{i=1}^{n}c_{i}\ket{\alpha_{i}}, \mtab
\Omega\ket{X}=\Omega\sum_{i=1}^{n}c_{i}\ket{\alpha_{i}}\]</span> and by
the linearity of the operator, this is just <span
class="math display">\[\Omega\ket{X}=\sum_{i=1}^{n}\Omega
c_{i}\ket{\alpha_{i}}=\sum_{i=1}^{n}
c_{i}\Omega\ket{\alpha_{i}}=\sum_{i=1}^{n}c_{i}\ket{\alpha&#39;_{i}}.\]</span>
However, things are not over from the perspective of a matrix
formulation. Not only does this specification not give any clue as to
how to express the operator as a matrix, but the resulting ket after the
transformation is made is also not in any way representable as a column
vector. This is because we must remember that a linear transformation on
the basis vectors of a space by no means produces another basis which
still spans the space at all (in which case a vector of components makes
no sense anymore). The best example of this, of course, would be if
<span class="math inline">\(\Omega=\Omega_{0}\)</span>, the null
operator, in which case all <span
class="math inline">\(\ket{\alpha&#39;_{i}}\)</span> would be null kets
and certainly span no space at all.<br />
<br />
So the above informs us that to complete the task we need to return all
the above to the original basis, which we know is serviceable for matrix
representations. We thus start by expressing the components of the
transformed kets <span
class="math inline">\(\setof{\ket{\alpha&#39;_{i}}}\)</span> in the
original basis. If <span
class="math display">\[\ket{\alpha&#39;_{j}}:=\sum_{i=1}^{n}c^{(j)}_{i}\ket{\alpha_{i}}\]</span>
and we assume the starting basis <span
class="math inline">\(\setof{\ket{\alpha_{i}}}\)</span> is orthonormal,
since otherwise it could be orthonormalised, then we clearly know the
components <span class="math inline">\(c_{i}^{(j)}\)</span> are simply
the inner products <span
class="math inline">\(\ip{\alpha_{i}}{\alpha&#39;_{j}}\)</span>. This
component is the component of the <span
class="math inline">\(j\)</span>’th transformed ket corresponding to the
<span class="math inline">\(i\)</span>’th basis ket. We can then define
the entities <span
class="math display">\[\Omega_{ij}:=\ip{\alpha_{i}}{\alpha&#39;_{j}}=\bra{\alpha_{i}}{\Omega}\ket{\alpha_{j}}\]</span>
and these are the components of the transformed kets in the original
basis before they were transformed. The original question can be
reposed. If we define <span
class="math display">\[\Omega\ket{X}:=\ket{x_{0}}:=\sum_{i=1}^{n}c&#39;_{i}\ket{\alpha_{i}}\]</span>
for some components <span class="math inline">\(c&#39;_{i}\)</span>,
then these components are <span
class="math display">\[c&#39;_{i}=\ip{\alpha_{i}}{x_{0}}=\bra{\alpha_{i}}\Omega\ket{X}=\bra{\alpha_{i}}\Omega\biggl(\sum_{i=1}^{n}c_{i}\ket{\alpha_{i}}\biggr).\]</span>
By the linearity of the operator, this becomes <span
class="math display">\[c&#39;_{i}=\bra{\alpha_{i}}\times\biggl(\sum_{j=1}^{n}c_{j}\Omega\ket{\alpha_{j}}\biggr),\]</span>
and then by linearity in ket this becomes <span
class="math display">\[c&#39;_{i}=\sum_{j=1}^{n}c_{j}\bra{\alpha_{i}}{\Omega}\ket{\alpha_{j}}.\]</span>
Thus using the same notations defined above this is simply <span
class="math display">\[c&#39;_{i}=\sum_{i=1}^{n}c_{i}\Omega_{ij}.\]</span>
The notation <span class="math inline">\(\Omega_{ij}\)</span> is clearly
meant to hint that these can be placed in some matrix where each value
<span class="math inline">\(\Omega_{ij}\)</span> (note these are inner
products, so they are indeed a scalar values) is the entry in the <span
class="math inline">\(i\)</span>’th row and <span
class="math inline">\(j\)</span>’th column of the matrix. And as each
<span class="math inline">\(\Omega_{ij}\)</span> is the component of the
<span class="math inline">\(j\)</span>’th transformed ket corresponding
to the <span class="math inline">\(i\)</span>’th basis ket, we say that
the upper limit of <span class="math inline">\(i\)</span> and <span
class="math inline">\(j\)</span> are both <span
class="math inline">\(n\)</span> since there are <span
class="math inline">\(n\)</span> original basis kets, and therefore
<span class="math inline">\(n\)</span> transformed kets as well. Thus we
can create an <span class="math inline">\(n\times n\)</span> matrix for
all the entries <span class="math inline">\(\Omega_{ij}\)</span>. The
relationship we get is that <span class="math display">\[\begin{bmatrix}
c&#39;_{1} \\
c&#39;_{2} \\
\vdots \\
c&#39;_{n} \\
\end{bmatrix}
=
\begin{bmatrix}
\langle{\alpha_{1}}|\Omega|{\alpha}_{1}\rangle &amp;
\langle{\alpha_{1}}|\Omega|{\alpha}_{2}\rangle &amp;
\dots &amp; \langle{\alpha_{1}}|\Omega|{\alpha}_{n}\rangle\\
\langle{\alpha_{2}}|\Omega|{\alpha}_{1}\rangle &amp; \ddots &amp;
\:\dots\: &amp; \vdots \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
\langle{\alpha_{n}}|\Omega|{\alpha}_{1}\rangle &amp; \dots &amp; \dots
&amp; \langle{\alpha_{n}}|\Omega|{\alpha}_{n}\rangle
\end{bmatrix}
\begin{bmatrix}
c_{1} \\
c_{2} \\
\vdots \\
c_{n} \\
\end{bmatrix}\]</span> as a way to relate the components of the
transformed vector <span
class="math inline">\(\Omega\ket{X}:=\ket{x_{0}}\)</span> to the
original components of <span class="math inline">\(\ket{X}\)</span>
before it was transformed. We see that the left hand side is the matrix
representation of <span class="math inline">\(\ket{x_{0}}\)</span> in
the basis we have been using, since it specifies the components of <span
class="math inline">\(\ket{x_{0}}\)</span> as a column vector.
Meanwhile, the right column vector clearly does the same for the
original <span class="math inline">\(\ket{X}\)</span>. And therefore
this whole matrix equation is clearly the matrix form of the definition
<span class="math inline">\(\ket{x_{0}}=\Omega\ket{X}\)</span>, which
then means that the <span class="math inline">\(n\times n\)</span>
matrix in the middle is the matrix representation of <span
class="math inline">\(\Omega\)</span>. So to conclude this discussion we
restate the fact that <span class="math display">\[\begin{bmatrix}
\langle{\alpha_{1}}|\Omega|{\alpha}_{1}\rangle &amp;
\langle{\alpha_{1}}|\Omega|{\alpha}_{2}\rangle &amp;
\dots &amp; \langle{\alpha_{1}}|\Omega|{\alpha}_{n}\rangle\\
\langle{\alpha_{2}}|\Omega|{\alpha}_{1}\rangle &amp; \ddots &amp;
\:\dots\: &amp; \vdots \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
\langle{\alpha_{n}}|\Omega|{\alpha}_{1}\rangle &amp; \dots &amp; \dots
&amp; \langle{\alpha_{n}}|\Omega|{\alpha}_{n}\rangle
\end{bmatrix}\]</span> is the way to represent the operator <span
class="math inline">\(\Omega\)</span> in the <span
class="math inline">\(n\)</span>-dimensional ket space spanned by the
orthonormal basis vectors <span
class="math inline">\(\setof{\ket{\alpha_{i}}}\)</span>.</p>
<h2 id="a-rudimentary-manipulation-toolbox">A Rudimentary Manipulation
Toolbox</h2>
<p>The introduction of Dirac notation is surprisingly powerful because
its unorthodox aesthetic form allows for the formulation of some
identities and operators which under normal circumstances would look
like a long amalgamation of random algebraic entities, but look simple
and orderly (and therefore easy to memorise and use) in Dirac notation.
This section will introduce essential identities and axioms which will
be used without second thought for advanced problem solving, and in
doing so, I hope the reader can also continue the process of becoming
fluent in Bra Ket.</p>
<h3 id="general-solution-of-the-eigenvalue-problem">General Solution of
the Eigenvalue Problem</h3>
<p>The first tool we will see is a general solution of the eigenvalue
problem, for which we have been waiting. The eigenvalue condition is
always <span class="math display">\[\Omega\ket{\omega} =
\lambda\ket{\omega}.\]</span> for some eigenvalue <span
class="math inline">\(\lambda\)</span> and eigenvector <span
class="math inline">\(\ket{\omega}\)</span> We know that all operators
in the space can be represented by an <span
class="math inline">\(n\times n\)</span> matrix (where <span
class="math inline">\(n\)</span> is the dimensionality of the space).
Just to convert the eigenvalue <span
class="math inline">\(\lambda\)</span> into matrix form as well, we will
multiply both sides by the <span class="math inline">\(n\times
n\)</span> identity operator <span class="math inline">\(I\)</span>:
<span class="math display">\[\begin{aligned}
\Omega\ket{\omega} &amp;= \lambda I\ket{\omega}\\
\Rightarrow\:\: (\Omega-\lambda I)\ket{\omega} &amp;= 0
\end{aligned}\]</span> where 0 is the null matrix. Considering the whole
in matrix form, we write: <span class="math display">\[\begin{bmatrix}
\Omega_{1,1}-\lambda &amp; \Omega_{1,2} &amp; \dots  &amp; \Omega_{1,n}
\\
\vdots  &amp;   \Omega_{2,2}-\lambda &amp; \vdots &amp; \vdots \\
\vdots &amp;  \dots &amp; \ddots &amp; \vdots \\
\Omega_{n,1} &amp; \dots &amp; \dots &amp; \Omega_{n,n}-\lambda\\
\end{bmatrix}
\begin{bmatrix}
c_{1} \\
c_{2} \\
\vdots \\
c_{n} \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
\vdots \\
0 \\
\end{bmatrix}.\]</span> Looking at the matrix representation of the
equation above, we can deduce that <span class="math display">\[\forall
i, \:\: \sum_{j}c_{j}(\Omega_{ij}-\lambda\delta_{ij})=0\]</span> by the
rules of matrix multiplication. This is a linear system of equations,
with coefficients <span
class="math inline">\(\Omega_{ij}-\lambda\delta_{ij}\)</span> and
unknowns <span class="math inline">\(c_{i}\)</span>. Therefore the logic
behind Cramer’s Rules apply: and since the <span
class="math inline">\(c_{i}\)</span> are the components of <span
class="math inline">\(\ket{\omega}\)</span> in the basis they are not
all zero since <span class="math inline">\(\ket{\omega} \neq 0\)</span>,
and thus we need nontrivial solutions and therefore the determinant of
the leftmost term must be zero. In other words, <span
class="math display">\[\det(\Omega-\lambda I) = 0 \iff
\forall i, \:\: \sum_{j}c_{j}(\Omega_{ij}-\lambda\delta_{ij})=0\]</span>
This is practically helpful, especially in problems with fewer
dimensional spaces, and as a theoretical concept which shows us we can
always solve the eigenvalue problem. There will be other methods to
solve eigenvalue equations too, but these often rely somewhat more on
inspection and then proof that one has all the solutions by inspection;
these inspection methods only arise as shortcuts based on specific
conditions we get.</p>
<h3 id="the-associative-axiom">The Associative Axiom</h3>
<p>Dirac introduced in his paper on Dirac notation a critical axiom
called the Associative Axiom. This axiom will save a huge amount of
confusion for those unfamiliar with Bra-Ket notation, and is possibly
the most powerful idea of them all. This axiom states that all (legal)
operations in Dirac notation are always associative.<br />
<br />
Put in scalar terms, this word associative is usually seen as <span
class="math display">\[(a\times b)\times c = a \times (b\times
c).\]</span> In other words, so long as the order of the terms <span
class="math inline">\(a,b,c\)</span> are kept (here, <span
class="math inline">\(a\)</span> before <span
class="math inline">\(b\)</span> before <span
class="math inline">\(c\)</span>), the two multiplication operations can
be performed in either order. With numbers this is completely natural
(as also is additive associativity, but not subtractive or divisive
associativity), but with Dirac notation this is much less trivial and
also much more powerful.<br />
<br />
The first example we have seen but not explained explicitly is the inner
product <span class="math display">\[\bra{X}{\Omega}\ket{Y}.\]</span>
This is an inner product since it is the product of a bra, <span
class="math inline">\(\bra{X}\)</span> with a ket <span
class="math inline">\(\Omega\ket{Y}\)</span> (which is another ket in
the ket space the operator <span class="math inline">\(\Omega\)</span>
has mapped the original ket <span class="math inline">\(\ket{Y}\)</span>
to). However, it can also be seen as a bra, <span
class="math inline">\(\bra{X}\Omega\)</span>, multiplied by a ket <span
class="math inline">\(\ket{Y}\)</span>. This is the associative axiom:
in fact it does not matter which way the multiplication goes and only
because of this can we write the concise <span
class="math inline">\(\bra{X}{\Omega}\ket{Y}\)</span>.<br />
<br />
The ramifications of this fact are profound, as beginners with Dirac
notation often get confused about which products are and must be
performed first, leading to a great state of bewilderment when a term
might have several bras, operators and kets next to each other. The
answer is very simply that, so long as operations are being performed
left to right, the order does not matter and the reader may read the
multiplications in any way they like. It also immediately leads to
theorems for entities in Dirac notation, by defining them through the
perspective of other known products. The first example of this we will
see is the following completeness relation.<br />
<br />
<strong><u>The Outer Product</u></strong><br />
<br />
We discussed above how the associative axiom holds for all legal
multiplications between bras, kets and operators. What constitutes an
illegal multiplication? Well, to start we can give the example of the
orthonormal expansion of any ket <span
class="math inline">\(\ket{X}\)</span>, <span
class="math display">\[\ket{X}=\sum_{i=1}^{n}\ket{\talpha_{i}}\ip{\talpha_{i}}{X}.\]</span>
Contrast this to the naive expansion <span
class="math display">\[\ket{X}=\sum_{i=1}^{n}c_{i}\ket{\talpha_{i}}\]</span>
which we might use if we were not sure the basis was orthonormal, for
example. We see that the position of the component shifted from the
leftmost sum term to the rightmost sum term when <span
class="math inline">\(c_{i}\)</span> was replaced with <span
class="math inline">\(\ip{\talpha_{i}}{X}\)</span>. One might ask why
this is. The best answer for this is simply the associative axiom: if we
had written <span
class="math display">\[\ket{X}=\sum_{i=1}^{n}\ip{\talpha_{i}}{X}\ket{\talpha_{i}},\]</span>
then we might think this is okay as we have a scalar <span
class="math inline">\(\ip{\talpha_{i}}{X}\)</span> multiplying a ket
<span class="math inline">\(\ket{\talpha_{i}}\)</span>. However, by the
associative axiom we also expect the above to be equally well expressed
as <span class="math display">\[\ket{X}=\sum_{i=1}^{n}
\bra{\talpha_{i}}\times\biggl(\ket{X}\times\ket{\talpha_{i}}\biggr).\]</span>
This is where our problem is– two kets cannot be multiplied together, as
they are both <span class="math inline">\(n\times 1\)</span> matrices!
Similarly, two bras cannot be multiplied together as they are both <span
class="math inline">\(1\times n\)</span> matrices. So this is why we
cannot write <span
class="math display">\[\ket{X}=\sum_{i=1}^{n}\ip{\talpha_{i}}{X}\ket{\talpha_{i}}.\]</span>
This however implies that for the expansion <span
class="math display">\[\ket{X}=\sum_{i=1}^{n}\ket{\talpha_{i}}\ip{\talpha_{i}}{X}\]</span>
it is correct and therefore not only is the operation <span
class="math inline">\(\ket{\talpha_{i}}\times\ip{\talpha_{i}}{X}\)</span>
legal (it is a scalar multiplying a ket, so it should be)- but also that
the operation <span
class="math display">\[\left(\op{\talpha_{i}}{\talpha_{i}}\right)\times
\ket{X}\]</span> should be possible. This product <span
class="math inline">\(\op{\talpha_{i}}{\talpha_{i}}\)</span> is called
the outer product between the bra <span
class="math inline">\(\bra{\talpha_{i}}\)</span> and ket <span
class="math inline">\(\ket{\talpha_{i}}\)</span>. We can verify it
should be possible, as it is an <span class="math inline">\(n\times
1\)</span> matrix multiplied by a <span class="math inline">\(1\times
n\)</span> matrix, which should give an <span
class="math inline">\(n\times n\)</span> matrix. As it produces an <span
class="math inline">\(n\times n\)</span> matrix it clearly does not
result in a scalar like the inner product; rather, we might posit that
it is an operator, as operators are represented by <span
class="math inline">\(n\times n\)</span> matrices. This is in fact a
true assumption: an outer product is fundamentally meant to be treated
as an operator. However, before we discuss that we should establish and
prove a fundamental theorem in quantum mechanics which will be
surprisingly useful for manipulation. This is the completeness relation,
which results from the associative axiom, and deserves its own section
for its importance even though the proof requires no considerable
thought if we have the associative axiom in mind.<br />
<br />
<strong><u>The Completeness Relation</u></strong><br />
<br />
We have seen that the representation <span
class="math display">\[\ket{X}=\northexp{X}{\talpha_{i}}\]</span> is
allowed because the sum term <span
class="math inline">\(\sop{\talpha_{i}}\times \ket{X}\)</span> is just
the action of the outer product on the ket <span
class="math inline">\({X}\)</span>. However, by the rule of linear
operators that <span
class="math display">\[(\Omega_{1}+\Omega_{2})\ket{X}=\Omega_{1}\ket{X}+\Omega_{2}\ket{X}\]</span>
and the fact that the outer products are operators, we can use the same
principle to write <span
class="math display">\[\ket{X}=\biggl(\sum_{i=1}^{n}\sop{\talpha_{i}}\biggr)\ket{X}.\]</span>
And this is very revealing, of course, because this is true for any
arbitrary ket <span class="math inline">\(\ket{X}\)</span>, and
therefore we come to the conclusion that <span
class="math display">\[\sum_{i=1}^{n}\sop{\talpha_{i}}=1.\]</span> This
is the most commonly written form of the completeness relation, but one
should be aware that the 1 here represents the identity operator, rather
than a scalar– since the sum of <span class="math inline">\(n\times
n\)</span> matrices will give us an <span class="math inline">\(n\times
n\)</span> matrix rather than a scalar.<br />
<br />
On the other hand, the more we foray into problem solving the more we
will see of this seemingly completely useless relation. One example can
be in the proof that the sum of the modulus squared components of a
normalised ket is also equal to <span class="math inline">\(1\)</span>:
<span class="math display">\[\sip{X}=\bra{X}\times\ket{X}=\bra{X}\times
(1\ket{X})=\bra{X}\times
\sum_{i=1}^{n}\sop{\talpha_{i}}\times\ket{X}\]</span> where the number
<span class="math inline">\(1\)</span> is the identity operator again
and the kets <span class="math inline">\(\setof{\talpha_{i}}\)</span>
are the orthonormal basis vectors. Then, by linearity in ket, we can
write this as <span
class="math display">\[\sip{X}=\sum_{i=1}^{n}\bra{X}\times\sop{\talpha_{i}}\times\ket{X}\]</span>
and by the associative axiom this is just <span
class="math display">\[\sip{X}=\sum_{i=1}^{n}\ip{X}{\talpha_{i}}\ip{\talpha_{i}}{X}=\sum_{i=1}^{n}\ip{\talpha_{i}}{X}\times(\ip{\talpha_{i}}{X})^{\ast}=\sum_{i=1}^{n}|\ip{\talpha_{i}}{X}|^{2}.\]</span>
However, as the inner products <span
class="math inline">\(\ip{\talpha_{i}}{X}\)</span> are the components of
<span class="math inline">\(\ket{X}\)</span> in the basis <span
class="math inline">\(\setof{\talpha_{i}}\)</span>, if the ket <span
class="math inline">\(X\)</span> is normalised then the left side is
<span class="math inline">\(1\)</span> so we get <span
class="math display">\[1=\sum_{i=1}^{n}|c_{i}|^2.\]</span> So the
modulus squared of the components of a normalised ket in a basis sum to
<span class="math inline">\(1\)</span>. We know these modulus squared
components are probabilities if we are working in an observable space by
the measurement postulate! So what we have just proved, in a flick of
the pen when working with Dirac notation, is that the reason we like
working with discrete wavefunctions is because their components in this
way perfectly represent probabilities summing in total to <span
class="math inline">\(1\)</span>! We have already proved this fact, at
the end of Chapter 4 on normalised discrete wavefunctions being
interpreted as probability mass functions, but, if one takes a quite
glance at that, the proof is neither so short, nor so elegant: it is
positively clunky in comparision. The purpose of showing this is to
exhibit that, just like in normal algebra, the world of Dirac notation
can belie some extremely unusual and ingenious manipulations which
without the notation would not be so easy to express, but with the
fluency in the notations, can be done in just a few lines or seen in a
matter of seconds.</p>
<h2 id="change-of-basis">Change of Basis</h2>
<p>One of the most important concepts in matrix mechanics is that of
diagonal matrices. A square matrix is diagonal if all its elements are
<span class="math inline">\(0\)</span> except those in the major
diagonal: that is, elements <span class="math inline">\(M_{ii}\)</span>,
which can be anything. One example of a diagonal matrix is the identity
matrix, which has all zero entries except the major diagonal, which is
filled with <span class="math inline">\(1\)</span>’s.<br />
<br />
<strong>Theorem: Every Hermitian operator has at least one
orthonormal</strong><br />
<strong>eigenbasis in which its matrix representation is diagonal.
The</strong><br />
<strong>diagonal entries are the eigenvalues of the
operator.</strong><br />
<br />
This proof is quite tough and not particularly illustrative for our
current needs, and therefore will not be shown for now. However, we will
find that the implications of this theorem are hugely powerful, because
it means that if we can diagonalise the operator matrix then we
immediately solve the eigenvalue problem as the diagonal matrix values
give us the eigenvalues from which deriving the eigenvectors should be
easy: clearly, a great problem-solving step.<br />
<br />
So we want to diagonalise the operator. By the theorem, this means we
want to take one matrix in any basis to the eigenbasis which
diagonalises the operator. Thus we will often be interested in a
<strong>change of basis</strong>.<br />
<br />
A change of basis is performed by applying an operator to the original
basis kets and mapping them onto the new kets. Such is a common idea in
quantum mechanics: to transform one ket to another we also try to see if
there is a way we can formulate it as an operator equation. Here, we
certainly can; such operators are usually denoted <span
class="math inline">\(U\)</span>. So we guess that for some original
basis <span class="math inline">\(\setof{\ket{\alpha_{i}}}\)</span> and
operator <span class="math inline">\(\Omega\)</span> with eigenvalues
<span class="math inline">\(\setof{\beta_{i}}\)</span> and eigenvectors
<span class="math inline">\(\setof{\ket{\beta_{i}}}\)</span>, <span
class="math display">\[\forall i, \stab
U\ket{\alpha_{i}}=\ket{\beta_{i}}.\]</span> Assuming without loss of
generality that these basis sets are both orthonormal, the operator
which works is <span
class="math display">\[U:=\sum_{j}\op{\beta_{j}}{\alpha_{j}}.\]</span>
We can prove this. For any original basis ket <span
class="math inline">\(\ket{\alpha_{i}}\)</span>, <span
class="math display">\[\biggl(\sum_{j}\op{\beta_{j}}{\alpha_{j}}\biggr)\ket{\alpha_{i}}=\sum_{j}\ket{\beta_{j}}\ip{\alpha_{j}}{\alpha_{i}}=\sum_{j}\ket{\beta_{j}}\delta_{ij}=\ket{\beta_{i}}\]</span>
and this holds for any <span class="math inline">\(i\)</span> since we
have orthonormal sets. What is much more important, however, is that the
operator <span class="math inline">\(U\)</span> satisfies a very
interesting condition. Consider, for the operator defined, the product
<span class="math display">\[U^\dagger
U=\biggl(\sum_{k}\op{\alpha_{k}}{\beta_{k}}\biggr)\times\biggl(\sum_{j}\op{\beta_{j}}{\alpha_{j}}\biggr)\]</span>
As the basis sets are orthonormal, we see that all the terms disappear
on account of the inner product <span
class="math inline">\(\ip{\beta_{k}}{\beta_{j}}\)</span>, except when
<span class="math inline">\(k=j\)</span>. In those cases, we get <span
class="math display">\[\sum_{k}\sop{\alpha_{k}}=1.\]</span> So for this
, we have <span class="math inline">\(U^{\dagger}U=1\)</span>.
Similarly, <span class="math inline">\(U U^{\dagger}=1\)</span>. Such an
operator is called <strong>unitary</strong>. Now consider some arbitrary
operator which is unitary and its action on a state ket which is
normalised. We know that for the ket <span
class="math inline">\(\ket{\Psi
}\)</span></p>
<h2 id="exercises-from-chapter-6ast">Exercises from Chapter 6<span
class="math inline">\(\ast\)</span></h2>
<ol>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>
</body>
</html>
