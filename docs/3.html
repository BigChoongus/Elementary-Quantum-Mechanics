<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>3</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <script>
    window.MathJax = {
      loader: { load: ['[tex]/newcommand', '[tex]/boldsymbol'] },
      tex: {
        packages: { '[+]': ['base', 'ams', 'newcommand', 'boldsymbol'] },
        macros: {
          bm: ["\\boldsymbol{#1}", 1]  // Define \bm{} using \boldsymbol{}
        },
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


      
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
    $$
    \newcommand{\Answer}{\begin{tcolorbox}}
    \newcommand{\Answerend}{\end{tcolorbox}}
    \newcommand{\ket}[1]{|#1\rangle}
    \newcommand{\bra}[1]{\langle#1|}
    \newcommand{\ip}[2]{\langle#1|#2\rangle}
    \newcommand{\bip}[2]{\left\langle#1\middle|#2\right\rangle}
    \newcommand{\qexp}[1]{\langle#1\rangle}
    \newcommand{\apos}[1]{``#1"}
    \newcommand{\sapos}[1]{`#1'}
    \newcommand{\elec}{e^{-}}
    \newcommand{\uspin}{(\uparrow)}
    \newcommand{\dspin}{(\downarrow)}
    \newcommand{\lspin}{(\leftarrow)}
    \newcommand{\rspin}{(\rightarrow)}
    \newcommand{\ulspin}{(\uparrow\leftarrow)}
    \newcommand{\urspin}{(\uparrow\rightarrow)}
    \newcommand{\dlspin}{(\downarrow\leftarrow)}
    \newcommand{\drspin}{(\downarrow\rightarrow)}
    \newcommand{\R}{\mathbb{R}}
    \newcommand{\stab}{\:\:}
    \newcommand{\mtab}{\:\:\:}
    \newcommand{\btab}{\:\:\:}
    \newcommand{\imp}{\Rightarrow}
    \newcommand{\doubimp}{\Leftrightarrow}
    \newcommand{\setof}[1]{\{#1\}}
    \newcommand{\infint}{\int_{-\infty}^{\infty}}
    \newcommand{\trans}[1]{\mathcal{T}(#1)}
    \newcommand{\dd}[2]{\delta(#1-#2)}
    \newcommand{\ipbig}[2]{\langle#1|#2\rangle}
    \newcommand{\talpha}{\tilde{\alpha}}
    \newcommand{\op}[2]{|#1\rangle\langle#2|}
    \newcommand{\sop}[1]{|#1\rangle\langle#1|}
    \newcommand{\prop}[2]{\mathcal{U}(#1,#2)}
    \newcommand{\propdagg}[2]{\mathcal{U}^{\dagger}(#1,#2)}
    \newcommand{\sip}[1]{\langle#1|#1\rangle}
    \newcommand{\optrip}[3]{\langle#1|\hat{#2}|#3\rangle}
    \newcommand{\nhoptrip}[3]{\langle#1|{#2}|#3\rangle}
    \newcommand{\northexp}[2]{\sum_{i=1}^{n}|#2\rangle\langle#2|#1\rangle}
    \newcommand{\orthexp}[4]{\sum_{#3=1}^#4|#2\rangle\langle#2|#1\rangle}
    \newcommand{\schrodeq}{i\hbar\frac{\partial \Psi(x,t)}{\partial t}=\hat{H}\Psi(x,t)}
    \newcommand{\nd}[2]{\frac{d#1}{d #2}}
    \newcommand{\snd}[2]{\frac{d^{2}#1}{d#2^2}}
    \newcommand{\pd}[2]{\frac{\partial#1}{\partial #2}}
    \newcommand{\spd}[2]{\frac{\partial^{2}#1}{\partial #2^2}}
    \newcommand{\duac}{\leftrightarrow}
    \newcommand{\oip}[2]{\left(#1,#2\right)}
    \newcommand{\obip}[2]{\left(#1,#2\right)}
    $$
<h1
id="chapter-3-the-state-vector-and-the-quantum-state-problem">Chapter 3:
The State Vector and the Quantum State Problem</h1>
<p>In the previous Chapter, we discovered two radical results of the
early quantum experiments which completely invalidated the Newtonian
model of classical mechanics. These were:</p>
<ol>
<li><p>The probabilistic state: there exist at the very least some
measurements for which we can never predict the result with certainty
because the very state itself can take different values with different
probabilities.</p></li>
<li><p>The superposition principle: the reason why probabilistic states
exist is because they are in some superposition of different possible
physical states to which there correspond separate probabilities. In
fact, any superposition of physical states should, in theory, create a
new possible physical state.</p></li>
</ol>
<p>A third result we obtained might be that there are collections of
observables for which a state cannot hold simultaneous values, but this
will come as a natural consequence after we set out the quantum
mechanical model, as we will see in Chapter 4. All these new principles
are offensive to our usual intuition, but undoubtedly are present in the
Stern Gerlach experiment, as well is in other famous experiments such as
the famous Double Slit experiment, and need to be accounted for in a
whole new model of physics. With these ideas laid out– ideas pertaining
in particular to the quantum <em>state problem</em>: in this chapter, we
will meet the first, and most fundamental, postulate of quantum
mechanics– which I call the State Postulate– and our study of the model
has begun.</p>
<h2 id="the-first-postulate-necessities">The First Postulate:
Necessities</h2>
<h3 id="complications-of-the-quantum-state">Complications of the Quantum
State</h3>
<p>The classical model of Newtonian mechanics deals so trivially with
the state problem that to those only classically trained the state
problem does not even seem like much of a problem at all– much less one
of the two most important problems in physics. One measures the momentum
of a ball, the mass of the ball, the radius of the ball and angle it is
travelling. This completes the classical state problem– at least from
the point of view of its basic trajectory: we could also measure its
electric charge or angular momentum if we wanted to approach the state
problem from a different angle. The measured momentum, mass, radius,
angle are the characteristics of the state: which we call values of its
observables. We can use equations like <span
class="math inline">\(p=mv\)</span> to find further quantities like the
velocity when we so desire– or, just find a way to measure them as well.
After this, the problem shifts to calculating collisions or terminal
velocities or whatever the situation calls for: and all of these
problems are of course time evolution questions because we are to
predict the future state given events which happen over time (eg, the
ball hits another ball, swings on a string or falls down a ramp). With
quantum phenomena, however, a certain measurement might tell you
everything about that state, if that state happened to have probability
1 of having the given measurement result, but it might also tell you
essentially nothing, such as for some state where the obtained
measurement actually was initially improbable and the state just
happened upon measurement to take an unlikely value out of countless
other possible values. The Newtonian procedure of dealing with the state
problem – which is to assume a state always has some value for each
observable and what only remains is to measure and record it at specific
times of interest– is far too simplistic to account for the phenomena of
the quantum world.<br />
<br />
So the main complication to the state problem, of course, comes with the
notion of inherent probability in a given state. This notion would have
been inconceivable in Newtonian mechanics. Probability could only ever
be introduced, into experiments by human made contrivances (such as
probabilistic machinery) and the current state would remain unchanged
until a probabilistic action changed it into a future state. However,
even in these cases, these probabilistic differences are not actually
pertaining to the inherent state problem, but rather problems of
external interference. As far as the state problem is concerned, having
a probabilistic state destroys the classical approach. For one, due to
the superposition principle, very few states in quantum mechanics can be
said to possess a value for an observable: we can no longer measure a
particle’s momentum and say that is the relevant momentum value of the
state if that value is only one of an infinite collection of possible
momenta we could have originally measured. We saw this in the Stern
Gerlach experiment, where attempting to say a single electron had up
spin failed miserably when we realised successive measurements (with a
<span class="math inline">\(x\)</span> magnetic field in the middle)
could yield down spin measurements as well. The problem of encapsulation
is therefore rendered by the results much more difficult than with only
classical intuition: because we now have the extra complication of
needing to find a concise way to not only store information on far more
possibilities for the same state, but also their respective
probabilities. The question of being able to store information on
infinite (so by definition, impossible to list) possibilities is another
concern in itself, and it should be clear to the reader that the
classical approach will be insufficient.<br />
<br />
Fortunately, contemporary and historic developments and concepts in
mathematics proved very useful when trying to deal with these new
modelling complications. Probability and how to handle it itself was
certainly not unexplored by mathematicians, and the burgeoning field of
vector spaces would prove to be useful in answering the superposition
principle. With an intellectual jump whose magic we cannot
underestimate, Heisenberg, Born and almost simultaneously Dirac realised
that they could incorporate these areas of mathematics into a new
quantum model of reality. Thus were born the postulates of quantum
mechanics, and with them, this seminal new physical model.</p>
<h3 id="one-to-one-correspondences">One-to-One Correspondences</h3>
<p>There was a detail in the previous chapter on Stern Gerlach
experiments, which, though commanding little attention, pertains to a
key concept which will be central to this chapter. That was the idea of
“state denomination”. If we recall, we invented a strange notation–
while futilely attempting to cling onto normal classical intuition– in
our early attempt to track the spins of different particles. There are
many things wrong with the attempted equation: <span
class="math display">\[500\elec=250\uspin + 250\dspin\]</span> –not
least the fact that it is useless in taking into account superpositions
and inherent probability. One asks therefore why it was included? This
was to introduce a much more useful idea: we will need to start
introducing abstract entities to represent <em>states</em> themselves.
In classical mechanics, we do not do this at all, because the state is
treated like a simple background from which to pick attributes rather
than anything worth studying in itself. We do not need to perform
addition (the mathematical representation of a superposition) on
classical states, or calculate inherent probabilities. If we really need
to distinguish between them, we treat them like situations or
mathematical cases, where symbols are given to observable qualities like
velocity <span class="math inline">\(v\)</span>. We might use subscripts
like <span class="math inline">\(v_{a}\)</span> to help us identify
which case we are working with– eg, the velocity of ball <span
class="math inline">\(a\)</span>. We would never mathematically
represent the state itself, however, when we could simply list its
physical properties and put those values into equations.<br />
<br />
On the other hand, because of these new quantum ideas of summing states
together (superposition) and including measurement probabilities in the
state, it will be extremely important to have <em>mathematical entities
representing quantum states themselves</em>.<br />
<br />
We will need to consider how to mathematically operate with states so
that we will be able to perform physical computations. The first step,
as we tackle how to do this, is to learn about vector spaces. Yet before
all that, we will give a mathematical definition for the idea of
‘labelling’ something. It may seem strange to do so, but having these
clarifications will be immensely useful because very quickly from now we
will need to understand this idea to navigate through labels of labels
of labels of objects and being able to track how these things exactly
are mathematically connected to each other is very useful.<br />
<br />
We have already started using the word ‘abstractions’ in this text. When
we speak of an abstraction, we mean something which is not physical– in
other words, a mathematical entity we indirectly use to describe
something physical. This goes beyond labelling with symbols, and can be
extended to the use of real mathematically manipulable objects to
connect to states as well. The way this is done is through one-to-one
correspondences.<br />
<br />
A one-to-one correspondence strictly occurs between two sets. It is
defined to be the relationship between two sets where for every element
in one set, there is one element in the other set, and for every element
in the other set, there is an element in the set. Intuitively, the idea
is not hard to imagine. Each element can be mapped to a single element
in another set, and vice versa, without any elements left out. The
conventional way to prove there is a one-to-one correspondence between
two sets is to introduce a one-to-one mapping – also known as a
bijection – whose existence can be proved if we can prove that for every
element in the first set there is a corresponding element in the second,
and for every element in the second set there is an element in the
first. Take for example <span
class="math display">\[A=\{2,3,5,7\}\]</span> to be the single digit
primes. We could set up a one to one mapping with the set <span
class="math display">\[B=\{9,27,243,2187\}\]</span> by using the rule
“take 3 to the power of the element” to map an element of <span
class="math inline">\(A\)</span> to an element of <span
class="math inline">\(B\)</span> and the rule “take the element log base
3” to map an element of <span class="math inline">\(B\)</span> to an
element of <span class="math inline">\(A\)</span>. Then there is a
one-to-one correspondence between sets <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> since there are two maps, one <span
class="math inline">\(A\)</span> to <span
class="math inline">\(B\)</span> and one <span
class="math inline">\(B\)</span> to <span
class="math inline">\(A\)</span> which set up a pairing between elements
in <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> such that each pair is unique and no
element in either set in left out.<br />
<br />
Now we can talk about the uses of one-to-one correspondences, which will
be a common point in this book, as quantum mechanics enjoys very many
one-to-one correspondences. The first use we will see is in label sets.
The idea is simple. If there is a one-to-one correspondence between two
sets, we can use elements in one set to label elements in the other set.
We know that every label will have something it represents (since each
element in the set of ‘labels’ is mapped to another element in the other
set); no label will represent more than one element and cause confusion
(since it is a one-to-one mapping); no element will be unlabelled (since
for every element in the other set there is a corresponding element in
the set of ‘labels’); no element will be labelled by two different
labels from the same label set and cause confusion (since it is a
one-to-one mapping). Another even more important use we will see is as
substitutes: if a one-to-one correspondence exists between two sets, we
can substitute the elements of one set for the elements of the other
set, since again the correspondence makes it very easy to identify which
substitutes are connected to which originals we are trying to
describe.<br />
<br />
With the two example sets of single digit primes and the results of
raising them to the power of 3 above, it makes absolutely no sense to
use sets <span class="math inline">\(B\)</span> or <span
class="math inline">\(A\)</span> as either labels or substitutes for
each other, since they are both sets of numbers, and we do not need to
label numbers with other different numbers, and it is equally difficult
to see why we might want to substitute numbers for other numbers in any
scenario. However, there will be uses later on for one-to-one
correspondences for both labelling and substituting for different
objects in quantum mechanics. Specifically, one important one-to-one
correspondence will be between functions and states. This is because, if
we define a certain set of functions properly and have a one-to-one
correspondence with physical states, we will be able to extract
information and values about that state with certain operations on that
function representing it. Clearly, we cannot perform mathematical
operations on physical states directly, which are not mathematical
objects at all. Nor do we want to describe states in words all the time,
as this would be beyond verbose and time-consuming. Thus these
mathematical objects both labelling and substituting for physical states
is a complete necessity in our new world away from the classical method
of ignoring the state problem, and towards our new, quantum realisation
that states are very much important and deep objects in
themselves.<br />
<br />
Thus we understand the goal of this chapter, and the first postulate of
quantum mechanics. We cannot perform mathematical operations on a state,
but we will need to find some way to achieve this in order to navigate
the complex superpositions and probabilities which never existed in the
classical model. Thus we want to set up some mathematical entities which
can be operated on and which are in one-to-one correspondence with
physical states! After that, we can perform mathematical operations on
those entities, and interpret the values which result as information
about the states they represent. The importance of the substitution and
labelling functions of one-to-one correspondences belie nearly all of
the quantum mechanical postulates, so having read this section, the
reader should now be prepared to handle these ideas without any more
confusion. So now, we are ready go through this process of searching for
mathematical objects to represent states by one-to-one correspondences
in the following section, on vector spaces. One final note before we do
so: one-to-one correspondences will in the future be referred to by
their mathematical name: <strong>bijections</strong>.</p>
<h2 id="vector-spaces">Vector Spaces</h2>
<p>The reader will be vaguely familiar already with the notion of
vectors, which may have been mentioned in physics and coordinate
geometry. However, they probably will have usually thought of them as
objects with magnitude (length) and direction; in particular,
geometrically one might have represented vectors with arrows and
extrapolated this idea to include the basic operations of vector
addition and scalar multiplication of vectors (Figure 1).<br />
<br />
This concept of all vectors as geometric arrows is not very helpful
here, and must be for the purpose of starting to learn quantum mechanics
completely discarded. Instead, we must think of them simply as
<strong>objects which can be summed</strong>, which are usually parts of
collections, called “vector spaces", with regular structures. Numbers
can be summed, but vectors can be objects which are not numbers but
which still can to be added to each other; for the purpose of quantum
mechanics, having this full space of “things which can be added”, but
which are not just simply numbers, will prove invaluable. This is
because we we need mathematical objects to form bijections (one-to-one
correspondences) with physical states: but by the superposition
principle any given group of states must be able to be superposed and
result some new physical state, and therefore the mathematical objects
representing them must be able to be summed together to make a new such
mathematical object which represents this new physical superposition
state. So it is absolutely essential to bear in mind that functions like
<span class="math inline">\(x^2\)</span> or <span
class="math inline">\((5x-3)^{3}\)</span> can be vectors just as much as
arrows in a coordinate system can be vectors: so long as there is a
system where they can be added together and a collection of other
objects which fit together into the system. We call this system of
objects able to be summed together a vector space, and those objects
constituent vectors. To support this new idea of vectors, we will
therefore be replacing the misleading arrow notation of <span
class="math inline">\(\vec{V}\)</span> with simply <span
class="math inline">\({V}\)</span>. At first, vectors will feel
extremely intangible given that they are only represented by letters
giving no indication what they are, but this is fine. Learning about the
mathematical system which contains the vectors which will represent
physical states is what matters, for now, because there are many
operations and ideas to work through.<br />
<br />
For a self-contained system working through the lens of linear algebra
there exists a need for a vector space: that is, the space containing
all relevant vectors where certain mathematical operations can be run
without ever requiring any vectors outside the vector space. Similarly,
for quantum mechanics there also exists a vector space for specific
vectors of interest, (which is called state space). If we can understand
this vector space we can then understand the component vectors a lot
better: in other words, our formalism of quantum mechanics becomes
defined at its farthest boundaries. Therefore, we will start by
introducing the definitions mathematicians use to define a valid vector
space so we can start to move towards this goal.<br />
<br />
A vector space <span class="math inline">\(\mathbb{V}\)</span> is a set
of vectors with the following properties:</p>
<ol>
<li><p><em>Null Vector</em>: Every vector space contains the unique null
vector <span class="math inline">\(0\)</span>, with properties <span
class="math display">\[\forall \bm{\alpha} \in \mathbb{V}, \:\:\:
\bm{\alpha} + 0 = \bm{\alpha}\]</span> and <span
class="math display">\[\forall \bm{\alpha} \in \mathbb{V}, \:\:\:
0\times\bm{\alpha} = 0.\]</span> Note that this is not a number, but a
vector (though it could be a number if our vector spaces consisted of
numbers as constituent vectors). Similarly, the identity vector <span
class="math inline">\(I\)</span> (sometimes, 1) is defined to be the
vector such that <span class="math display">\[\forall \bm{\alpha} \in
\mathbb{V}, \:\:\: 1\times\bm{\alpha} = \bm{\alpha}.\]</span> Indeed,
the labels <span class="math inline">\(0\)</span> and <span
class="math inline">\(1\)</span> are reasonable since they are very
clearly the vector analogs of the numbers <span
class="math inline">\(0\)</span> and <span
class="math inline">\(1\)</span>.</p></li>
<li><p><em>Additive closure</em>: For a defined rule for producing a sum
of two vectors, <span class="math display">\[\forall
\bm{\alpha},\beta\in\mathbb{V}, \btab \bm{\alpha} + \beta \in
\mathbb{V}.\]</span> The fact that the sum of any of the vectors in the
vector space is another vector in the vector space means the set is
<strong>complete</strong>, and will be a very important point for our
requirements, owing to the superposition principle.</p></li>
<li><p><em>Commutative property of addition</em>: For a defined rule for
producing a sum of two vectors, <span class="math display">\[\bm{\alpha}
+ \beta =  \beta +  \bm{\alpha}.\]</span></p></li>
<li><p><em>Associative property of addition</em>: For a defined rule for
producing a sum of two vectors, <span class="math display">\[\bm{\alpha}
+ (\beta +\gamma) =  (\bm{\alpha} + \beta) +\gamma.\]</span></p></li>
<li><p><em>Additive inverses</em>: For every vector <span
class="math inline">\(\bm{\alpha}\)</span>, there exists a unique
additive inverse vector <span
class="math inline">\(-\bm{\alpha}\)</span> such that <span
class="math display">\[\bm{\alpha} + -\bm{\alpha} = 0.\]</span></p></li>
<li><p><em>Distributive property of scalar multiplication</em>: For a
defined rule for vector multiplication by scalars, <span
class="math display">\[c(\bm{\alpha}+ \beta) = c\bm{\alpha} +
c\beta.\]</span> is the requisite distributive property for vectors of
scalar multiplication of vectors. There is also a distributive property
for scalars in scalar multiplication of vectors, which is similar: <span
class="math display">\[(c_1+c_2)\bm{\alpha} = c_1\bm{\alpha} + c_2
\bm{\alpha}.\]</span></p></li>
<li><p><em>Associative property of scalar multiplication</em>: For a
defined rule for vector multiplication by scalars, <span
class="math display">\[c_1(c_2\bm{\alpha})=c_1c_2\bm{\alpha}.\]</span></p></li>
</ol>
<p>Most of these properties are hardly unusual to us and do not require
much thought, such as the associative properties, because it is unlikely
the reader will have worked with mathematical objects which do not
follow them before (note the arithmetic numbers certainly do). The
closure of addition is the critical point, however, because it allows us
to get a sense of why vectors are a useful way to denote objects which
one might want to add together: they can fit into a vector space which
then provides an mathematical enclosure so one knows that no matter how
many different constituent vectors they add together, they will never
“break” the vector space and end up with a new type of vector outside of
the vector space. This enclosed space of objects will be useful for
quantum mechanics, as we want to be able to superpose <em>any</em>
number of states without creating a non-physical state– and therefore,
we want to be able to add the mathematical objects which represent
physical states without creating a mathematical object which is not in
that set of mathematical objects representing physical states.<br />
<br />
There exists one final property of vector spaces which make them crucial
to quantum mechanics. It pertains to the problem of defining a vector
space. One can imagine that in order for additive closure to be a
property of a vector space one needs a huge number of vectors to
constitute most vector spaces, since we need every possible sum between
two or three or any number of vectors in the space. Thus trying to list
constituent vectors as a set in order to specify a vector space would be
useless and impossible. We need a shorter way to specify a vector space
we are working with. The elegant solution to this problem, of accounting
for any possible combination of different vectors, is through the
concept of a <strong>basis</strong>.</p>
<h3 id="dimensions-of-a-vector-space">Dimensions of a Vector Space</h3>
<p>We have expressed the definition of a vector space through
comprehensive discussion, but are no closer yet to having a concrete
understanding of what examples might actually be. We will now consider
the most common vector space we work with: the Cartesian 2-dimensional
plane (figure 2).<br />
<br />
There is no need to further explain the coordinate system– only that
clearly it does satisfy all the rules above for a vector space
(including that of additive inverses, when we include the negative y and
x axis). This vector space is known as the coordinate representation of
<span class="math inline">\(\mathbb{R}^2\)</span>: the 2-dimensional
(hence the exponent 2) real-valued (taking real values only) vector
space: where the constituent vectors are numbers!<br />
<br />
How do we define the dimensions here? Qualitatively, we are familiar
with the concept of the <span class="math inline">\(x\)</span>-axis and
<span class="math inline">\(y\)</span>-axis, but mathematically it is
less easy to say why they qualify as dimensions. The answer lies in the
definition of <strong>linear independence</strong>, whence this idea of
a basis we need will follow .<br />
<br />
A set of <span class="math inline">\(n\)</span> linearly independent
vectors <span class="math inline">\(\bm{\alpha}_{i}\)</span> is defined
mathematically as follows: <span
class="math display">\[\sum_{i=1}^{n}c_{i}\bm{\alpha}_{i}=0
\Rightarrow\:\:\: \forall i, \:\:\: c_{i}=0.\]</span> In words, there is
no nontrivial combination of linearly independent vectors which equals
<span class="math inline">\(0\)</span> when summed together– a trivial
combination would occur where all the multiplicative coefficients are
<span class="math inline">\(0\)</span>. If a linear combination of
linearly independent vectors sums to zero then it implies that the
coefficients in the combination must be zero. If there exists some
combination without all the coefficients equalling <span
class="math inline">\(0\)</span>, then the vectors are not linearly
independent.<br />
<br />
Let’s consider a few examples, using <span
class="math inline">\(2\times2\)</span> square matrices as vectors and
the <span class="math inline">\(2\times2\)</span> null matrix as the
<span class="math inline">\(0\)</span> vector. If <span
class="math display">\[\bm{\alpha}_{1} = \begin{bmatrix}
    1 &amp; 0 \\
    0 &amp; 0 \\
    \end{bmatrix}, \:\:
\bm{\alpha}_{2} = \begin{bmatrix}
    0 &amp; 2 \\
    0 &amp; 0 \\
    \end{bmatrix}, \:\:
\bm{\alpha}_{3}= \begin{bmatrix}
    0 &amp; 0 \\
    3 &amp; 0 \\
    \end{bmatrix}, \:\:
\bm{\alpha}_{4} = \begin{bmatrix}
    0 &amp; 0 \\
    0 &amp; 4 \\
    \end{bmatrix}\]</span> and we set some combination with coefficients
<span class="math inline">\(\setof{c_{i}}\)</span> <span
class="math display">\[\sum_{i=0}^{4}c_{i}\bm{\alpha}_{i} = 0,\]</span>
then <span class="math display">\[\begin{aligned}
c_{1}\bm{\alpha}_{1} &amp;= c_{1}\begin{bmatrix}
    1 &amp; 0 \\
    0 &amp; 0 \\
    \end{bmatrix}, \:\:
c_{2}\bm{\alpha}_{2} = c_{2}\begin{bmatrix}
    0 &amp; 2 \\
    0 &amp; 0 \\
    \end{bmatrix}, \:\:
c_{3}\bm{\alpha}_{3} = c_{3}\begin{bmatrix}
    0 &amp; 0 \\
    3 &amp; 0 \\
    \end{bmatrix}, \\
c_{4}\bm{\alpha}_{4} &amp;= c_{4}\begin{bmatrix}
    0 &amp; 0 \\
    0 &amp; 4 \\
    \end{bmatrix} \\
\Rightarrow \:\: \sum_{i=0}^{4}c_{i}\bm{\alpha}_{i}&amp;=
\begin{bmatrix}
c_{1} &amp; 2c_{2} \\
3c_{3} &amp; 4c_{4} \\
\end{bmatrix}
\Rightarrow \:\: \begin{bmatrix}
c_{1} &amp; 2c_{2} \\
3c_{3} &amp; 4c_{4}
\end{bmatrix} =
\begin{bmatrix}
0 &amp; 0 \\
0 &amp; 0 \\
\end{bmatrix}\\
\end{aligned}\]</span> so we can see that we necessarily must have <span
class="math display">\[c_{1}=c_{2}=c_{3}=c_{4}=0\]</span> and therefore
<span class="math display">\[\bm{\alpha}_{1} = \begin{bmatrix}
    1 &amp; 0 \\
    0 &amp; 0 \\
    \end{bmatrix}, \:\:
\bm{\alpha}_{2} = \begin{bmatrix}
    0 &amp; 2 \\
    0 &amp; 0 \\
    \end{bmatrix}, \:\:
\bm{\alpha}_{3} = \begin{bmatrix}
    0 &amp; 0 \\
    3 &amp; 0 \\
    \end{bmatrix}, \:\:
\bm{\alpha}_{4} = \begin{bmatrix}
    0 &amp; 0 \\
    0 &amp; 4 \\
    \end{bmatrix}\]</span> are linearly independent vectors.<br />
<br />
On the contrary, if <span class="math display">\[\beta_{1} =
\begin{bmatrix}
    -1 &amp; 0 \\
    5 &amp; 10 \\
    \end{bmatrix}, \:\:
\beta_{2} = \begin{bmatrix}
    1 &amp; 3 \\
    1 &amp; -1 \\
    \end{bmatrix}, \:\:
\beta_{3}= \begin{bmatrix}
    2 &amp; 7 \\
    4 &amp; 1 \\
    \end{bmatrix}\]</span> Then <span class="math inline">\(c_1=1, \:\:
c_2=7, \:\: c_{3}=-3\)</span> gives <span
class="math display">\[\begin{aligned}
c_{1}\beta_{1}+c_2\beta_{2}+c_3\beta_{3}
&amp;=  1\begin{bmatrix}
    -1 &amp; 0 \\
    5 &amp; 10 \\
    \end{bmatrix} + 7\begin{bmatrix}
    1 &amp; 3 \\
    1 &amp; -1 \\
    \end{bmatrix} -3\begin{bmatrix}
    2 &amp; 7 \\
    4 &amp; 1 \\
    \end{bmatrix}\\
&amp;= \begin{bmatrix}
    -1 &amp; 0 \\
    5 &amp; 10 \\
    \end{bmatrix}+
\begin{bmatrix}
    7 &amp; 21 \\
    7 &amp; -7 \\
    \end{bmatrix}+
\begin{bmatrix}
    -6 &amp; -21 \\
    -12 &amp; -3 \\
    \end{bmatrix}\\
&amp;=\begin{bmatrix}
0 &amp; 0 \\
0 &amp; 0 \\
\end{bmatrix}=0
\end{aligned}\]</span> which is a nontrivial combination of the vectors
<span class="math inline">\(\beta_{1}\)</span>, <span
class="math inline">\(\beta_{2}\)</span>, <span
class="math inline">\(\beta_{3}\)</span> as the coefficients are not all
<span class="math inline">\(0\)</span>. Therefore, <span
class="math display">\[\beta_{1} = \begin{bmatrix}
    -1 &amp; 0 \\
    5 &amp; 10 \\
    \end{bmatrix}, \:\:
\beta_{2} = \begin{bmatrix}
    1 &amp; 3 \\
    1 &amp; -1 \\
    \end{bmatrix}, \:\:
\beta_{3} = \begin{bmatrix}
    2 &amp; 7 \\
    4 &amp; 1 \\
    \end{bmatrix}\]</span> are not linearly independent vectors (we say
therefore they are linearly dependent) since there exists at least one
nontrivial linear combination of them which sums to make the <span
class="math inline">\(0\)</span> vector.<br />
<br />
Now that we have established this definition of linear dependence,
defining the dimensions of a vector space is simple. We have 3 crucial
definitions and theorems:</p>
<ol>
<li><p>An <span class="math inline">\(n\)</span>-dimensional vector
space contains <span class="math inline">\(n\)</span> linearly
independent vectors.</p></li>
<li><p>The set of <span class="math inline">\(n\)</span> linearly
independent vectors in the <span
class="math inline">\(n\)</span>-dimensional vector space is called the
<strong>basis</strong> of the vector space.</p></li>
<li><p>Each vector <span class="math inline">\(v\)</span> in an <span
class="math inline">\(n\)</span>-dimensional vector space with
basis<br />
<span class="math inline">\(\mathbb{B}= \{\bm{\alpha}_1, \bm{\alpha}_2,
...\bm{\alpha}_n\}\)</span> can be expressed as <span
class="math display">\[v=\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i},\]</span> a
unique linear combination of the linearly independent basis vectors. The
coefficients <span class="math inline">\(c_{i}\)</span> of the expansion
for <span class="math inline">\(v\)</span> are called the
<strong>components</strong> of the vector <span
class="math inline">\(v\)</span> in the basis.</p></li>
</ol>
<p>Definition <span class="math inline">\(3\)</span> contains two
assertions. The first statement is that each vector <span
class="math inline">\(v\)</span> in an <span
class="math inline">\(n\)</span>-dimensional vector space with basis
<span class="math inline">\(\mathbb{B}= \{\bm{\alpha}_1, \bm{\alpha}_2,
...\bm{\alpha}_n\}\)</span> can be expressed as <span
class="math display">\[v=\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}\]</span> for
some scalar coefficients <span
class="math inline">\(\setof{c_{i}}\)</span>. This can be proved by
contradiction. Assume that in the <span
class="math inline">\(n\)</span>-dimensional vector space with basis
<span class="math inline">\(\mathbb{B}= \{\bm{\alpha}_1, \bm{\alpha}_2,
...\bm{\alpha}_n\}\)</span> there exists some vector <span
class="math inline">\(v\)</span> which is not a linear combination of
the basis vectors. Clearly <span class="math inline">\(\forall j, \:\:\:
v \neq \bm{\alpha}_{j} \in \mathbb{B}\)</span> since otherwise the
linear combination <span
class="math display">\[1\times\bm{\alpha}_{j}(x)+ \sum_{i=0\neq
j}^{n}0\times\bm{\alpha}_{i}=v\]</span> would immediately violate the
assumptions about <span class="math inline">\(v\)</span> not being able
to be expressed as a linear combination of the basis vectors. But then
this would mean that all the coefficients <span
class="math inline">\(c_{i}\)</span> are <span
class="math inline">\(0\)</span> since any coefficient being nonzero
would create a valid linear combination of basis vectors equalling the
vector <span class="math inline">\(v\)</span>, and so <span
class="math display">\[v=\sum_{i=0}^{n}0\times\bm{\alpha}_{i} +
v\]</span> but then: <span
class="math display">\[\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i} + c_{n+1}v= 0
\Rightarrow\:\:\: c_{n+1}v=-\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i},\]</span>
which is, <span
class="math display">\[v=-\sum_{i=0}^{n}\left(\frac{c_{i}}{c_{n+1}}\right)\bm{\alpha}_{i}.\]</span>
This then violates the assumption that <span
class="math inline">\(v\)</span> cannot be expressed as a linear
combination of the basis vectors since the coefficients <span
class="math inline">\(c_{i}/c_{n+1}\)</span> corresponding to basis
vectors <span class="math inline">\(\bm{\alpha}_{i}\)</span> would give
a valid linear combination producing <span
class="math inline">\(v\)</span>. The only way this would not be true
would be if <span class="math inline">\(c_{n+1}=0\)</span>. In other
words, if we want our assumption to hold true, we get <span
class="math display">\[\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i} + c_{n+1}v= 0
\Rightarrow\:\:\:
c_{n+1}=0\Rightarrow\:\:\:\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}=0\Rightarrow\:\:\:\forall\stab
i, c_{i}=0.\]</span> By definition, therefore, in order for the
assumption to be true <span class="math inline">\(v\)</span> must be a
vector linearly independent from the other basis vectors since there is
no non-trivial combination of the linearly independent basis vectors and
the new vector <span class="math inline">\(v\)</span> which sums to the
null vector. This however means there is a contradiction since we
defined <span class="math inline">\(v\)</span> to be a vector in the
<span class="math inline">\(n\)</span>-dimensional space, but adding the
new vector <span class="math inline">\(v\)</span> would mean there are
<span class="math inline">\(n+1\)</span> linearly independent vectors in
an <span class="math inline">\(n\)</span>-dimensional space: defined in
Definition 1 to be impossible. Therefore any vector <span
class="math inline">\({v}\)</span> which is not a linear combination of
the basis vectors cannot exist in the vector space; all the vectors in
the vector space must be able to be expressed by a linear combination of
the basis vectors. We say that the basis (which we must remember is the
<em>set</em> of basis vectors and so is be treated as a singular entity
in reference) <strong>spans</strong> the vector space, because all
vectors in the vector space can be created by some linear combination of
the constituent basis vectors of that basis.<br />
<br />
The second simple but important theorem to prove is that the expansion
for any given vector <span class="math inline">\(v\)</span> <span
class="math display">\[v = \sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}\]</span>
is unique. We again prove this by contradiction. Assume <span
class="math display">\[v =
\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}=\sum_{i=0}^{n}c&#39;_{i}\bm{\alpha}_{i}\]</span>
for <span class="math inline">\(\setof{c_{i}} \neq
\setof{c&#39;_{i}}\)</span>. Then, for basis vectors <span
class="math inline">\(\bm{\alpha}_{i} \neq 0\)</span>, <span
class="math display">\[\begin{aligned}
v - v = 0=
&amp;\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}-\sum_{i=0}^{n}c&#39;_{i}\bm{\alpha}_{i}=\sum_{i=0}^{n}(c_{i}-c&#39;_{i})\bm{\alpha}_{i}.
\end{aligned}\]</span> However, this combination of basis vectors is
equal to <span class="math inline">\({0}\)</span>, but the basis vectors
<span class="math inline">\(\setof{\bm{\alpha}_{i}}\)</span> are
linearly independent so necessarily this implies the new coefficients
<span class="math inline">\(\setof{c_{i}-c&#39;_{i}}\)</span> are <span
class="math inline">\(0\)</span>. So <span
class="math display">\[\forall i, \:\: (c_{i}-c&#39;_{i})=0
\Rightarrow\:\: c_{i} = c&#39;_{i}\]</span> a clear contradiction with
the original assumption that the coefficients could be different and
thereby produce more than one unique way of expressing a vector in the
basis. So indeed, each vector in a vector space is uniquely specified by
one single linear combination of the basis vectors of the vector
space.<br />
<br />
New definitions follow:</p>
<ol>
<li><p>The basis is the set of vectors spanning the vector
space.</p></li>
<li><p>For each constituent vector in the vector space with basis <span
class="math inline">\(\setof{\bm{\alpha}_{i}}\)</span> defined to be
<span
class="math inline">\(v:=\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}\)</span>, we
call the coefficients <span class="math inline">\(\setof{c_{i}}\)</span>
the <strong>components</strong> of the vector in that basis.</p></li>
<li><p>The sum which uniquely defines a vector of the vector space in a
given basis is called its <strong>expansion</strong> in that
basis.</p></li>
</ol>
<p>Critically, note that we always talk about components <em>in a
basis</em> and expansions <em>in a basis</em>. Without some underlying
basis, none of those concepts make sense. In vector spaces with multiple
possible bases– which we shall see very much exist– expansions and
thereby components for the same vector are very different depending on
which of the bases we are working in.<br />
<br />
It is now clear why a basis is powerful for its original role– to
describe a vector space. This is because with simply the basis vectors
(which can be chosen quite easily for some vector spaces) and the
components of <em>any</em> constituent vector in the space spanned by
that basis, we can uniquely specify that vector. Then, with this
knowledge of unique expansions in mind, we can define the rules for the
simple operations addition, subtraction and multiplication in a vector
space:<br />
<br />
To multiply a vector <span class="math inline">\(v\)</span> by scalar
<span class="math inline">\(k\)</span>, we multiply its components each
by scalar <span class="math inline">\(k\)</span>: <span
class="math display">\[kv =
k\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}=\sum_{i=0}^{n}kc_{i}\bm{\alpha}_{i}.\]</span><br />
To sum two vectors <span
class="math inline">\(v=\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}\)</span> and
<span
class="math inline">\(w=\sum_{i=0}^{n}c&#39;_{i}\bm{\alpha}_{i}\)</span>
we sum their components: <span class="math display">\[v + w =
\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}+\sum_{i=0}^{n}c&#39;_{i}\bm{\alpha}_{i}
= \sum_{i=0}^{n}(c_{i}+c&#39;_{i})\bm{\alpha}_{i},\]</span> which is a
new expansion uniquely specifying a new vector in the vector space, and
for subtraction of <span class="math inline">\(w\)</span> from <span
class="math inline">\(v\)</span> we subtract the components of <span
class="math inline">\(w\)</span> from the corresponding components of
<span class="math inline">\(v\)</span>: <span class="math display">\[v -
w =
\sum_{i=0}^{n}c_{i}\bm{\alpha}_{i}-\sum_{i=0}^{n}c&#39;_{i}\bm{\alpha}_{i}
= \sum_{i=0}^{n}(c_{i}-c&#39;_{i})\bm{\alpha}_{i}\]</span> which is also
a new expansion uniquely specifying a new vector in the vector space.
The additive inverse is obtained by multiplying a vector by the negative
identity: <span
class="math display">\[w=\sum_{i=0}^{n}c&#39;_{i}\bm{\alpha}_{i}
\implies -w=\sum_{i=0}^{n}-c&#39;_{i}\bm{\alpha}_{i},\]</span>
multiplying by the null vector means multiplying all the coefficients by
<span class="math inline">\(0\)</span> (creating the null vector, as
expected), and multiplying by the identity means doing nothing to the
components, resulting in the same vector.</p>
<h2 id="the-state-vector">The State vector</h2>
<p>We are finally ready to introduce the mathematical objects chosen to
be in bijection with physical states, having hinted heavily that with
additive closure vectors in some vector spaces are the answer. This is
the First Postulate of Quantum Mechanics.<br />
<br />
<u><strong>Postulate 1: The State Vector and its
Wavefunctions</strong></u><br />
<br />
Any physical states at time <span class="math inline">\(t\)</span> can
be represented by a state vector <span
class="math inline">\(\Psi_{t}\)</span>. State vectors are
complex-valued vectors which stand in a bijection with vectors in a
Hilbert space, which we call the state space; state vectors can be
transformed into unique probability distribution functions, called
wavefunctions, to give probabilities of different measurements of
different observables occurring. So there we have it: the mathematical
objects we have been alluding to, which are in a bijection with physical
states and can therefore be used as substitutes for them in order to
perform physical computations. There is a lot to unpack with this
postulate, which is central to quantum mechanics and the state problem,
so we will now do this systematically.<br />
<br />
The state vector is a vector in the vector space we call the state
space. While we will cover the state space subsequently, it is
absolutely crucial at this point to remember the previous section on
vector spaces, which showed us that all we need chiefly is additive
closure and a set of objects can be considered vectors in a vector
space. The additive closure property is critical for superpositions,
which as previously explained, make this necessary because we need to be
able to add any number of states without making an unphysical state, and
therefore also need to be able to add any number of state vectors
without creating a vector outside of the state space.<br />
<br />
The state vector is also a powerful mathematical tool because it can be
broken up into basis vectors; an infinite dimensional space like the
state space has infinite bases of infinite cardinality. The importance
of this pertains to the second part of the postulate. The second part
tells us that from the state vector we can generate different unique
probability distribution functions, called wavefunctions, to give
probabilities of different measurements <em>for different
observables</em>: the ability to express the state vector in different
bases will later be shown to be essential for considering the state
represented by the state vector with respect to the different
observables we are concerned with. These wavefunctions are the part of
the Postulate which are most tangible to us, because we will directly be
able to achieve numerical values from them.<br />
<br />
We continue now by dealing with this postulate and the rest of its
assertions with a bit more detail. This book will not have such a
verbose discourse on any other postulate of quantum mechanics than that
which is to follow, and there is a serious reason for making so many
clarifications prior to starting to consider the mathematics of state
vectors– everything else is predicated on this postulate!</p>
<ol>
<li><p>The functionality of the state vector is very important to
understand fully. We recall that in our section on bijections there were
two important employments of this relationship between two sets: the
first, for labelling, and the second, for substituting. The labelling
function of the state vector is very clear. Normally a capital Psi,
<span class="math inline">\(\Psi\)</span>, is used, and we can simply
say “the state <span class="math inline">\(\Psi\)</span>”, just like we
can say “ball <span class="math inline">\(a\)</span>”. We know that
every <span class="math inline">\(\Psi\)</span> is unique and in
bijection to each state, so it acts well as a unique label.<br />
<br />
As a substitute for physical states, we have said that the main goal of
this chapter is to produce something mathematically manipulable since
physical states are not. The main operation we will perform on it–
called the inner product– which will be shown imminently, will be useful
for multiple purposes: but the most useful will be to transform the
state vector into the probability distribution functions called
wavefunctions. We recall from the preliminary on probability that a
probability distribution function is a function we can use to
encapsulate different possibilities and their probabilities in a single
function, so the operation which produces a probability distribution
function from the state vector representing a state will be essentially
the final step of considering that state with respect to certain
observables. Achieving these wavefunctions is normally the final goal of
solving a problem in quantum mechanics.</p></li>
<li><p>Time is an important factor, of course, to the state problem, but
it represented here by a subscript rather than a variable in <span
class="math inline">\(\Psi_{t}\)</span>. The subscript is also helpful
to remember that each state vector represents a physical state at an
instantaneous time <span class="math inline">\(t\)</span> (since any
state only exists at a single time before it transforms into a new
state). Mainly because we are still covering the state problem, it is
unwise to consider time too much while there is so much to learn with
regards to the encapsulation and extraction of information; therefore,
all of the discussion on quantum states we will have takes place at one
instance of time.<br />
<br />
However, there is more commonly in quantum mechanics the notation <span
class="math display">\[\Psi(t),\]</span> representing the state vector
as a function of time. This doesn’t mean the state vector is something
like <span class="math display">\[\Psi(t)=t^2,\]</span> but rather that
<span class="math display">\[\Psi(t)=\Psi_{t}.\]</span> In other words,
the function notation is just a shorthand of referring to the same
isolated system across different moments in time, where inputting a time
value gives the state vector at that time. This is the same idea that we
are using, but just more concise, so there is no need to get confused if
we see this written elsewhere. Again, we are dealing with stationary
states, so the notation <span class="math inline">\(\Psi_{t}\)</span> is
valid, intuitive and sufficient especially in these stationary
cases.</p></li>
<li><p>A mathematical vector and a physical circumstance is not the same
thing, naturally. Nevertheless, we often call a state vector “a state”.
The reason is because of the bijection. In those cases, we should say
“one represents the other”. However, we will still sometimes end up
saying “one <em>is</em> the other” when this bijection exists– even
though they might technically be different types of objects which
therefore cannot <em>be</em> each other. Such grammatical
simplifications are simply because having the bijection eliminates
ambiguity. For a physical state, we cannot really say “the hydrogen
electron in a magnetic field with one third probability of having
momentum <span class="math inline">\(p_{1}\)</span>, one seven-hundred
and eighth probability of having momentum <span
class="math inline">\(p_{2}\)</span>...” since this would be a
manifestly inefficient method of specifying it. Thus, a mathematical
representation like the state vector encapsulates all these details of a
state concisely while being linked to that state with no ambiguity; we
therefore call it the state.<br />
<br />
This seems like a terribly semantically pedantic discussion, but there
will be times where readers get confused by the difference between <span
class="math inline">\(A\)</span> is <span
class="math inline">\(B\)</span> and <span
class="math inline">\(A\)</span> is in a bijection with <span
class="math inline">\(B\)</span>. With this clarification it is hoped
such confusions are eliminated in the mind of the reader. So long as one
follows the mathematics, the answer to the difference discussed above
will always be clear, and this needn’t ever evolve into a massive
obstacle in understanding. When two things are deemed
<strong>equivalent</strong>, the reader should simply consider closely
how they may be related to each other, rather than jump to conclusions
that they have somehow misunderstood different types of objects.
Remember the failed state denomination during the Stern Gerlach
experiment: we should not get attached to trying to make mathematical
symbols sacred, and should only concern ourselves with the
functionalities they provide. Therefore, the phrase ‘the state <span
class="math inline">\(\Psi_{1}\)</span>’ would be perfectly
acceptable.</p></li>
<li><p>This book will use the letter <span
class="math inline">\(\psi\)</span> ubiquitously both in its upper case
form (<span class="math inline">\(\Psi\)</span>) and its lower case form
(<span class="math inline">\(\psi\)</span>). When a reader sees the
capital letter <span class="math inline">\(\Psi\)</span>, we are
referring to a state vector. The capital <span
class="math inline">\(\Psi\)</span> will never be used to denote
anything which is not a state.</p></li>
<li><p>Though I have said the state space is infinite dimensional, we
will be working under the assumption that everything we are doing takes
place in a discrete space until chapter 7. This doesn’t violate the
conditions of the postulate, though explaining why exactly this is takes
us to discussion of infinities (cf. Chapter 6) which is wholly
unnecessary. We just have to accept the assumption that we are working
in discrete cases, and it will be enough for us to build up our
understanding of what we actually need to understand at this moment. The
fact we are working in discrete cases, for example, means that we can
use our sigma summation notation very comfortably whilst postponing
integrals to another chapter where will be much better prepared and
disposed to deal with them.</p></li>
</ol>
<p>Now that these clarifications about state vectors have been made, we
can move onto studying the state space, as well as the all-important
inner product operation, in more detail.</p>
<h3 id="the-state-space">The State Space</h3>
<p>The vector space relevant to quantum mechanics is a Hilbert space
<span class="math inline">\(\mathscr{H}\)</span>, which contains all the
state vectors corresponding to possible states. It turns out that we do
not have to worry about the name of the space too much: even though
Hilbert spaces are special types of vector spaces very much constituting
their own mathematical field with much deep analysis and theory, these
analyses are matters of mathematical rigor and for our purposes we have
neither the luxury nor the need to be so precise. In general, therefore,
we will call it, by convention, the state space– since it is the vector
space of state vectors– to avoid the exotic ‘Hilbert space’ name getting
in our way and drawing more attention than it needs to.<br />
<br />
There is an important idea to understand about the state space and
therefore the state vector, however. The human perspective is to try and
imagine the vector space and its components in some sort of physical
form. It is expected that, despite warnings not to, any reader who has
seen the arrow representation of 2 dimensional vectors will be thinking
of something similar every time they think about vectors– or perhaps
even envisaging other melodramatic depictions of quantum mechanics in
popular media.<br />
<br />
However, we need to be extremely careful here. Vectors are abstract
entities which cannot be imagined physically <em>until we choose a
basis</em>; this point must be hugely emphasised. In different bases,
the vector takes different forms: based on its unique expansions in
those bases! Without picking a basis, the state vector very much exists:
it just simply isn’t given an explicit form yet. It is usually
impossible to predict which form a vector will take in different bases
until we know what the bases are and perform some operations, because
there is no set pattern each vector follows. To fully deliver this idea
of a state vector being able to exist without an explicit mathematical
form, a vernacular analogy will be given– to consolidate an
understanding for the reader at the cost of temporarily violating the
rigorous nature of the text.<br />
<br />
Take a vector we probably have seen before in common use: the velocity
vector of a moving car. If one asked whether it exists, then obviously
the answer would be the affirmative: a moving car (or even a stationary
one) must have a velocity vector. However, if someone were to point to a
moving car and ask you to write down its velocity vector on a piece of
paper, then you would find it impossible. This is because it can take a
wide variety of forms depending on how we define our coordinate system–
we can use an axis where moving to the left of us is negative, but we
can also use one where moving to the right of us is negative (changing
the values of the velocity vector we are employing); we can consider it
with respect to another moving car or our sedentary selves, but we can
also consider it with respect to any fixed point as an origin of
movement; in any case we could pick any moving car or any origin of
movement and this would give us different <em>forms</em> of the velocity
vector: because the numbers are different depending on what we are
considering the car’s motion with respect to. However– no matter what
coordinate system we choose, the vector exists. We do not need an origin
of displacement for a velocity vector to exist– we only need an origin
of displacement for the velocity vector to take a form! <strong>It
should be clear that to exist as a mathematical object which can take
multiple forms depending on the underlying frame of reference we used is
different from taking one specific form</strong>. For the state vector,
we have the very same idea: we choose a basis, analogous to defining an
axes in the above example, and we specify its components, analogous to
the familiar process of specifying coordinates. Both the basis and the
components are necessary for us to ‘pinpoint’ the state vector, but it
exists to be expressed in infinite different bases and sets of
components regardless of whether or not we give it a certain algebraic
specification.<br />
<br />
The example of the car velocity vector should enforce a crucial idea of
abstract mathematical objects existing but being impossible to express
explicitly. In the exact same way, a state vector always exists no
matter how we choose to see it or portray it, and to give it a single
specific form, we need to pick a basis, which is equivalent to picking a
coordinate system in the example above. It does not have a form until we
pick a basis. So if we ever see the statement that the state vector is
<span class="math display">\[\Psi(x)=cx^{2}\]</span> (for example), then
we should immediately realise that this statement implies some
underlying basis, because there must be some basis in order for it to
have this explicit algebraic form. Otherwise, there would be no way we
could express the state vector as a function of some variable in this
way. Any mathematical equation we run with state vectors will always be
implicitly taking place in some basis; that does not at all mean things
would look that way in all bases, but one can assume that the basis
picked will be the most convenient one– just like we would not pick the
Tokyo Tower to be the origin of displacement for the velocity vector of
a car in London, because that form would be inconvenient and
uninformative. Choosing bases intelligently, we will see, is already in
itself an important problem solving tool in quantum mechanics.<br />
<br />
Next, beyond simple operations like scalar multiplication, quantum
mechanics employs another operation, which will be extremely central to
all of quantum mechanics. This is the inner product, which will be as
common in quantum mechanics as multiplication in arithmetic.</p>
<h3 id="inner-products">Inner Products</h3>
<p>Suppose we are working in a basis. There then exists a new operation,
called the <strong>inner product</strong>, between two state vectors
<span class="math inline">\(\Psi_{1}\)</span> and <span
class="math inline">\(\Psi_{2}\)</span> in the state space. This inner
product is denoted as <span
class="math inline">\(\oip{\Psi_{1}}{\Psi_{2}}\)</span> and is defined
to be: <span
class="math display">\[\oip{\Psi_{1}}{\Psi_{2}}:=\sum_{\{x\}}\bar{\psi}_{1}^{\ast}(x)\psi_{2}\equiv\sum_{\{i\}}c^{(1)\ast}_{i}c^{(2)}_{i}\]</span>
for components <span class="math inline">\(\setof{c^{(1)}_{i}}\)</span>
for <span class="math inline">\(\Psi_{1}\)</span> and components <span
class="math inline">\(\setof{c^{(2)}_{i}}\)</span> for <span
class="math inline">\(\Psi_{2}\)</span> .<br />
<br />
We will see more often <span
class="math display">\[\oip{\Psi_{1}}{\Psi_{2}}:=\sum_{\{i\}}c^{(1)\ast}_{i}c^{(2)}_{i}\]</span>
because there is no need to involve the discrete wavefunction in things
when components are easy to track in the discrete case. This form is
also very illustrative: it shows us that all the inner product is doing
is producing a sum of the products of matching components, with one its
complex conjugate. Many readers of this book will be able to understand
what I refer to when I say this is essentially the quantum mechanical
(really, simply complex valued) equivalent of a vector dot product.
However, for less advanced readers who have not met the latter before,
this point is not important.<br />
<br />
Next, most of the time the inner product is non-commutative- the order
matters. In fact, since we have the definition above, it is very easy to
see that what we must have is the relationship <span
class="math display">\[\oip{\Psi_{1}}{\Psi_{2}}=\oip{\Psi_{2}}{\Psi_{1}}^{\ast}.\]</span>
regardless if we try to see this in the discrete case or continuous
case. This is an essential short-form fact to memorise, as it will
return in algebraic manipulations. Now we list a few more facts about
the inner product:</p>
<ol>
<li><p>There is a kind of “constant multiple rule" which comes with the
inner product. We will always use the short-form of it, but expressing
the inner product in sum form makes everything completely clear: <span
class="math display">\[\oip{\Psi_{1}}{c\Psi_{2}}=\sum_{i=1}^{n}c^{(1)\ast}_{i}cc^{(2)}_{i}
=
c\sum_{i=1}^{n}c^{(1)\ast}_{i}c^{(2)}_{i}=c\oip{\Psi_{1}}{\Psi_{2}}\]</span>
and <span
class="math display">\[\oip{c\Psi_{1}}{\Psi_{2}}=\sum_{i=1}^{n}c^{\ast}c^{(1)\ast}_{i}c^{(2)}_{i}
=
c^{\ast}\sum_{i=1}^{n}c^{(1)\ast}_{i}c^{(2)}_{i}=c^{\ast}\oip{\Psi_{1}}{\Psi_{2}}.\]</span>
The short-form facts are simply <span
class="math display">\[\oip{\Psi_{1}}{c\Psi_{2}}=c\oip{\Psi_{1}}{\Psi_{2}},\:\:\:\:
\oip{c\Psi_{1}}{\Psi_{2}}=c^{\ast}\oip{\Psi_{1}}{\Psi_{2}}.\]</span></p></li>
<li><p>We now define the <strong>norm</strong> of a vector to be the
inner product of the vector with itself: <span
class="math display">\[\oip{\Psi_{1}}{\Psi_{1}}:=\sum_{i=1}^{n}c^{(1)\ast}_{i}c^{(1)}_{i}=\sum_{i=1}^{n}|c^{(1)}_{i}|^2.\]</span>
The modulus squared of a complex number is always real nonnegative so
the same applies here. The norm of a vector being <span
class="math inline">\(0\)</span> also would imply that the vector is the
null vector since all the components would be 0. Taking this further, we
require that every vector in the state space <span
class="math inline">\(\mathscr{H}\)</span> must have finite norm, and
that every vector which satisfies the other conditions and does have
finite norm is a vector in the state space. That is, <span
class="math display">\[\oip{\Psi_{1}}{\Psi_{1}}=\sum_{i=1}^{n}|c^{(1)}_{i}|^2&lt;\infty.\]</span>
Next, as <span
class="math display">\[\oip{\psi_{1}}{\psi_{2}}=\oip{\psi_{2}}{\psi_{1}}^{\ast},\]</span>
we must have <span
class="math display">\[\oip{\psi}{\psi}=\oip{\psi}{\psi}^{\ast}\implies
\oip{\psi}{\psi}\in\R.\]</span> The <em>positive semidefinite
metric</em> postulate further states that: <span
class="math display">\[\oip{\psi}{\psi} \geq 0\]</span> and <span
class="math display">\[\oip{\psi}{\psi} = 0 \iff\:\: \psi =
0.\]</span></p></li>
<li><p><span id="LDip" data-label="LDip"></span> Another important rule
which is immensely helpful in solving quantum mechanical problems is
that inner products can distribute linearly over a sum. This can be
again shown by writing out the sum form: <span
class="math display">\[\begin{aligned}
    \oip{\Psi_{1}}{a\Psi_{2}+b\Psi_{3}}&amp;=\sum_{i=1}^{n}c^{(1)\ast}_{i}(ac^{(2)}_{i}+bc^{(3)}_{i})=\sum_{i=1}^{n}c^{(1)\ast}_{i}ac^{(2)}_{i}+\sum_{i=1}^{n}c^{(1)\ast}_{i}bc^{(3)}_{i}\\
    &amp;=\oip{\Psi_{1}}{a\Psi_{2}}+\oip{\Psi_{1}}{b\Psi_{3}}=a\oip{\Psi_{1}}{\Psi_{2}}+b\oip{\Psi_{1}}{\Psi_{3}}.
    \end{aligned}\]</span> Similarly, <span
class="math display">\[\begin{aligned}
    \oip{\Psi_{1}}{a\Psi_{2}+b\Psi_{3}}&amp;=\sum_{i=1}^{n}(ac^{(2)}_{i}+bc^{(3)}_{i})^{\ast}c^{(1)}_{i}=\sum_{i=1}^{n}a^{\ast}c^{(2)\ast}_{i}c^{(1)}_{i}+\sum_{i=1}^{n}b^{\ast}c^{(3)\ast}_{i}c^{(1)}_{i}\\
    &amp;=\oip{\Psi_{1}}{a\Psi_{2}}+\oip{\Psi_{1}}{b\Psi_{3}}=a\oip{\Psi_{1}}{\Psi_{2}}+b\oip{\Psi_{1}}{\Psi_{3}}.
    \end{aligned}\]</span> As the sigma summation continues to
distribute over any sum, the facts above can be extended to include more
than three vectors in an inner product. The short-form facts are: <span
class="math display">\[\obip{\Psi_{1}}{\sum_{i}
c_{i}\Psi_{i}}=\sum_{i}[c_{i}\oip{\Psi_{1}}{\Psi_{i}}]\]</span> and
<span class="math display">\[\obip{\sum_{i}
c_{i}\Psi_{i}}{\Psi_{1}}=\sum_{i}[c_{i}^{\ast}\oip{\Psi_{i}}{\Psi_{1}}]\]</span></p></li>
<li><p>The above then leads to more implications. If we define a linear
combination of state space vectors <span
class="math inline">\(\psi_{1}\)</span> and <span
class="math inline">\(\psi_{2}\)</span> with coefficients <span
class="math inline">\(c_{1}\)</span> and <span
class="math inline">\(c_{2}\)</span>, then <span
class="math display">\[c_{1}\Psi_{1}+c_{2}\Psi_{2}:=\Psi\]</span> is in
the state space as <span class="math display">\[\begin{aligned}
    \oip{\Psi}{\Psi}&amp;=\oip{c_{1}\Psi_{1}+c_{2}\Psi_{2}}{c_{1}\Psi_{1}+c_{2}\Psi_{2}}\\
    &amp;=c_{1}^{\ast}c_{1}\oip{\Psi_{1}}{\Psi_{1}}+c_{1}^{\ast}c_{2}\oip{\Psi_{1}}{\Psi_{2}}+c_{2}^{\ast}c_{1}\oip{\Psi_{2}}{\Psi_{1}}+
c_{2}^{\ast}c_{2}\oip{\Psi_{2}}{\Psi_{2}}
    \end{aligned}\]</span> by separating the summation into the sum of
these separate inner products (sum) by rule IP 3 of inner products, all
the constants and inner products above must be finite so the norm of
<span class="math inline">\(\Psi\)</span>, a linear combination of state
space vectors, is finite. This therefore means that for any basis
vectors of the state space, a linear combination of them is also in the
state space. This is the formal mathematical justification for why in
the state space all possible state vector additions and therefore
physical state superpositions are possible.</p></li>
</ol>
<h3 id="orthonormality">Orthonormality</h3>
<p>The inner product is an operation which is defined for state space
vectors in a set basis, matching-and-multiplying their components in the
basis – or, alternatively, the values of their component functions, or
wavefunctions, and producing a finite number. As it is so critical to
all quantum mechanical calculation, it is necessary to be fluent with
the above rules of the inner product as they will not always be
repeated. If in doubt, writing out an inner product into the explicit
sum form should be revelatory for those who are not yet so fluent.<br />
<br />
Now we return to the idea that if we were working in an infinite
dimensional vector space, like the state space, there are infinite bases
which can span the vector space. This then poses the question of how to
choose which bases we want to work in. Ultimately, the answer to this
question cannot be given until we reach discussion on the
representations of observables, since we want most of the time to work
from the perspective of different observables when we are solving a
problem; however, to every basis we have a process can be undertaken to
make the basis substantially more convenient to work with, while still
spanning the same space as a basis. The concept is of an
<strong>orthonormal basis</strong>: the attribute of basis vectors being
orthogonal to each other and also normalised. To understand how these
vectors span the we will need to break down these two characteristics
into simple definitions.<br />
<br />
Two vectors <span class="math inline">\(\bm{\alpha}\)</span> and <span
class="math inline">\(\beta\)</span> are <strong>orthogonal</strong> if
the following is true: <span
class="math display">\[\oip{\bm{\alpha}}{\beta} =  0\]</span> we also
know that this means that <span
class="math display">\[\oip{\beta}{\bm{\alpha}} =
\oip{\bm{\alpha}}{\beta}^{\ast} = 0\]</span> and therefore the order of
the inner product does not matter for orthogonal vectors, since the
complex conjugate of <span class="math inline">\(0\)</span> is still
<span class="math inline">\(0\)</span>. Meanwhile, a singular vector
<span class="math inline">\(\tilde{\bm{\alpha}}\)</span> is said to be
<strong>normalised</strong> if it has norm <span
class="math inline">\(1\)</span>: <span
class="math display">\[\oip{\tilde{\bm{\alpha}}}{\tilde{\bm{\alpha}}} =
1.\]</span> So for an <strong>orthonormal basis</strong> of a vector
space <span
class="math inline">\(\mathbb{O}=\{\tilde{\bm{\alpha}}_{1},\tilde{\bm{\alpha}}_{2},...,\tilde{\bm{\alpha}_{n}}\}\)</span>:
<span class="math display">\[\forall\:\:
\tilde{\bm{\alpha}}_{i},\tilde{\bm{\alpha}}_{j} \in \mathbb{O}, \:\:\:\:
\oip{\tilde{\bm{\alpha}}_{i}}{\tilde{\bm{\alpha}}_{j}}=
\delta_{ij}\]</span> where <span
class="math inline">\(\delta_{ij}\)</span> is the <strong>Kronecker
delta</strong> (which will appear often in quantum mechanics): <span
class="math display">\[\delta_{ij}=
\begin{cases}
1, &amp;\text{if}\ i=j\\
0, &amp;\text{if}\ i \neq j
\end{cases}.\]</span> In other words, an orthonormal basis set is a
basis where all the vectors are orthogonal to each other and themselves
are normalised (note that orthogonality is a property shared by two or
more vectors, while normalisation is a property of single vectors). If
it is not clear now how an orthonormal basis will make manipulating
inner products and therefore wavefunctions much easier, it will become
clear in the close future. First, though, we will prove that every basis
of linearly independent vectors can be converted to an orthonormal
basis. This is in fact a famous theorem: the Gram-Schmidt Theorem, and
it is a highly-useful fact to know that we can perform it on any set of
linearly independent vectors.<br />
<br />
<br />
<u><strong>Gram-Schmidt Theorem</strong></u>: Every basis of linearly
independent vectors can be transformed by a defined procedure into an
orthonormal set.<br />
<br />
<strong>Proof- The Gram Schmidt Process</strong>: Let <span
class="math inline">\(\{\bm{\alpha}_{1},\bm{\alpha}_{2},...
\bm{\alpha}_{n}\}\)</span> be a linearly independent basis. We will
start by normalising the first vector using a simple procedure.<br />
<br />
Take the first vector, denoted without loss of generality as <span
class="math inline">\(\bm{\alpha}_{1}\)</span>. Take the norm of the
vector, <span
class="math display">\[|\bm{\alpha}_{1}|=\sqrt{\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}}}.\]</span>
By the positive semidefinite metric, <span
class="math display">\[\begin{aligned}
\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}} \geq 0 &amp;\implies \:\:
\sqrt{\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}}} \in \mathbb{R}\\
&amp;\implies \:\:
|\bm{\alpha}_1|^{\ast}=\left(\sqrt{\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}}}\right)^{\ast}=
\sqrt{\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}}} =|\bm{\alpha}_1|
\end{aligned}\]</span> Then, if we define <span
class="math display">\[\tilde{\bm{\alpha}}_{1} :=
\frac{\bm{\alpha}_{1}}{|\bm{\alpha}_{1}|}\]</span> we clearly get <span
class="math display">\[\oip{\tilde{\bm{\alpha}}_{1}}{\tilde{\bm{\alpha}}_{1}}
=
\frac{\oip{{\bm{\alpha}}_{1}}{{\bm{\alpha}}_{1}}}{|\bm{\alpha}_{1}|^{\ast}|\bm{\alpha}_{1}|}=\frac{\oip{{\bm{\alpha}}_{1}}{{\bm{\alpha}}_{1}}}{|\bm{\alpha}_{1}|^{2}}=1\]</span>
because <span class="math display">\[|\bm{\alpha}_{1}|^2=
\sqrt{\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}}}^2 =
\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}}.\]</span> Therefore, we indeed
verify that for any arbitrary vector <span
class="math inline">\(\bm{\alpha}_{1}\)</span> dividing it by its norm
<span
class="math inline">\(|\bm{\alpha}_{1}|=\sqrt{\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}}}\)</span>
creates a normalised vector <span
class="math inline">\(\tilde{\bm{\alpha}}_{1}\)</span> whose inner
product with itself is equal to 1. If the vector was already normalised
such that <span
class="math inline">\(\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}}=1\)</span>
then division by its norm <span
class="math inline">\(\sqrt{\oip{\bm{\alpha}_{1}}{\bm{\alpha}_{1}}}\)</span>
is equal to division by <span class="math inline">\(\sqrt{1}=1\)</span>,
so it would simply be unchanged by this process and remain
normalised.<br />
<br />
Now that we have normalised the first vector in our basis, we move on to
consider the second vector <span
class="math inline">\(\bm{\alpha}_{2}\)</span>. Define <span
class="math display">\[\bm{\alpha}&#39;_{2}:=
\bm{\alpha}_{2}-\tilde{\bm{\alpha}}_{1}
\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{2}}.\]</span> Is it
orthogonal to <span
class="math inline">\(\tilde{\bm{\alpha}}_{1}\)</span>? We can verify,
remembering fact IP 3 that the inner product distributes linearly: <span
class="math display">\[\oip{{\tilde{\bm{\alpha}}_{1}}}{\bm{\alpha}&#39;_{2}}=\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{2}}-\oip{\tilde{\bm{\alpha}}_{1}}{\tilde{\bm{\alpha}}_{1}}
\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{2}}\]</span> and since <span
class="math inline">\(\tilde{\bm{\alpha}}_{1}\)</span> is normalised,
<span
class="math inline">\(\oip{\tilde{\bm{\alpha}}_{1}}{\tilde{\bm{\alpha}}_{1}}=1\)</span>,
so <span
class="math display">\[\oip{{\tilde{\bm{\alpha}}_{1}}}{\bm{\alpha}&#39;_{2}}=
\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{2}}-
\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{2}}= 0\]</span> Therefore
<span class="math inline">\(\bm{\alpha}&#39;_{2}\)</span> is indeed
orthogonal to <span
class="math inline">\(\tilde{\bm{\alpha}}_{1}\)</span>. Now we can run
the normalisation procedure: <span
class="math display">\[\tilde{{\bm{\alpha}_{2}}}:=
\frac{\bm{\alpha}&#39;_{2}}{\sqrt{\oip{\bm{\alpha}&#39;_{2}}{\bm{\alpha}&#39;_{2}}}}\]</span>
This is still clearly orthogonal to <span
class="math inline">\(\tilde{\bm{\alpha}}_{1}\)</span> because of how we
defined <span class="math inline">\(\bm{\alpha}&#39;_{2}\)</span>. This
time, we just get instead: <span
class="math display">\[\oip{\tilde{\bm{\alpha}}_{1}}{\tilde{\bm{\alpha}}_{2}}=\frac{\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}&#39;_{2}}}{\sqrt{\oip{\bm{\alpha}&#39;_{2}}{\bm{\alpha}&#39;_{2}}}}=\frac{0}{\sqrt{\oip{\bm{\alpha}&#39;_{2}}{\bm{\alpha}&#39;_{2}}}}=0.\]</span><br />
<br />
The process from now continues in the same way for as many basis vectors
we need to convert into this orthonormal basis. Orthogonal vector <span
class="math inline">\(\bm{\alpha}&#39;_{3}\)</span> is introduced by
<span class="math display">\[\begin{aligned}
\bm{\alpha}&#39;_{3}&amp;=\bm{\alpha}&#39;_{3}-\tilde{\bm{\alpha}}_{1}\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{3}}-\tilde{\bm{\alpha}}_{2}\oip{\tilde{\bm{\alpha}}_{2}}{\bm{\alpha}_{3}}
\\
\Rightarrow\:\:
\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}&#39;_{3}}&amp;=\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{3}}-\oip{\tilde{\bm{\alpha}}_{1}}{\tilde{\bm{\alpha}}_{1}}\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{3}}-\oip{\tilde{\bm{\alpha}}_{1}}{\tilde{\bm{\alpha}}_{2}}\oip{\tilde{\bm{\alpha}}_{2}}{\bm{\alpha}_{3}}\\
&amp;=
\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{3}}-1\times\oip{\tilde{\bm{\alpha}}_{1}}{\bm{\alpha}_{3}}-0\times\oip{\tilde{\bm{\alpha}}_{2}}{\bm{\alpha}_{3}}\\
&amp;=0\\
\end{aligned}\]</span> After that, it can be normalised by dividing by
its norm as already proven for all vectors. This orthonormalisation
process can run forever, regardless of which other normalised basis
vector we are taking the inner product with. We prove this rigorously in
general form by induction.<br />
<br />
<strong>Claim</strong>: For a set of orthonormal vectors <span
class="math inline">\(\mathbb{O}\)</span> = <span
class="math inline">\(\{\tilde{\bm{\alpha}}_{1},\tilde{\bm{\alpha}}_{2},...\tilde{\bm{\alpha}}_{n-1}\}\)</span>,
the vector <span class="math display">\[\bm{\alpha}&#39;_{n}=
\bm{\alpha}_{n}-\left(\sum_{i=0}^{n-1}\tilde{\bm{\alpha}}_{i}\oip{\tilde{\bm{\alpha}_{i}}}{\bm{\alpha}_{n}}\right)\]</span>
is orthogonal to all the vectors in <span
class="math inline">\(\mathbb{O}\)</span>.<br />
<br />
<strong>Proof</strong>: The base case is for <span
class="math inline">\(n=3\)</span>, which we have already proved above.
The inductive step is: <span class="math display">\[\begin{aligned}
&amp;\text{Assume we have an orthonormal set}\:
\{\tilde{\bm{\alpha}}_{1},\tilde{\bm{\alpha}}_{2},...\tilde{\bm{\alpha}}_{n-1}\}.
\:\:\: \forall\:\tilde{{\bm{\alpha}}}_{j &lt; n}, \\
&amp;\oip{\tilde{\bm{\alpha}}_{j}}{\bm{\alpha}&#39;_{n}}=\oip{\tilde{\bm{\alpha}_{j}}}{\bm{\alpha}_{n}}-\tilde{\bm{\alpha}_{j}}\left(\sum_{i=0}^{n-1}\tilde{\bm{\alpha}_{i}}\oip{\tilde{\bm{\alpha}}_{i}}{\bm{\alpha}_{n}}\right)\\
\Rightarrow\:&amp;\oip{\tilde{\bm{\alpha}_{j}}}{\bm{\alpha}&#39;_{n}}=\oip{\tilde{\bm{\alpha}_{j}}}{\bm{\alpha}_{n}}-\oip{\tilde{\bm{\alpha}_{j}}}{\tilde{\bm{\alpha}_{j}}}\oip{\tilde{\bm{\alpha}_{j}}}{\bm{\alpha}_{n}}-\sum_{i=0\neq
j}^{n-1}\oip{\tilde{\bm{\alpha}}_{j}}{\tilde{\bm{\alpha}_{i}}}\oip{\tilde{\bm{\alpha}_{i}}}{\bm{\alpha}_{n}}\\
\Rightarrow\:&amp;\oip{\tilde{\bm{\alpha}_{j}}}{\bm{\alpha}&#39;_{n}}=\oip{\tilde{\bm{\alpha}}_{j}}{\bm{\alpha}_{n}}-1\times\oip{\tilde{\bm{\alpha}_{j}}}{\bm{\alpha}_{n}}-\sum_{i=0\neq
j}^{n-1}0\times\oip{\tilde{\bm{\alpha}_{i}}}{\bm{\alpha}_{n}}=0
\end{aligned}\]</span> This is true since our assumption that all <span
class="math inline">\(\tilde{\bm{\alpha}}_{j}\)</span> for <span
class="math inline">\(j&lt;n\)</span> are normalised and orthogonal to
all other <span class="math inline">\(\tilde{\bm{\alpha}}_{i\neq
j}\)</span> for <span class="math inline">\(i&lt;n\)</span> (an
assumption which is valid as it follows from the base case) necessitates
that <span class="math display">\[\forall\:\: i,j &lt; n, \:\:
\oip{\tilde{\bm{\alpha}}_{j}}{\tilde{\bm{\alpha}}_{i}}=
\delta_{ij}\]</span> Therefore, by induction, our orthogonalisation
process works for as many basis vectors as we need to convert to our
orthonormal basis.<br />
<br />
Then, after we obtain <span
class="math inline">\(\bm{\alpha}&#39;_{n}\)</span>, <span
class="math display">\[\tilde{\bm{\alpha}_{n}}=\frac{\bm{\alpha}&#39;_{n}(x)}{\sqrt{\oip{\bm{\alpha}&#39;_{n}}{\bm{\alpha}&#39;_{n}}}}\]</span>
is normalised and does not affect the orthogonality condition: <span
class="math display">\[\left(\tilde{\bm{\alpha}}_{j\neq n},
\frac{\bm{\alpha}&#39;_{n}}{\sqrt{\oip{\bm{\alpha}&#39;_{n}}{\bm{\alpha}&#39;_{n}}}}\right)=\frac{\oip{\tilde{\bm{\alpha}}_{j\neq
n}}{\bm{\alpha}&#39;_{n}}}{\sqrt{\oip{\bm{\alpha}&#39;_{n}}{\bm{\alpha}&#39;_{n}}}}=
\frac{0}{\sqrt{\oip{\bm{\alpha}&#39;_{n}}{\bm{\alpha}&#39;_{n}}}}=0.\]</span><br />
We have now fully mathematically proven a procedure which allows us to
take a linearly independent basis of any size and convert it to an
orthonormal basis. It obviously is a tedious process if you have a large
number of basis vectors, but its theoretical importance is twofold: the
first, that once you have converted to an orthonormal basis, the inner
product will become much easier to evaluate; the second, that we now
know there is a valid process which exists– no matter if we choose to
perform it or not. Therefore, we can assume <em>without loss of
generality</em> that all bases we use are orthonormal, since if they
weren’t we would easily theoretically be able to orthonormalise them.
This procedure is the elegant <strong>Gram-Schmidt Process</strong>: a
hugely powerful weapon in linear algebra and indeed therefore in quantum
mechanics.<br />
<br />
The original point of this section was to further define a vector space
in terms of its dimensions. Now, having defined orthonormality, we will
go full circle back to the first vector space we considered– the
Cartesian x,y plane, or, <span
class="math inline">\(\mathbb{R}^2\)</span>. In our most basic study of
this vector space, we associate the numbers of dimensions with the
number of perpendicular directions. We will find that in fact the
natural way to come back to this assertion is to look at it not in terms
of linearly independent vectors, but, in a very subtle change, in terms
of the number of mutually orthogonal vectors. Let’s see why this is an
appropriate replacement from our earlier definition of dimensions in
terms of linearly independent vectors.<br />
<br />
<strong>Theorem</strong>: All mutually orthogonal vectors are also
linearly independent.<br />
<br />
This is very easy to prove. Suppose we have a set of mutually orthogonal
vectors <span class="math inline">\(\mathbb{O}=\{\bm{\alpha}_{1},
\bm{\alpha}_{2}, ..., \bm{\alpha}_{n}\}\)</span>. They are linearly
independent if no nontrivial combination of these vectors is equal to
<span class="math inline">\(0\)</span>. Let us write a generalised form
of a linear combination of these vectors, and set it equal to the null
vector: <span
class="math display">\[\sum_{i=1}^{n}c_{i}\bm{\alpha}_{i}=0\]</span> Now
we can manipulate the mutual orthogonality of these vectors. <span
class="math display">\[\begin{aligned}
\forall\: j \in [1,n], \:\:
\left(\bm{\alpha}_{j},\sum_{i=1}^{n}c_{i}\bm{\alpha}_{i}\right)
&amp;=\oip{\bm{\alpha}_{j}}{0} \\
\Rightarrow\:\:  c_{j}\oip{\bm{\alpha}_{j}}{\bm{\alpha}_{j}}+\sum_{i=1\neq
j}^{n}c_{i}\oip{\bm{\alpha}_{j}}{\bm{\alpha}_{i}} &amp;= 0 \\
\Rightarrow\:\:
c_{j}\oip{\bm{\alpha}_{j}}{\bm{\alpha}_{j}}+\sum_{i=1\neq j}^{n}0\times
c_{i} &amp;= 0 \\
\Rightarrow\:\:  c_{j}(\bm{\alpha}_{j},\bm{\alpha}_{j})= 0
\Rightarrow\:\:c_{j} &amp;= 0
\end{aligned}\]</span> unless <span
class="math inline">\(\oip{\bm{\alpha}_{j}}{\bm{\alpha}_{j}}=0\)</span>
which is impossible unless it is the null vector, which is irrelevant in
these discussions as it is never considered a basis vector. This proof
clearly works without loss of generality for each of the constants <span
class="math inline">\(c_{i}\)</span>, so it implies that every constant
<span class="math inline">\(c_{i}=0\)</span>. Therefore the only
possible linear combination of a set of mutually orthogonal vectors
equal to the null vector is the trivial combination– and so they are all
linearly independent if they are all mutually orthogonal.<br />
<br />
Our final definition of dimensionality is therefore the number of
mutually orthogonal vectors which can reside in a vector space. This
number cannot be exceeded, since it is also the maximum number of
linearly independent vectors which can be accommodated, and every new
mutually orthogonal vector is in itself a new linearly independent
vector.<br />
<br />
Finally, comes the inner product punchline, which really shows why the
operation, combined with an orthonormal basis, is so powerful. If we
recall, for a given basis <span
class="math inline">\(\setof{\bm{\alpha}_{i}}\)</span> of the state
space it spans the space and therefore all state vectors can be
expressed in the form: <span
class="math display">\[\Psi=\sum_{i}c_{i}\bm{\alpha}_{i}.\]</span> The
coefficients <span class="math inline">\(c_{i}\)</span> are called the
components of the vector in that basis <span
class="math inline">\(\setof{\bm{\alpha}_{i}}\)</span>, and are
seemingly difficult to determine for the given basis depending on which
vector we choose. However, consider the case when the basis chosen is
orthonormal, or was not orthonormal but has now undergone
orthonormalisation under the Gram-Schmidt Process. Now, consider the
inner product <span
class="math display">\[\oip{\bm{\alpha}_{j}}{\Psi}\]</span> for some
given basis vector <span class="math inline">\(\bm{\alpha}_{j}\)</span>.
This is, according to the above expansion, <span
class="math display">\[\oip{\bm{\alpha}_{j}}{\Psi}=\oip{\bm{\alpha}_{j}}{\sum_{\{i\}}c_{i}\bm{\alpha}_{i}}.\]</span>
Then by the fact S2 of the constant multiple rule and IP 3 of linear
distributivity again, this is: <span
class="math display">\[\sum_{\{i\}}c_{i}\oip{\bm{\alpha}_{j}}{\bm{\alpha}_{i}}\]</span>
which by the Kronecker delta results in just <span
class="math display">\[\oip{\bm{\alpha}_{j}}{\Psi}=\biggl(\sum_{i\neq
j}\oip{\bm{\alpha}_{j}}{\bm{\alpha}_{i}}\biggr)+c_{i}\oip{\bm{\alpha}_{j}}{\bm{\alpha}_{j}}=0+c_{j}\times
1=c_{j}.\]</span> This is critical, as we see that these components
<span class="math inline">\(c_{i}\)</span> in an orthonormal basis are
not mathematically random: the component corresponding to a basis vector
can be obtained through the inner product of that basis vector with the
state vector! The much more eye-opening form of a state vector is after
you make this substitution for the components: <span
class="math display">\[\Psi=\sum_{\{i\}}\oip{\bm{\alpha}_{i}}{\Psi}\bm{\alpha}_{i}.\]</span>
If we find a basis of the state space, we can immediately orthonormalise
it by the Gram-Schmidt process, and if we can find an orthonormal basis
for the state space and understand the vectors which form it we can then
theoretically define any vector in that basis exceptionally easily. We
therefore always assume we are working with an orthonormal basis: such
an assumption is valid since the Gram-Schmidt procedure exists, and is
useful because orthonormality can often make algebraic manipulations
like the above.<br />
<br />
There is a final note to make. If <span
class="math display">\[{\psi}_{\bm{\alpha}}(\bm{\alpha}_{i})=c_{i}\]</span>
and <span
class="math display">\[\oip{\bm{\alpha}_{i}}{\Psi}=c_{i}\]</span> for an
orthonormal basis, we therefore have <span
class="math display">\[{\psi}_{\bm{\alpha}}(\bm{\alpha}_{i})=\oip{\bm{\alpha}_{i}}{\Psi}.\]</span>
In the continuous case, we have <span
class="math display">\[\oip{x}{\Psi}=\psi_{\bm{\alpha}}(x)\]</span>
where <span class="math inline">\(x\)</span> is the continuously
variable orthonormal basis vector. This is dramatic, because it tells us
exactly how to form the discrete and continuous wavefunctions given some
basis– convert it into an orthonormal basis, and find the inner product
of the state vector with the basis vectors to produce the wavefunction!
So, we are done with everything we need to know about the state vector.
We have an object which represents physical states, and we have a way to
transform it into a tangible wavefunction by which we can obtain
probabilities of measurements in a way shortly to follow.</p>
<h3 id="scaling-state-vectors">Scaling State Vectors</h3>
<p>Through careful discussion, we have begun to understand the key
bijection which forms the state postulate: <span
class="math display">\[\text{Physical States} \leftrightarrow
\text{Hilbert Space Vectors}.\]</span> Technically, however, this is
still not quite correct. The <em>actual</em> bijection is <span
class="math display">\[\text{Physical States} \leftrightarrow
\text{Hilbert Space Rays}.\]</span> Here, a ‘ray’ is a vector with norm
1. So in particular, taking any state vector <span
class="math inline">\(\Psi\)</span> and performing <span
class="math display">\[\Psi \to
\tilde{\Psi}:=\frac{\Psi}{\sqrt{(\Psi,\Psi)}}\]</span> transforms it
into a ray in “the same direction". This notion of a “direction" comes
from the arrow visualisation of vectors, but with the caveat that we are
in infinite dimensional space so we cannot actually visualise
geometrically what these directions should look like. However, the
notion of a direction is useful for our understanding because it helps
us understand that we can take a unit ray and then multiply it by any
scalar, creating a new Hilbert space vector, but which is equivalent to
the original unit ray <em>up to some scaling</em>.<br />
<br />
In practice, quantum mechanics regards these scalings as ‘unphysical
phase factors’. In other words, given some normalised state <span
class="math inline">\(\tilde{\Psi}\)</span> and any constant <span
class="math inline">\(c\in\mathbb{C}\)</span> then the state <span
class="math inline">\(\Psi:=c\tilde{\Psi}\)</span> is “the same state,
up to some unphysical scaling". Hilbert space vectors which do not have
norm 1 are not physical states, but since we can always divide by their
norm to create a unit ray in the Hilbert space, they can be considered
to be physical states with some redundant scale factor attached. It may
seem rather arbitrary to discard the multiplicative constants like this,
but there are two logical justifications for this:</p>
<ul>
<li><p>Since we must accept <em>any</em> superposition of basis states
as a new state, then <span class="math display">\[\forall c_{i}\in
\mathbb{C}, \:\:\:\: \sum_{\{i\}}c_{i}\bm{\alpha}_{i} \text{is a
state}\]</span> but then <span class="math display">\[\forall c_{i}\in
\mathbb{C}, \:\:\:\: \sum_{\{i\}}(10000000)c_{i}\bm{\alpha}_{i} \text{is
a state}\]</span> and it obviously would seem quite pointless if
multiplying by arbitrary numbers in the above way created something
completely new every time.</p></li>
<li><p>More concretely, the components of the state vector are connected
to probabilities of measuring the basis states in some way. We do not
know how just yet, but intuitively, it should make sense that it the
probabilistic distribution is about the <em>relative weight</em> that
each basis state gets, not about the absolute weight. In other words,
multiplying every component by <span
class="math inline">\(10000000\)</span> might change the absolute value
of the components, but the component of some arbitrary basis vector
<span class="math inline">\(\bm{\alpha}_{i}\)</span> will still be
double the component of some arbitrary basis vector <span
class="math inline">\(\bm{\alpha}_{j}\)</span> if this was originally
the case before multiplying by <span
class="math inline">\(10000000\)</span>. So the relative likelihood of
<span class="math inline">\(\bm{\alpha}_{i}\)</span> over <span
class="math inline">\(\bm{\alpha}_{j}\)</span> is in a sense still
conserved.</p></li>
<li><p>The reason why having norm 1 makes a state physical, we will see,
is related to the fact that the probabilities for all possibilities of a
probabilistic event must sum to 1: something happens with probability 1.
This is why Hilbert space unit rays are the physically relevant state
vectors, and any other scaled Hilbert space vectors should be reduced to
the unit rays in the same directions.<br />
<br />
This discussion will become more concrete in the near future once we
actually connect the components to measurement probabilities, but
hopefully it should intuitively make sense why scaling state vectors by
arbitrary constants should not really have much meaning.</p></li>
</ul>
<p>In practice, this will actually make our life much easier, because
when we solve problems and algebraically manipulate the state vector we
do not have to preserve normalisation through every step: we can work
with these unphysical scalings, and only at the end of the problem we
can normalise the final Hilbert space vector to become a physical state
space vector. Since unphysical phase factors do not differ from the
relevant unit ray state, we will always be transforming between states
in the correct ways regardless of the norms of the state vectors in the
intermediary steps.</p>
<h3 id="wavefunctions">Wavefunctions</h3>
<p>We now understand that there is a bijection <span
class="math display">\[\text{Physical States} \leftrightarrow
\text{Hilbert Space Rays}\]</span> However, recall our lengthy
discussion on how the state vector, without a basis, is
<em>formless</em>. We must specify a coordinate basis to numerically
specify the position of an object on a map, and similarly we must
specify a vector basis for our state space in order to give the state
vector an actual form. On the other hand, if we <em>are</em> given some
fixed basis, say <span
class="math inline">\(\setof{\bm{\alpha}_{i}}\)</span>, we can write
<span class="math display">\[\Psi =
\sum_{\setof{i}}c_{i}\bm{\alpha}_{i}\]</span> for some set of
coefficients <span class="math inline">\(\setof{c_{i}}\)</span>, and in
particular this set of coefficients is unique, since we have already
proved the expansion of any constituent vector in a spanning basis of a
vector space is unique.<br />
<br />
Then, however, we can create a new bijection. <strong>In some fixed
basis</strong>, <span class="math display">\[\text{State Vectors}
\leftrightarrow \text{Sets of Components}\]</span> using the fact every
state vector has a unique expansion, and every expansion corresponds to
a unique state vector. However, we can do better, by defining the
mapping <span class="math display">\[\psi_{\bm{\alpha}}(x) : \mathbb{B}
\to \mathbb{C}, \:\: \psi_{\bm{\alpha}}(\bm{\alpha}_{i}) \mapsto
c_{i}\]</span> which takes an input basis vector from the basis <span
class="math inline">\(\mathbb{B}\)</span> and outputs its component with
respect to that basis vector. Note here that the subscript <span
class="math inline">\(\alpha\)</span> on <span
class="math inline">\(\psi_{\alpha}\)</span> reflects that in practice
we always use the same letter with varying indices to denote a set of
basis vectors, and thus can use the same letter to indicate what basis
our map is defined in. Now, what does that map look like? There are two
cases:</p>
<ol>
<li><p><strong>Discrete case:</strong> The map will also clearly take a
discrete set of values, one for each of the discrete basis vectors. A
clue is that this map <em>resembles the probability mass function</em>,
which also outputs a discrete set of scalars depending on a discrete set
of outcomes.</p></li>
<li><p><strong>Continuous case:</strong> The map should also clearly
take a continuous set of values. The reason for this is that in for the
concept of a continuous basis to arise we must have some notion of basis
vectors being ‘infinitely close’ to each other (otherwise there would be
gaps between basis vectors and we would have a discrete basis). But
then, if the Hilbert space state vector is well defined then clearly for
infinitesimally close basis vectors we must have infinitesimally close
coefficients in its expansion, which in turn makes this map have a
continuous output.<br />
<br />
A clue is that this map <em>resembles the probability density
function</em>, which is also a continuous function, where the output
scalars (which are probabilities) are infinitesimally close for
infinitesimally close inputs (outcomes).</p></li>
</ol>
<p>So we see why our notion of this ‘component map’ will be crucial.
Given some meaningful basis, our component map should be able to
transform into a probability distribution function! Now we do not know
how it does so numerically yet, and neither do we know which bases are
<em>meaningful</em>, but we will reach this imminently.<br />
<br />
Remember, the form of this map, which we henceforth call the
<strong>wavefunction</strong>, changes based on which basis we have
picked to express it in (since the output components will be different).
Thus, there are infinitely many different wavefunctions. We have thus a
new rich set of bijections! <span class="math display">\[\text{State
Vectors} \leftrightarrow\text{Basis Wavefunctions}\]</span> but also,
since <span class="math display">\[\text{Physical States}\leftrightarrow
\text{State Vectors}\]</span> we therefore must also have <span
class="math display">\[\text{Physical States}\leftrightarrow\text{Basis
Wavefunctions}\]</span> So in a sense our state vector is the more
general, intermediate object, which is in a <em>basis independent</em>
bijection with physical states; the <em>basis-dependent</em>
wavefunction is then our final object, which we can use to actually
encode and deduce probabilities.</p>
<h3 id="summary">Summary</h3>
<p>We have begun our study of the quantum state problem. We know that
there is a state vector in bijection with physical states, and how to
develop functional forms of it. One can now move onto the next chapter,
where we will subsequently be expanding this theory, especially with
regards to observables. Most importantly, we need to understand which
bases are useful for us in quantum mechanics; to do this, we will need
to fit observables into the picture and very fruitful results will
follow.</p>
<h2 id="exercises-from-chapter-3ast">Exercises from Chapter 3<span
class="math inline">\(\ast\)</span></h2>
<ol>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>
<hr>
<div style="text-align: center; font-size: 1.2em; padding-top: 20px;">
    <a href='2.html'>&larr; Previous Chapter</a> &nbsp;&nbsp; | &nbsp;&nbsp; <a href='4.html'>Next Chapter &rarr;</a>
</div>
</body>
</html>
